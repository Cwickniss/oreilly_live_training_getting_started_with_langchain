{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2023-09-09-19-16-04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "browserless_api_key = os.getenv(\"BROWSERLESS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt_question):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k-0613\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful research and\\\n",
    "            programming assistant\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt_question}]\n",
    "    )\n",
    "    \n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Given an initial url. I want an agent to use the resources found there to try and answer a given query (rather than look for it using the a search engine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def get_num_tokens(prompt, model=\"gpt-3.5-turbo-16k-0613\"):\n",
    "    \n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    return len(enc.encode(prompt))\n",
    "\n",
    "\n",
    "def scrape_website(url: str, max_token_size=16385) -> str:\n",
    "    \"\"\"Scrape the contents of a website and return the text\"\"\"\n",
    "    response = requests.get(url)\n",
    "    # soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # webpage_contents = soup.get_text()\n",
    "    output = response.text\n",
    "    num_tokens = get_num_tokens(output)\n",
    "    if num_tokens>max_token_size:\n",
    "        len_diff = int((num_tokens - max_token_size)*4.2)\n",
    "        output = output[:len(output) - len_diff]\n",
    "    \n",
    "    return output \n",
    "\n",
    "\n",
    "def extract_clean_urls(input_string):\n",
    "    # Step 1: Extract the URLs using regular expression\n",
    "    url_pattern = r'https?://[^\\s]+'\n",
    "    raw_urls = re.findall(url_pattern, input_string)\n",
    "    \n",
    "    # Step 2: Clean the URLs\n",
    "    cleaned_urls = []\n",
    "    for url in raw_urls:\n",
    "        # Remove trailing symbols like '\\n1', '\\n2', etc.\n",
    "        cleaned_url = re.sub(r'\\\\n\\d+$', '', url)\n",
    "        cleaned_urls.append(cleaned_url)\n",
    "    \n",
    "    # Step 3: Return the cleaned URLs as a Python list\n",
    "    return cleaned_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\">\\n\\n<head>\\n\\n  <meta charset=\"utf-8\">\\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n  <meta name=\"description\" content=\"Keras documentation\">\\n  <meta name=\"author\" content=\"Keras Team\">\\n  <link rel=\"shortcut icon\" href=\"https://keras.io/img/favicon.ico\">\\n\\n  <!-- Social -->\\n  <meta property=\"og:title\" content=\"Keras documentation: Code examples\">\\n  <meta property=\"og:image\" content=\"https://keras.io/img/logo-k-keras-wb.png\">\\n  <meta name=\"twitter:title\" content=\"Keras documentation: Code examples\">\\n  <meta name=\"twitter:image\" content=\"https://keras.io/img/k-keras-social.png\">\\n  <meta name=\"twitter:card\" content=\"summary\">\\n\\n  <title>Code examples</title>\\n\\n  <!-- Bootstrap core CSS -->\\n  <link href=\"/css/bootstrap.min.css\" rel=\"stylesheet\">\\n\\n  <!-- Custom fonts for this template -->\\n  <link href=\"https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600;700;800&display=swap\" rel=\"stylesheet\">\\n\\n  <!-- Custom styles for this template -->\\n  <link href=\"/css/docs.css\" rel=\"stylesheet\">\\n  <link href=\"/css/monokai.css\" rel=\"stylesheet\">\\n\\n  <!-- Google Tag Manager -->\\n  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({\\'gtm.start\\':\\n  new Date().getTime(),event:\\'gtm.js\\'});var f=d.getElementsByTagName(s)[0],\\n  j=d.createElement(s),dl=l!=\\'dataLayer\\'?\\'&l=\\'+l:\\'\\';j.async=true;j.src=\\n  \\'https://www.googletagmanager.com/gtm.js?id=\\'+i+dl;f.parentNode.insertBefore(j,f);\\n  })(window,document,\\'script\\',\\'dataLayer\\',\\'GTM-5DNGF4N\\');\\n  </script>\\n  <script>\\n  (function(i,s,o,g,r,a,m){i[\\'GoogleAnalyticsObject\\']=r;i[r]=i[r]||function(){\\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\\n  })(window,document,\\'script\\',\\'https://www.google-analytics.com/analytics.js\\',\\'ga\\');\\n  ga(\\'create\\', \\'UA-175165319-128\\', \\'auto\\');\\n  ga(\\'send\\', \\'pageview\\');\\n  </script>\\n  <!-- End Google Tag Manager -->\\n\\n  <script async defer src=\"https://buttons.github.io/buttons.js\"></script>\\n\\n</head>\\n\\n<body>\\n  <!-- Google Tag Manager (noscript) -->\\n  <noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-5DNGF4N\"\\n  height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\\n  <!-- End Google Tag Manager (noscript) -->\\n\\n  <div class=\\'k-page\\'>\\n  \\n    <div class=\"k-nav\" id=\"nav-menu\">\\n      <a href=\\'/\\'><img src=\\'/img/logo-small.png\\' class=\\'logo-small\\' /></a>\\n\\n      <div class=\"nav flex-column nav-pills\" role=\"tablist\" aria-orientation=\"vertical\">\\n\\n        \\n          <a class=\"nav-link\" href=\"/about/\" role=\"tab\" aria-selected=\"\">About Keras</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/getting_started/\" role=\"tab\" aria-selected=\"\">Getting started</a>\\n          \\n        \\n          <a class=\"nav-link active\" href=\"/examples/\" role=\"tab\" aria-selected=\"\">Code examples</a>\\n          \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/vision/\">Computer Vision</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/nlp/\">Natural Language Processing</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/structured_data/\">Structured Data</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/timeseries/\">Timeseries</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/generative/\">Generative Deep Learning</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/audio/\">Audio Data</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/rl/\">Reinforcement Learning</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/graph/\">Graph Data</a>\\n                \\n            \\n              <a class=\"nav-sublink\" href=\"/examples/keras_recipes/\">Quick Keras Recipes</a>\\n                \\n            \\n          \\n        \\n          <a class=\"nav-link\" href=\"/guides/\" role=\"tab\" aria-selected=\"\">Developer guides</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/api/\" role=\"tab\" aria-selected=\"\">API reference</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/keras_core/\" role=\"tab\" aria-selected=\"\">Keras Core: Keras for TensorFlow, JAX, and PyTorch</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/keras_tuner/\" role=\"tab\" aria-selected=\"\">KerasTuner: Hyperparameter Tuning</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/keras_cv/\" role=\"tab\" aria-selected=\"\">KerasCV: Computer Vision Workflows</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/keras_nlp/\" role=\"tab\" aria-selected=\"\">KerasNLP: Natural Language Workflows</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/why_keras/\" role=\"tab\" aria-selected=\"\">Why choose Keras?</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/governance/\" role=\"tab\" aria-selected=\"\">Community & governance</a>\\n          \\n        \\n          <a class=\"nav-link\" href=\"/contributing/\" role=\"tab\" aria-selected=\"\">Contributing to Keras</a>\\n          \\n        \\n\\n      </div>\\n\\n    </div>\\n\\n    <div class=\\'k-main\\'>\\n      \\n      <div class=\\'k-main-top\\'>\\n        <script>\\n          function displayDropdownMenu() {\\n            e = document.getElementById(\"nav-menu\");\\n            if (e.style.display == \"block\") {\\n              e.style.display = \"none\";\\n            }\\n            else {\\n              e.style.display = \"block\";\\n              document.getElementById(\"dropdown-nav\").style.display = \"block\";\\n            }\\n          }\\n\\n          function resetMobileUI() {\\n            if (window.innerWidth <= 840) {\\n              document.getElementById(\"nav-menu\").style.display = \"none\";\\n              document.getElementById(\"dropdown-nav\").style.display = \"block\";\\n            }\\n            else {\\n              document.getElementById(\"nav-menu\").style.display = \"block\";\\n              document.getElementById(\"dropdown-nav\").style.display = \"none\";\\n            }\\n          }\\n\\n          window.onresize = resetMobileUI;\\n        </script>\\n        <div id=\\'dropdown-nav\\' onclick=\"displayDropdownMenu();\">\\n          <svg viewBox=\"-20 -20 120 120\" width=\"60\" height=\"60\">\\n            <rect width=\"100\" height=\"20\"></rect>\\n            <rect y=\"30\" width=\"100\" height=\"20\"></rect>\\n            <rect y=\"60\" width=\"100\" height=\"20\"></rect>\\n          </svg>\\n        </div>\\n\\n        <form class=\"bd-search d-flex align-items-center k-search-form\" id=\"search-form\">\\n          <input type=\"search\" class=\"k-search-input\" id=\"search-input\" placeholder=\"Search Keras documentation...\" aria-label=\"Search Keras documentation...\" autocomplete=\"off\">\\n            <button class=\"k-search-btn\">\\n              <svg width=\"13\" height=\"13\" viewBox=\"0 0 13 13\"><title>search</title><path d=\"m4.8495 7.8226c0.82666 0 1.5262-0.29146 2.0985-0.87438 0.57232-0.58292 0.86378-1.2877 0.87438-2.1144 0.010599-0.82666-0.28086-1.5262-0.87438-2.0985-0.59352-0.57232-1.293-0.86378-2.0985-0.87438-0.8055-0.010599-1.5103 0.28086-2.1144 0.87438-0.60414 0.59352-0.8956 1.293-0.87438 2.0985 0.021197 0.8055 0.31266 1.5103 0.87438 2.1144 0.56172 0.60414 1.2665 0.8956 2.1144 0.87438zm4.4695 0.2115 3.681 3.6819-1.259 1.284-3.6817-3.7 0.0019784-0.69479-0.090043-0.098846c-0.87973 0.76087-1.92 1.1413-3.1207 1.1413-1.3553 0-2.5025-0.46363-3.4417-1.3909s-1.4088-2.0686-1.4088-3.4239c0-1.3553 0.4696-2.4966 1.4088-3.4239 0.9392-0.92727 2.0864-1.3969 3.4417-1.4088 1.3553-0.011889 2.4906 0.45771 3.406 1.4088 0.9154 0.95107 1.379 2.0924 1.3909 3.4239 0 1.2126-0.38043 2.2588-1.1413 3.1385l0.098834 0.090049z\"></path></svg>\\n            </button>\\n        </form>\\n        <script>\\n          var form = document.getElementById(\\'search-form\\');\\n          form.onsubmit = function(e) {\\n            e.preventDefault();\\n            var query = document.getElementById(\\'search-input\\').value;\\n            window.location.href = \\'/search.html?query=\\' + query;\\n            return False\\n          }\\n        </script>\\n\\n      </div>\\n      <div class=\\'k-main-inner\\'>\\n        <div class=\\'k-location-slug\\'>\\n            Â»  Code examples\\n        </div>\\n        <div class=\\'k-content\\'>\\n          <h1 id=\"code-examples\">Code examples</h1>\\n<p>Our code examples are short (less than 300 lines of code), focused demonstrations of vertical deep learning workflows.</p>\\n<p>All of our examples are written as Jupyter notebooks and can be run in one click in <a href=\"https://colab.research.google.com/notebooks/welcome.ipynb\">Google Colab</a>,\\na hosted notebook environment that requires no setup and runs in the cloud. Google Colab includes GPU and TPU runtimes.</p>\\n\\n<p><div class=\"example-highlight\">â\\x98\\x85</div> = Good starter example</p>\\n\\n\\n\\n\\t<h2 id=\"computer-vision\"><a href=\"/examples/vision/\">Computer Vision</a></h2>\\n\\n\\t\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Image classification</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/image_classification_from_scratch\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification from scratch\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/mnist_convnet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSimple MNIST convnet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/image_classification_efficientnet_fine_tuning\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification via fine-tuning with EfficientNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/image_classification_with_vision_transformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification with Vision Transformer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/bit\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage Classification using BigTransfer (BiT)\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/attention_mil_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tClassification using Attention-based Deep Multiple Instance Learning\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/mlp_image_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification with modern MLP models\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/mobilevit\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tA mobile-friendly Transformer-based model for image classification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/xray_classification_with_tpus\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tPneumonia Classification on TPU\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/cct\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tCompact Convolutional Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/convmixer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification with ConvMixer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/eanet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification with EANet (External Attention Transformer)\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/involution\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tInvolutional neural networks\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/perceiver_image_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification with Perceiver\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/reptile\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tFew-Shot learning with Reptile\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/semisupervised_simclr\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSemi-supervised image classification using contrastive pretraining with SimCLR\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/swin_transformers\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage classification with Swin Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/vit_small_ds\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTrain a Vision Transformer on small datasets\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/shiftvit\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tA Vision Transformer without Attention\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Image segmentation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/oxford_pets_image_segmentation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage segmentation with a U-Net-like architecture\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/deeplabv3_plus\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMulticlass semantic segmentation using DeepLabV3+\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/basnet_segmentation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tHighly accurate boundaries segmentation using BASNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Object detection</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/retinanet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tObject Detection with RetinaNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/keypoint_detection\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tKeypoint Detection with Transfer Learning\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/object_detection_using_vision_transformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tObject detection with Vision Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">3D</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/3D_image_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\t3D image classification from CT scans\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/depth_estimation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMonocular depth estimation\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/nerf\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\t3D volumetric rendering with NeRF\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/pointnet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tPoint cloud classification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">OCR</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/captcha_ocr\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tOCR model for reading Captchas\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/handwriting_recognition\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tHandwriting recognition\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Image enhancement</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/autoencoder\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tConvolutional autoencoder for image denoising\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/mirnet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tLow-light image enhancement using MIRNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/super_resolution_sub_pixel\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage Super-Resolution using an Efficient Sub-Pixel CNN\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/edsr\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tEnhanced Deep Residual Networks for single-image super-resolution\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/zero_dce\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tZero-DCE for low-light image enhancement\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Data augmentation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/cutmix\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tCutMix data augmentation for image classification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/mixup\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMixUp augmentation for image classification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/randaugment\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tRandAugment for Image Classification for Improved Robustness\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Image & Text</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/image_captioning\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage captioning\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/nl_image_search\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tNatural language image search with a Dual Encoder\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Vision models interpretability</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/visualizing_what_convnets_learn\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tVisualizing what convnets learn\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/integrated_gradients\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tModel interpretability with Integrated Gradients\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/probing_vits\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tInvestigating Vision Transformer representations\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/grad_cam\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tGrad-CAM class activation visualization\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Image similarity search</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/near_dup_search\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tNear-duplicate image search\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/semantic_image_clustering\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSemantic Image Clustering\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/siamese_contrastive\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage similarity estimation using a Siamese Network with a contrastive loss\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/siamese_network\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage similarity estimation using a Siamese Network with a triplet loss\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/metric_learning\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMetric learning for image similarity search\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/metric_learning_tf_similarity\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMetric learning for image similarity search using TensorFlow Similarity\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Video</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/video_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tVideo Classification with a CNN-RNN Architecture\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/conv_lstm\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tNext-Frame Video Prediction with Convolutional LSTMs\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/video_transformers\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tVideo Classification with Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/vivit\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tVideo Vision Transformer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Other</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/adamatch\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSemi-supervision and domain adaptation with AdaMatch\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/barlow_twins\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tBarlow Twins for Contrastive SSL\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/cait\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tClass Attention Image Transformers with LayerScale\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/consistency_training\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tConsistency training with supervision\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/deit\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDistilling Vision Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/fixres\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tFixRes: Fixing train-test resolution discrepancy\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/focal_modulation_network\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tFocal Modulation: A replacement for Self-Attention\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/forwardforward\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tUsing the Forward-Forward Algorithm for Image Classification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/fully_convolutional_network\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImage Segmentation using Composable Fully-Convolutional Networks\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/gradient_centralization\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tGradient Centralization for Better Training Performance\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/knowledge_distillation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tKnowledge Distillation\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/learnable_resizer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tLearning to Resize in Computer Vision\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/masked_image_modeling\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMasked image modeling with Autoencoders\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/nnclr\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSelf-supervised contrastive learning with NNCLR\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/patch_convnet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tAugmenting convnets with aggregated attention\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/pointnet_segmentation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tPoint cloud segmentation with PointNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/segformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSemantic segmentation with SegFormer and Hugging Face Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/simsiam\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSelf-supervised contrastive learning with SimSiam\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/supervised-contrastive-learning\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSupervised Contrastive Learning\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/temporal_latent_bottleneck\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tWhen Recurrence meets Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/token_learner\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tLearning to tokenize in Vision Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/vision/yolov8\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tEfficient Object Detection with YOLOV8 and KerasCV\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"natural-language-processing\"><a href=\"/examples/nlp/\">Natural Language Processing</a></h2>\\n\\n\\t\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Text classification</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/text_classification_from_scratch\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText classification from scratch\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/active_learning_review_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tReview Classification using Active Learning\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/fnet_classification_with_keras_nlp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText Classification using FNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/multi_label_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tLarge-scale multi-label text classification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/text_classification_with_transformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText classification with Transformer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/text_classification_with_switch_transformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText classification with Switch Transformer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/tweet-classification-using-tfdf\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText classification using Decision Forests and pretrained embeddings\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/pretrained_word_embeddings\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tUsing pre-trained word embeddings\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/bidirectional_lstm_imdb\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tBidirectional LSTM on IMDB\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Machine translation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/neural_machine_translation_with_keras_nlp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tEnglish-to-Spanish translation with KerasNLP\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/neural_machine_translation_with_transformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tEnglish-to-Spanish translation with a sequence-to-sequence Transformer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/lstm_seq2seq\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tCharacter-level recurrent sequence-to-sequence model\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Entailment prediction</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/multimodal_entailment\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tMultimodal entailment\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Named entity recognition</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/ner_transformers\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tNamed Entity Recognition using Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Sequence-to-sequence</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/text_extraction_with_bert\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText Extraction with BERT\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/addition_rnn\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSequence to sequence learning for performing number addition\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Text similarity search</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/semantic_similarity_with_bert\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSemantic Similarity with BERT\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Language modeling</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/masked_language_modeling\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tEnd-to-end Masked Language Modeling with BERT\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/pretraining_BERT\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tPretraining BERT with Hugging Face Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Other</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/abstractive_summarization_with_bart\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tAbstractive Text Summarization with BART\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/data_parallel_training_with_keras_nlp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tData Parallel Training with KerasNLP\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/mlm_training_tpus\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTraining a language model from scratch with ð\\x9f¤\\x97 Transformers and TPUs\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tParameter-efficient fine-tuning of GPT-2 with LoRA\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/question_answering\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tQuestion Answering with Hugging Face Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/semantic_similarity_with_keras_nlp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSemantic Similarity with KerasNLP\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/sentence_embeddings_with_sbert\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tSentence embeddings using Siamese RoBERTa-networks\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/nlp/t5_hf_summarization\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tAbstractive Summarization with Hugging Face Transformers\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"structured-data\"><a href=\"/examples/structured_data/\">Structured Data</a></h2>\\n\\n\\t\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Structured data classification</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/structured_data_classification_with_feature_space\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tStructured data classification with FeatureSpace\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/imbalanced_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tImbalanced classification: credit card fraud detection\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/structured_data_classification_from_scratch\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tStructured data classification from scratch\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/wide_deep_cross_networks\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tStructured data learning with Wide, Deep, and Cross networks\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/classification_with_grn_and_vsn\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tClassification with Gated Residual and Variable Selection Networks\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/classification_with_tfdf\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tClassification with TensorFlow Decision Forests\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/deep_neural_decision_forests\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tClassification with Neural Decision Forests\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/tabtransformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tStructured data learning with TabTransformer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Recommendation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/collaborative_filtering_movielens\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tCollaborative Filtering for Movie Recommendations\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/movielens_recommendations_transformers\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tA Transformer-based recommendation system\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Other</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/structured_data/feature_space_advanced\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tFeatureSpace avanced use cases\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"timeseries\"><a href=\"/examples/timeseries/\">Timeseries</a></h2>\\n\\n\\t\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Timeseries classification</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/timeseries/timeseries_classification_from_scratch\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTimeseries classification from scratch\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/timeseries/timeseries_classification_transformer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTimeseries classification with a Transformer model\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/timeseries/eeg_signal_classification\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tElectroencephalogram Signal Classification for action identification\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Anomaly detection</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/timeseries/timeseries_anomaly_detection\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTimeseries anomaly detection using an Autoencoder\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Timeseries forecasting</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/timeseries/timeseries_traffic_forecasting\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTraffic forecasting using graph neural networks and LSTM\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/timeseries/timeseries_weather_forecasting\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTimeseries forecasting for weather prediction\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"generative-deep-learning\"><a href=\"/examples/generative/\">Generative Deep Learning</a></h2>\\n\\n\\t\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Image generation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/ddim\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDenoising Diffusion Implicit Models\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/random_walks_with_stable_diffusion\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tA walk through latent space with Stable Diffusion\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/dreambooth\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDreamBooth\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/ddpm\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDenoising Diffusion Probabilistic Models\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/fine_tune_via_textual_inversion\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tTeach StableDiffusion new concepts via Textual Inversion\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/finetune_stable_diffusion\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tFine-tuning Stable Diffusion\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/vae\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tVariational AutoEncoder\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/dcgan_overriding_train_step\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tGAN overriding Model.train_step\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/wgan_gp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tWGAN-GP overriding Model.train_step\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/conditional_gan\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tConditional GAN\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/cyclegan\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tCycleGAN\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/gan_ada\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tData-efficient GANs with Adaptive Discriminator Augmentation\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/deep_dream\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDeep Dream\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/gaugan\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tGauGAN for conditional image generation\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/pixelcnn\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tPixelCNN\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/stylegan\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tFace image generation with StyleGAN\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/vq_vae\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tVector-Quantized Variational Autoencoders\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Style transfer</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/neural_style_transfer\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tNeural style transfer\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/adain\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tNeural Style Transfer with AdaIN\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Text generation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/gpt2_text_generation_with_kerasnlp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-highlight\">â\\x98\\x85</div>\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tGPT2 Text Generation with KerasNLP\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/text_generation_gpt\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tGPT text generation from scratch with KerasNLP\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/text_generation_with_miniature_gpt\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText generation with a miniature GPT\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/lstm_character_level_text_generation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tCharacter-level text generation with LSTM\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/text_generation_fnet\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tText Generation using FNet\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Graph generation</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/molecule_generation\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDrug Molecule Generation with VAE\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/wgan-graphs\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tWGAN-GP with R-GCN for the generation of small molecular graphs\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\n\\t\\t\\t<h3 class=\"example-subcategory-title\">Other</h3>\\n\\n\\t\\t\\t\\n\\t\\t\\t\\t<a href=\"/examples/generative/real_nvp\">\\n\\t\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\t\\tDensity estimation using Real NVP\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</a>\\n\\t\\t\\t\\n\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"audio-data\"><a href=\"/examples/audio/\">Audio Data</a></h2>\\n\\n\\t\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/audio/ctc_asr\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tAutomatic Speech Recognition using CTC\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/audio/melgan_spectrogram_inversion\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tMelGAN-based spectrogram inversion using feature matching\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/audio/speaker_recognition_using_cnn\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tSpeaker Recognition\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/audio/transformer_asr\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tAutomatic Speech Recognition with Transformer\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/audio/uk_ireland_accent_recognition\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tEnglish speaker accent recognition using Transfer Learning\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/audio/wav2vec2_audiocls\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tAudio Classification with Hugging Face Transformers\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"reinforcement-learning\"><a href=\"/examples/rl/\">Reinforcement Learning</a></h2>\\n\\n\\t\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/rl/actor_critic_cartpole\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tActor Critic Method\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/rl/ddpg_pendulum\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tDeep Deterministic Policy Gradient (DDPG)\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/rl/deep_q_network_breakout\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tDeep Q-Learning for Atari Breakout\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/rl/ppo_cartpole\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tProximal Policy Optimization\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"graph-data\"><a href=\"/examples/graph/\">Graph Data</a></h2>\\n\\n\\t\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/graph/gat_node_classification\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tGraph attention network (GAT) for node classification\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/graph/gnn_citations\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tNode Classification with Graph Neural Networks\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/graph/mpnn-molecular-graphs\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tMessage-passing neural network (MPNN) for molecular property prediction\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/graph/node2vec_movielens\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tGraph representation learning with node2vec\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n\\t<h2 id=\"quick-keras-recipes\"><a href=\"/examples/keras_recipes/\">Quick Keras Recipes</a></h2>\\n\\n\\t\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/antirectifier\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tSimple custom layer example: Antirectifier\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/approximating_non_function_mappings\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tApproximating non-Function Mappings with Mixture Density Networks\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/bayesian_neural_networks\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tProbabilistic Bayesian Neural Networks\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/better_knowledge_distillation\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tKnowledge distillation recipes\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/creating_tfrecords\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tCreating TFRecords\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/debugging_tips\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tKeras debugging tips\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/endpoint_layer_pattern\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tEndpoint layer pattern\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/memory_efficient_embeddings\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tMemory-efficient embeddings for recommendation systems\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/quasi_svm\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tA Quasi-SVM in Keras\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/reproducibility_recipes\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tReproducibility in Keras Models\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/sample_size_estimate\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tEstimating required sample size for model training\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/sklearn_metric_callbacks\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tEvaluating and exporting scikit-learn metrics in a Keras callback\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/subclassing_conv_layers\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tCustomizing the convolution operation of a Conv2D layer\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/tensorflow_numpy_models\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tWriting Keras Models With TensorFlow NumPy\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/tf_serving\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tServing TensorFlow models with TFServing\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/tfrecord\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tHow to train a Keras model on TFRecord files\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\t\\t<a href=\"/examples/keras_recipes/trainer_pattern\">\\n\\t\\t\\t\\t<div class=\"example-card\">\\n\\t\\t\\t\\t\\t<div class=\"example-card-title\">\\n\\t\\t\\t\\t\\t\\tTrainer pattern\\n\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</a>\\n\\t\\t\\n\\t\\n\\n\\t<hr>\\n\\n<h2 id=\"adding-a-new-code-example\">Adding a new code example</h2>\\n<p>We welcome new code examples! Here are our rules:</p>\\n<ul>\\n<li>They should be shorter than 300 lines of code (comments may be as long as you want).</li>\\n<li>They should demonstrate modern Keras / TensorFlow 2 best practices.</li>\\n<li>They should be substantially different in topic from all examples listed above.</li>\\n<li>They should be extensively documented &amp; commented.</li>\\n</ul>\\n<p>New examples are added via Pull Requests to the <a href=\"https://github.com/keras-team/keras-io\">keras.io repository</a>.\\nThey must be submitted as a <code>.py</code> file that follows a specific format. They are usually generated from Jupyter notebooks.\\nSee the <a href=\"https://github.com/keras-team/keras-io/blob/master/README.md\"><code>tutobooks</code> documentation</a> for more details.</p>\\n        </div>\\n        \\n        <div class=\\'k-outline\\'>\\n          \\n            <div class=\\'k-outline-depth-1\\'>\\n              \\n              <a href=\\'#code-examples\\'>Code examples</a>\\n            </div>\\n          \\n            <div class=\\'k-outline-depth-2\\'>\\n               â\\x97\\x86  \\n              <a href=\\'#adding-a-new-code-example\\'>Adding a new code example</a>\\n            </div>\\n          \\n        </div>\\n        \\n      </div>\\n    </div>\\n\\n  </div>\\n\\n</body>\\n\\n<footer style=\"float: left; width: 100%; padding: 1em; border-top: solid 1px #bbb;\">\\n  <a href=\"https://policies.google.com/terms\">Terms</a> | <a href=\"https://policies.google.com/privacy\">Privacy</a>\\n</footer>\\n\\n</html>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://keras.io/examples/\"\n",
    "\n",
    "website_contents = scrape_website(url)\n",
    "website_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://keras.io/img/favicon.ico\">',\n",
       " 'https://keras.io/img/logo-k-keras-wb.png\">',\n",
       " 'https://keras.io/img/k-keras-social.png\">',\n",
       " 'https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600;700;800&display=swap\"',\n",
       " \"https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);\",\n",
       " \"https://www.google-analytics.com/analytics.js','ga');\",\n",
       " 'https://buttons.github.io/buttons.js\"></script>',\n",
       " 'https://www.googletagmanager.com/ns.html?id=GTM-5DNGF4N\"',\n",
       " 'https://colab.research.google.com/notebooks/welcome.ipynb\">Google',\n",
       " 'https://github.com/keras-team/keras-io\">keras.io',\n",
       " 'https://github.com/keras-team/keras-io/blob/master/README.md\"><code>tutobooks</code>',\n",
       " 'https://policies.google.com/terms\">Terms</a>',\n",
       " 'https://policies.google.com/privacy\">Privacy</a>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all the links from the page\n",
    "links = extract_clean_urls(website_contents)\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def get_num_tokens(prompt):\n",
    "    \n",
    "    enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo-16k-0613\")\n",
    "\n",
    "    return len(enc.encode(prompt))\n",
    "\n",
    "\n",
    "get_num_tokens(website_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Image classification\"\n",
    "prompt = f\"GIven the contents of a webpage shown below: {website_contents}, extract all the relevant urls related to this topic: '{topic}'. The output should ONLY be a Python list with the complete urls containing the domain, for example: ['https://domain.com/example/nested/', ....]. Output:\"\n",
    "\n",
    "urls = get_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://keras.io/examples/vision/image_classification_from_scratch',\n",
       " 'https://keras.io/examples/vision/mnist_convnet',\n",
       " 'https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning',\n",
       " 'https://keras.io/examples/vision/image_classification_with_vision_transformer',\n",
       " 'https://keras.io/examples/vision/bit',\n",
       " 'https://keras.io/examples/vision/attention_mil_classification',\n",
       " 'https://keras.io/examples/vision/mlp_image_classification',\n",
       " 'https://keras.io/examples/vision/mobilevit',\n",
       " 'https://keras.io/examples/vision/xray_classification_with_tpus',\n",
       " 'https://keras.io/examples/vision/cct',\n",
       " 'https://keras.io/examples/vision/convmixer',\n",
       " 'https://keras.io/examples/vision/eanet',\n",
       " 'https://keras.io/examples/vision/involution',\n",
       " 'https://keras.io/examples/vision/perceiver_image_classification',\n",
       " 'https://keras.io/examples/vision/reptile',\n",
       " 'https://keras.io/examples/vision/semisupervised_simclr',\n",
       " 'https://keras.io/examples/vision/swin_transformers',\n",
       " 'https://keras.io/examples/vision/vit_small_ds',\n",
       " 'https://keras.io/examples/vision/shiftvit']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "urls = ast.literal_eval(urls)\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for each of these urls, we would like to search for information related to the problem or topic we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contents of this url are too long to be processed by GPT-3. Please try another url.\n",
      "Answer:\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for url in urls:\n",
    "    url_contents = scrape_website(url)\n",
    "    prompt2 = f\"Given this topic im searching for: '{topic}', is there relevant information in this url: {url} whose contents are below: '''{url_contents}'''? The output should ONLY be 'True' or 'False', nothing else. Output:\"\n",
    "    if get_num_tokens(url_contents) > 16385:\n",
    "        print(\"The contents of this url are too long to be processed by GPT-3. Please try another url.\")\n",
    "    else:\n",
    "        response = get_response(prompt2)\n",
    "        print(\"Answer:\")\n",
    "        print(response)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://keras.io/examples/vision/mnist_convnet'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an resource url containing relevant information we can prompt ChatGPT to extract what we want from that url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure! The relevant code for \\'Image classification\\' is as follows:\\n\\n```python\\nimport numpy as np\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\n\\n# Model / data parameters\\nnum_classes = 10\\ninput_shape = (28, 28, 1)\\n\\n# Load the data and split it between train and test sets\\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\\n\\n# Scale images to the [0, 1] range\\nx_train = x_train.astype(\"float32\") / 255\\nx_test = x_test.astype(\"float32\") / 255\\n\\n# Make sure images have shape (28, 28, 1)\\nx_train = np.expand_dims(x_train, -1)\\nx_test = np.expand_dims(x_test, -1)\\nprint(\"x_train shape:\", x_train.shape)\\nprint(x_train.shape[0], \"train samples\")\\nprint(x_test.shape[0], \"test samples\")\\n\\n# convert class vectors to binary class matrices\\ny_train = keras.utils.to_categorical(y_train, num_classes)\\ny_test = keras.utils.to_categorical(y_test, num_classes)\\n\\nmodel = keras.Sequential(\\n    [\\n        keras.Input(shape=input_shape),\\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Flatten(),\\n        layers.Dropout(0.5),\\n        layers.Dense(num_classes, activation=\"softmax\"),\\n    ]\\n)\\n\\nmodel.summary()\\n\\nbatch_size = 128\\nepochs = 15\\n\\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\\n\\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\\n\\nscore = model.evaluate(x_test, y_test, verbose=0)\\nprint(\"Test loss:\", score[0])\\nprint(\"Test accuracy:\", score[1])\\n```\\n\\nThis code loads the MNIST dataset, preprocesses the data, builds a convolutional neural network model for image classification, trains the model, and evaluates its performance on the test set.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_contents = scrape_website(url)\n",
    "prompt3 = f\"Extract the relevant code for '{topic}' from the following contents: {url_contents}.\"\n",
    "final_output = get_response(prompt3)\n",
    "\n",
    "final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Final output example:\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1]) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First level automation with langchain would be to use PromptTemplates and OutputParsers to organize this code into a more cohesive tool\n",
    "in this case we would join everything into a chain that we could use to perform the actions we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://keras.io/examples/vision/image_classification_from_scratch', 'https://keras.io/examples/vision/mnist_convnet', 'https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning', 'https://keras.io/examples/vision/image_classification_with_vision_transformer', 'https://keras.io/examples/vision/bit', 'https://keras.io/examples/vision/attention_mil_classification', 'https://keras.io/examples/vision/mlp_image_classification', 'https://keras.io/examples/vision/mobilevit', 'https://keras.io/examples/vision/xray_classification_with_tpus', 'https://keras.io/examples/vision/cct', 'https://keras.io/examples/vision/convmixer', 'https://keras.io/examples/vision/eanet', 'https://keras.io/examples/vision/involution', 'https://keras.io/examples/vision/perceiver_image_classification', 'https://keras.io/examples/vision/reptile', 'https://keras.io/examples/vision/semisupervised_simclr', 'https://keras.io/examples/vision/swin_transformers', 'https://keras.io/examples/vision/vit_small_ds', 'https://keras.io/examples/vision/shiftvit']\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Lets run this first prompt inside a chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "topic = \"Image classification\"\n",
    "#prompt = f\"Given the contents of a webpage shown below: {website_contents}, extract all the relevant urls related to this topic: '{topic}'. The output should ONLY be a Python list with the complete urls containing the domain, for example: ['https://domain.com/example/nested/', ....]. Output:\"\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo-16k-0613\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"website_contents\", \"topic\"],\n",
    "    template=\"Given the contents of a webpage shown below: {website_contents}, extract all the relevant urls related to this topic: '{topic}'. The output should ONLY be a Python list with the complete urls containing the domain, for example: ['https://domain.com/example/nested/', ....]. Output:\",\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm, prompt=prompt)\n",
    "# Run the chain only specifying the input variable.\n",
    "urls = chain1.run({\"website_contents\": website_contents,\"topic\": topic})\n",
    "urls = ast.literal_eval(urls)\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now that we seem to have succeessffuly extracted the urls, let's go to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "template = \"Given this topic im searching for: '{topic}', is there relevant information in this url: {url} whose contents are below: '''{website_contents}'''? The output should ONLY be 'True' or 'False', nothing else. Output:\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"url\", \"website_contents\"],\n",
    "    template=template,\n",
    "    \n",
    "    \n",
    ")\n",
    "chain2 = LLMChain(llm=llm, prompt=prompt2)\n",
    "topic = \"Image classification\"\n",
    "url = urls[1]\n",
    "url_contents = scrape_website(url)\n",
    "output = chain2.run({\"topic\": topic, \"url\": url, \"website_contents\": url_contents})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"''' \\n# Imports\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nfrom tensorflow import keras\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Data loading and preprocessing\\ndata = pd.read_csv('health_data.csv')\\nX = data.drop('target', axis=1)\\ny = data['target']\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\nscaler = StandardScaler()\\nX_train_scaled = scaler.fit_transform(X_train)\\nX_test_scaled = scaler.transform(X_test)\\n\\n# Model architecture\\nmodel = keras.Sequential([\\n    keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\\n    keras.layers.Dense(32, activation='relu'),\\n    keras.layers.Dense(1, activation='sigmoid')\\n])\\n\\n# Model compilation\\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\n\\n# Model training\\nmodel.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\\n\\n# Model evaluation\\nloss, accuracy = model.evaluate(X_test_scaled, y_test)\\nprint('Test loss:', loss)\\nprint('Test accuracy:', accuracy)\\n\\n# Model saving\\nmodel.save('health_model.h5')\\n'''\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"Given the contents of a webpage shown below: {website_contents}, extract the relevant formatted code related to this topic: '{topic}'.\\\n",
    "    The output should ONLY be Python code formatted as such:\\\n",
    "        '''\\\n",
    "        <imports>\\\n",
    "            Imports go here\\\n",
    "        <imports>\\\n",
    "        <Data loading and preprocessing>\\\n",
    "            Data code goes here\\\n",
    "        <Data loading and preprocessing>\\\n",
    "        <training and evaluation>\\\n",
    "            training and eval code go here\\\n",
    "        <training and evaluation>\\\n",
    "        <visualization>\\\n",
    "            viz code goes here\\\n",
    "        <visualization>\\\n",
    "        <inference code>\\\n",
    "            inference code goes here\\\n",
    "        <inference code>\\\n",
    "        <model saving code>\\\n",
    "            model saving code goes here\\\n",
    "        <model saving code>\\\n",
    "        '''\\\n",
    "    In the case where the code is not found or irrelevant for specific parts, just leave a blank there.\\\n",
    "    Output:\\\n",
    "        Code:\\n\"\n",
    "\n",
    "prompt3 = PromptTemplate(\n",
    "    input_variables=[\"website_contents\", \"topic\"],\n",
    "    template=template,\n",
    ")\n",
    "chain3 = LLMChain(llm=llm, prompt=prompt3)\n",
    "chain3.run({\"website_contents\": website_contents, \"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.base import Chain\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class ConcatenateChain(Chain):\n",
    "    chain_1: LLMChain\n",
    "    chain_2: LLMChain\n",
    "    chain_3: LLMChain\n",
    "    \n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        all_input_vars = set(self.chain_1.input_keys)\n",
    "        return list(all_input_vars)\n",
    "    \n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['final_output']\n",
    "    \n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        output1 = self.chain_1.run(inputs)\n",
    "        output1 = ast.literal_eval(output1)\n",
    "        assert type(output1)==list\n",
    "        for url in output1:\n",
    "            print(\"Inside loop!\")\n",
    "            print(\"Processing this url: \", url)\n",
    "            website_contents = scrape_website(url)\n",
    "            output2 = self.chain_2.run({\"topic\": topic, \"url\": url, \"website_contents\": website_contents})\n",
    "            print(\"Output2: \", output2)\n",
    "            if \"False\" in output2:\n",
    "                continue\n",
    "            elif \"True\" in output2 or \"Yes\" in output2[:4]:\n",
    "                print(\"Inside the True part\")\n",
    "                output3 = self.chain_3.run({\"website_contents\": website_contents, \"topic\": topic})\n",
    "                print(\"Relevant URL: \", url)\n",
    "                return {\"final_output\": output3}\n",
    "            else:\n",
    "                print(\"Nor false or true in the output.\")\n",
    "                output3 = \"\"\n",
    "        \n",
    "        return {\"final_output\": \"No relevant urls found for this topic\"}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside loop!\n",
      "Processing this url:  https://keras.io/examples/vision/xray_classification_with_tpus\n",
      "Output2:  Yes, there is relevant information in the given URL. The URL provides a code example for building an X-ray image classification model to predict whether an X-ray scan shows the presence of pneumonia.\n",
      "Inside the True part\n",
      "Relevant URL:  https://keras.io/examples/vision/xray_classification_with_tpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```python\\nimport re\\nimport os\\nimport random\\nimport numpy as np\\nimport pandas as pd\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\n\\ntry:\\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\\n    print(\"Device:\", tpu.master())\\n    strategy = tf.distribute.TPUStrategy(tpu)\\nexcept:\\n    strategy = tf.distribute.get_strategy()\\nprint(\"Number of replicas:\", strategy.num_replicas_in_sync)\\n\\nAUTOTUNE = tf.data.AUTOTUNE\\nBATCH_SIZE = 25 * strategy.num_replicas_in_sync\\nIMAGE_SIZE = [180, 180]\\nCLASS_NAMES = [\"NORMAL\", \"PNEUMONIA\"]\\n\\ntrain_images = tf.data.TFRecordDataset(\\n    \"gs://download.tensorflow.org/data/ChestXRay2017/train/images.tfrec\"\\n)\\ntrain_paths = tf.data.TFRecordDataset(\\n    \"gs://download.tensorflow.org/data/ChestXRay2017/train/paths.tfrec\"\\n)\\n\\nds = tf.data.Dataset.zip((train_images, train_paths))\\n\\n\\ndef get_label(file_path):\\n    parts = tf.strings.split(file_path, \"/\")\\n    return parts[-2] == \"PNEUMONIA\"\\n\\n\\ndef decode_img(img):\\n    img = tf.image.decode_jpeg(img, channels=3)\\n    return tf.image.resize(img, IMAGE_SIZE)\\n\\n\\ndef process_path(image, path):\\n    label = get_label(path)\\n    img = decode_img(image)\\n    return img, label\\n\\n\\nds = ds.map(process_path, num_parallel_calls=AUTOTUNE)\\n\\nCOUNT_NORMAL = len(\\n    [\\n        filename\\n        for filename in train_paths\\n        if \"NORMAL\" in filename.numpy().decode(\"utf-8\")\\n    ]\\n)\\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\\n\\nCOUNT_PNEUMONIA = len(\\n    [\\n        filename\\n        for filename in train_paths\\n        if \"PNEUMONIA\" in filename.numpy().decode(\"utf-8\")\\n    ]\\n)\\nprint(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))\\n\\nds = ds.shuffle(10000)\\ntrain_ds = ds.take(4200)\\nval_ds = ds.skip(4200)\\n\\nfor image, label in train_ds.take(1):\\n    print(\"Image shape: \", image.numpy().shape)\\n    print(\"Label: \", label.numpy())\\n\\ntest_images = tf.data.TFRecordDataset(\\n    \"gs://download.tensorflow.org/data/ChestXRay2017/test/images.tfrec\"\\n)\\ntest_paths = tf.data.TFRecordDataset(\\n    \"gs://download.tensorflow.org/data/ChestXRay2017/test/paths.tfrec\"\\n)\\ntest_ds = tf.data.Dataset.zip((test_images, test_paths))\\n\\ntest_ds = test_ds.map(process_path, num_parallel_calls=AUTOTUNE)\\ntest_ds = test_ds.batch(BATCH_SIZE)\\n\\n\\ndef prepare_for_training(ds, cache=True):\\n    if cache:\\n        if isinstance(cache, str):\\n            ds = ds.cache(cache)\\n        else:\\n            ds = ds.cache()\\n\\n    ds = ds.batch(BATCH_SIZE)\\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\\n\\n    return ds\\n\\n\\ntrain_ds = prepare_for_training(train_ds)\\nval_ds = prepare_for_training(val_ds)\\n\\nimage_batch, label_batch = next(iter(train_ds))\\n\\ndef show_batch(image_batch, label_batch):\\n    plt.figure(figsize=(10, 10))\\n    for n in range(25):\\n        ax = plt.subplot(5, 5, n + 1)\\n        plt.imshow(image_batch[n] / 255)\\n        if label_batch[n]:\\n            plt.title(\"PNEUMONIA\")\\n        else:\\n            plt.title(\"NORMAL\")\\n        plt.axis(\"off\")\\n\\nshow_batch(image_batch.numpy(), label_batch.numpy())\\n```'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_chain = ConcatenateChain(chain_1=chain1, chain_2=chain2, chain_3=chain3)\n",
    "topic = \"Pneumonia classifier\"\n",
    "url = \"https://keras.io/examples/\"\n",
    "url_contents = scrape_website(url)\n",
    "concat_chain.run({\"website_contents\": url_contents, \"topic\": topic, \"url\": url})\n",
    "#chain1.run({\"website_contents\": url_contents,\"topic\": topic})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_agents",
   "language": "python",
   "name": "langchain_agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
