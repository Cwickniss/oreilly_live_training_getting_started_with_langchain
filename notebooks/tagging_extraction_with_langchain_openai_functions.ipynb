{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the coolest examples of stuff you can do with LangChain is tagging and extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets-resources/tagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's invert stuff a bit and let's work through an example before diving into any details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOr this example imagine you have a github repository that you want to test it out.\n",
    "\n",
    "Usually you would go to that repo and read through the instalation section to see which commands you have to run in the terminal to set up that repo locally. How about using openai functinos to extract the exact list of commands needed to set that up?\n",
    "\n",
    "This way we could later pass that into some model that can do that for us!\n",
    "\n",
    "Let's try that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Call this function to extract the terminal commands to set up a given github repository\"\"\"\n",
    "    commands: List[str] = Field(description=\"URL to the GitHub repo\")\n",
    "\n",
    "\n",
    "tagging_function = convert_pydantic_to_openai_function(Tagging)\n",
    "\n",
    "functions = [tagging_function]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "\n",
    "llm_chat_with_tagging_function = llm_chat.bind(functions=functions, function_call={\"name\": \"Tagging\"})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{input_document}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need some other function to read through the github repository content.\n",
    "We can use LangChain's `WebBaseLoader` to load the text from a webpage like a github repo page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "url = \"https://github.com/ml-explore/mlx#installation\"\n",
    "\n",
    "docs = WebBaseLoader(url).load()    \n",
    "\n",
    "# getting the contents of the extracted page document\n",
    "document = docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's inspect this `docs` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Markdown\n",
    "# Markdown(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use our tagging function to extract the commands from the document by feeding that document into a chain that uses the function we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"commands\": [\\n    \"git clone https://github.com/ml-explore/mlx.git\",\\n    \"cd mlx\"\\n  ]\\n}', 'name': 'Tagging'}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_chain = prompt | llm_chat_with_tagging_function\n",
    "output_commands = tagging_chain.invoke({\"input_document\": document})\n",
    "output_commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it! THe correct commands, properly organized in a list we can easily call. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at extraction which is similar to tagging but for more complex extraction scenarios where we want to extract multiple pieces of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
