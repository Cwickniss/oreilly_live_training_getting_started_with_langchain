{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading, retrieval methods and text splitting\n",
    "%pip install -qU langchain langchain_community\n",
    "\n",
    "# Local vector store via Chroma\n",
    "%pip install -qU langchain_chroma\n",
    "\n",
    "# Local inference and embeddings via Ollama\n",
    "%pip install -qU langchain_ollama\n",
    "\n",
    "# Web Loader\n",
    "%pip install -qU beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "links = [\"https://python.langchain.com/docs/how_to/structured_output/\",\n",
    "         \"https://python.langchain.com/docs/how_to/tool_calling/\",\n",
    "         \"https://python.langchain.com/docs/how_to/few_shot_examples/\",\n",
    "         \"https://python.langchain.com/docs/how_to/prompts_composition/\",\n",
    "         \"https://python.langchain.com/docs/how_to/functions/\",\n",
    "         \"https://python.langchain.com/docs/how_to/parallel/\",\n",
    "         \"https://python.langchain.com/docs/how_to/sequence/\",\n",
    "         \"https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel\",\n",
    "         \"https://python.langchain.com/docs/how_to/installation/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_markdown/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_json/\",      \n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_pdf/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_web/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_csv/\",\n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_directory/\", \n",
    "         \"https://python.langchain.com/docs/how_to/document_loader_html/\",\n",
    "         \"https://python.langchain.com/docs/tutorials/rag/\"\n",
    "         ]\n",
    "\n",
    "loader = WebBaseLoader(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to return structured data from a model | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nFunction/tool calling'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content=\"It is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\\nThe .with_structured_output() method\\u200b\\n\\nSupported modelsYou can find a list of models that support this method here.\\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or Messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\\nAs an example, let's get a model to generate a joke and separate the setup from the punchline:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='OpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nPydantic class\\u200b\\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7)\\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model\\'s function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\\nTypedDict or JSON Schema\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content=\"TypedDict or JSON Schema\\u200b\\nIf you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\\nRequirements\\nCore: langchain-core>=0.2.26\\nTyping extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nChoosing between multiple schemas\\u200b\\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute:\\nfrom typing import Union# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user\\'s query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\nFinalResponse(final_output=Joke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7))\\nstructured_llm.invoke(\"How are you today?\")\\nFinalResponse(final_output=ConversationalResponse(response=\"I\\'m just a bunch of code, so I don\\'t have feelings, but I\\'m here and ready to help you! How can I assist you today?\"))\\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don\\'t have to use nested schemas. See this how-to guide for more details.\\nStreaming\\u200b\\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\\ninfoNote that what\\'s yielded is already aggregated chunks, not deltas.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='{}{\\'setup\\': \\'\\'}{\\'setup\\': \\'Why\\'}{\\'setup\\': \\'Why was\\'}{\\'setup\\': \\'Why was the\\'}{\\'setup\\': \\'Why was the cat\\'}{\\'setup\\': \\'Why was the cat sitting\\'}{\\'setup\\': \\'Why was the cat sitting on\\'}{\\'setup\\': \\'Why was the cat sitting on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nFew-shot prompting\\u200b\\nFor more complex schemas it\\'s very useful to add few-shot examples to the prompt. This can be done in a few ways.\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don\\'t planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what\\'s something funny about woodpeckers\")API Reference:ChatPromptTemplate\\n{\\'setup\\': \\'Woodpecker\\', \\'punchline\\': \"Woodpecker who? Woodpecker who can\\'t find a tree is just a bird with a headache!\", \\'rating\\': 7}\\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you\\'re using makes use of tool calling in its API reference.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don\\'t planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") \\\\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\\n{\\'setup\\': \\'Crocodile\\', \\'punchline\\': \\'Crocodile be seeing you later, alligator!\\', \\'rating\\': 7}\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\\nJSON modeIf using JSON mode you\\'ll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you\\'re using supports JSON mode, check its entry in the API reference.\\nstructured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}\\n(Advanced) Raw outputs\\u200b\\nLLMs aren\\'t perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='structured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'function\\': {\\'arguments\\': \\'{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}\\', \\'name\\': \\'Joke\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 33, \\'prompt_tokens\\': 93, \\'total_tokens\\': 126}, \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_4e2b2da518\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0\\', tool_calls=[{\\'name\\': \\'Joke\\', \\'args\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}, \\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 93, \\'output_tokens\\': 33, \\'total_tokens\\': 126}), \\'parsed\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}, \\'parsing_error\\': None}\\nPrompting and parsing model outputs directly\\u200b\\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you\\'ll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\\nUsing PydanticOutputParser\\u200b\\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\\nLet‚Äôs take a look at what information is sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke(query).to_string())\\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\\\`\\\\`\\\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\\\`\\\\`\\\\`Human: Anna is 23 years old and she is 6 feet tall'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='And now let\\'s invoke it:\\nchain = prompt | llm | parserchain.invoke({\"query\": query})\\nPeople(people=[Person(name=\\'Anna\\', height_in_meters=1.8288)])\\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\\nCustom Parsing\\u200b\\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\\\`\\\\`\\\\`json\\\\n{schema}\\\\n\\\\`\\\\`\\\\`. \"            \"Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\\\`\\\\`\\\\`json(.*?)\\\\`\\\\`\\\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\\\`\\\\`\\\\`json{\\'title\\': \\'People\\', \\'description\\': \\'Identifying information about all people in a text.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'people\\': {\\'title\\': \\'People\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/definitions/Person\\'}}}, \\'required\\': [\\'people\\'], \\'definitions\\': {\\'Person\\': {\\'title\\': \\'Person\\', \\'description\\': \\'Information about a person.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'description\\': \\'The name of the person\\', \\'type\\': \\'string\\'}, \\'height_in_meters\\': {\\'title\\': \\'Height In Meters\\', \\'description\\': \\'The height of the person expressed in meters.\\', \\'type\\': \\'number\\'}}, \\'required\\': [\\'name\\', \\'height_in_meters\\']}}}\\\\`\\\\`\\\\`. Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\\nAnd here\\'s what it looks like when we invoke it:\\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\\n[{\\'people\\': [{\\'name\\': \\'Anna\\', \\'height_in_meters\\': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to use chat models to call tools | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use chat models to call toolsOn this pageHow to use chat models to call tools'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nTool calling\\nTools\\nOutput parsers'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Tool calling allows a chat model to respond to a given prompt by \"calling a tool\".\\nRemember, while the name \"tool calling\" implies that the model is directly performing some action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\\nTool calling is a general technique that generates structured output from a model, and you can use it even when you don\\'t intend to invoke any tools. An example use-case of that is extraction from unstructured text.\\n\\nIf you want to see how to use the model-generated tool call to actually run a tool check out this guide.\\nSupported modelsTool calling is not universal, but is supported by many popular LLM providers. You can find a list of all models that support tool calling here.\\nLangChain implements standard interfaces for defining tools, passing them to LLMs, and representing tool calls.\\nThis guide will cover how to bind tools to an LLM, then invoke the LLM to generate these arguments.\\nDefining tool schemas\\u200b\\nFor a model to be able to call tools, we need to pass in tool schemas that describe what the tool does and what it\\'s arguments are. Chat models that support tool calling features implement a .bind_tools() method for passing tool schemas to the model. Tool schemas can be passed in as Python functions (with typehints and docstrings), Pydantic models, TypedDict classes, or LangChain Tool objects. Subsequent invocations of the model will pass in these tool schemas along with the prompt.\\nPython functions\\u200b\\nOur tool schemas can be Python functions:\\n# The function name, type hints, and docstring are all part of the tool# schema that\\'s passed to the model. Defining good, descriptive schemas# is an extension of prompt engineering and is an important part of# getting models to perform well.def add(a: int, b: int) -> int:    \"\"\"Add two integers.    Args:        a: First integer        b: Second integer    \"\"\"    return a + bdef multiply(a: int, b: int) -> int:    \"\"\"Multiply two integers.    Args:        a: First integer        b: Second integer    \"\"\"    return a * b\\nLangChain Tool\\u200b\\nLangChain also implements a @tool decorator that allows for further control of the tool schema, such as tool names and argument descriptions. See the how-to guide here for details.\\nPydantic class\\u200b\\nYou can equivalently define the schemas without the accompanying functions using Pydantic.\\nNote that all fields are required unless provided a default value.\\nfrom pydantic import BaseModel, Fieldclass add(BaseModel):    \"\"\"Add two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class multiply(BaseModel):    \"\"\"Multiply two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")\\nTypedDict class\\u200b\\nRequires langchain-core>=0.2.25\\nOr using TypedDicts and annotations:\\nfrom typing_extensions import Annotated, TypedDictclass add(TypedDict):    \"\"\"Add two integers.\"\"\"    # Annotations must have the type and can optionally include a default value and description (in that order).    a: Annotated[int, ..., \"First integer\"]    b: Annotated[int, ..., \"Second integer\"]class multiply(TypedDict):    \"\"\"Multiply two integers.\"\"\"    a: Annotated[int, ..., \"First integer\"]    b: Annotated[int, ..., \"Second integer\"]tools = [add, multiply]\\nTo actually bind those schemas to a chat model, we\\'ll use the .bind_tools() method. This handles converting\\nthe add and multiply schemas to the proper format for the model. The tool schema will then be passed it in each time the model is invoked.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='OpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/firefunction-v1\", temperature=0)pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nllm_with_tools = llm.bind_tools(tools)query = \"What is 3 * 12?\"llm_with_tools.invoke(query)\\nAIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_iXj4DiW1p7WLjTAQMRO0jxMs\\', \\'function\\': {\\'arguments\\': \\'{\"a\":3,\"b\":12}\\', \\'name\\': \\'multiply\\'}, \\'type\\': \\'function\\'}], \\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 17, \\'prompt_tokens\\': 80, \\'total_tokens\\': 97}, \\'model_name\\': \\'gpt-4o-mini-2024-07-18\\', \\'system_fingerprint\\': \\'fp_483d39d857\\', \\'finish_reason\\': \\'tool_calls\\', \\'logprobs\\': None}, id=\\'run-0b620986-3f62-4df7-9ba3-4595089f9ad4-0\\', tool_calls=[{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 3, \\'b\\': 12}, \\'id\\': \\'call_iXj4DiW1p7WLjTAQMRO0jxMs\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 80, \\'output_tokens\\': 17, \\'total_tokens\\': 97})\\nAs we can see our LLM generated arguments to a tool! You can look at the docs for bind_tools() to learn about all the ways to customize how your LLM selects tools, as well as this guide on how to force the LLM to call a tool rather than letting it decide.\\nTool calls\\u200b\\nIf tool calls are included in a LLM response, they are attached to the corresponding\\nmessage\\nor message chunk\\nas a list of tool call\\nobjects in the .tool_calls attribute.\\nNote that chat models can call multiple tools at once.\\nA ToolCall is a typed dict that includes a\\ntool name, dict of argument values, and (optionally) an identifier. Messages with no\\ntool calls default to an empty list for this attribute.\\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"llm_with_tools.invoke(query).tool_calls'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='tool calls default to an empty list for this attribute.\\nquery = \"What is 3 * 12? Also, what is 11 + 49?\"llm_with_tools.invoke(query).tool_calls\\n[{\\'name\\': \\'multiply\\',  \\'args\\': {\\'a\\': 3, \\'b\\': 12},  \\'id\\': \\'call_1fyhJAbJHuKQe6n0PacubGsL\\',  \\'type\\': \\'tool_call\\'}, {\\'name\\': \\'add\\',  \\'args\\': {\\'a\\': 11, \\'b\\': 49},  \\'id\\': \\'call_fc2jVkKzwuPWyU7kS9qn1hyG\\',  \\'type\\': \\'tool_call\\'}]\\nThe .tool_calls attribute should contain valid tool calls. Note that on occasion,\\nmodel providers may output malformed tool calls (e.g., arguments that are not\\nvalid JSON). When parsing fails in these cases, instances\\nof InvalidToolCall\\nare populated in the .invalid_tool_calls attribute. An InvalidToolCall can have\\na name, string arguments, identifier, and error message.\\nParsing\\u200b\\nIf desired, output parsers can further process the output. For example, we can convert existing values populated on the .tool_calls to Pydantic objects using the\\nPydanticToolsParser:\\nfrom langchain_core.output_parsers import PydanticToolsParserfrom pydantic import BaseModel, Fieldclass add(BaseModel):    \"\"\"Add two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")class multiply(BaseModel):    \"\"\"Multiply two integers.\"\"\"    a: int = Field(..., description=\"First integer\")    b: int = Field(..., description=\"Second integer\")chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])chain.invoke(query)API Reference:PydanticToolsParser\\n[multiply(a=3, b=12), add(a=11, b=49)]\\nNext steps\\u200b\\nNow you\\'ve learned how to bind tool schemas to a chat model and have the model call the tool.\\nNext, check out this guide on actually using the tool by invoking the function and passing the results back to the model:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/tool_calling/', 'title': 'How to use chat models to call tools | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Pass tool results back to model\\n\\nYou can also check out some more specific uses of tool calling:\\n\\nGetting structured outputs from models\\nFew shot prompting with tools\\nStream tool calls\\nPass runtime values to tools\\nEdit this pageWas this page helpful?PreviousHow to return artifacts from a toolNextHow to disable parallel tool callingDefining tool schemasPython functionsLangChain ToolPydantic classTypedDict classTool callsParsingNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='In this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Examples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to compose prompts together | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to compose prompts togetherOn this pageHow to compose prompts together'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='LangChain provides a user friendly interface for composing different parts of prompts together. You can do this with either string prompts or chat prompts. Constructing prompts this way allows for easy reuse of components.\\nString prompt composition\\u200b\\nWhen working with string prompts, each template is joined together. You can work with either prompts directly or strings (the first element in the list needs to be a prompt).\\nfrom langchain_core.prompts import PromptTemplateprompt = (    PromptTemplate.from_template(\"Tell me a joke about {topic}\")    + \", make it funny\"    + \"\\\\n\\\\nand in {language}\")promptAPI Reference:PromptTemplate\\nPromptTemplate(input_variables=[\\'language\\', \\'topic\\'], template=\\'Tell me a joke about {topic}, make it funny\\\\n\\\\nand in {language}\\')\\nprompt.format(topic=\"sports\", language=\"spanish\")\\n\\'Tell me a joke about sports, make it funny\\\\n\\\\nand in spanish\\'\\nChat prompt composition\\u200b\\nA chat prompt is made up a of a list of messages. Similarly to the above example, we can concatenate chat prompt templates. Each new element is a new message in the final prompt.\\nFirst, let\\'s initialize the a ChatPromptTemplate with a SystemMessage.\\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessageprompt = SystemMessage(content=\"You are a nice pirate\")API Reference:AIMessage | HumanMessage | SystemMessage\\nYou can then easily create a pipeline combining it with other messages or message templates.\\nUse a Message when there is no variables to be formatted, use a MessageTemplate when there are variables to be formatted. You can also use just a string (note: this will automatically get inferred as a HumanMessagePromptTemplate.)\\nnew_prompt = (    prompt + HumanMessage(content=\"hi\") + AIMessage(content=\"what?\") + \"{input}\")\\nUnder the hood, this creates an instance of the ChatPromptTemplate class, so you can use it just as you did before!\\nnew_prompt.format_messages(input=\"i said hi\")\\n[SystemMessage(content=\\'You are a nice pirate\\'), HumanMessage(content=\\'hi\\'), AIMessage(content=\\'what?\\'), HumanMessage(content=\\'i said hi\\')]\\nUsing PipelinePrompt\\u200b\\nLangChain includes a class called PipelinePromptTemplate, which can be useful when you want to reuse parts of prompts. A PipelinePrompt consists of two main parts:\\n\\nFinal prompt: The final prompt that is returned\\nPipeline prompts: A list of tuples, consisting of a string name and a prompt template. Each prompt template will be formatted and then passed to future prompt templates as a variable with the same name.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/prompts_composition/', 'title': 'How to compose prompts together | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='from langchain_core.prompts import PipelinePromptTemplate, PromptTemplatefull_template = \"\"\"{introduction}{example}{start}\"\"\"full_prompt = PromptTemplate.from_template(full_template)introduction_template = \"\"\"You are impersonating {person}.\"\"\"introduction_prompt = PromptTemplate.from_template(introduction_template)example_template = \"\"\"Here\\'s an example of an interaction:Q: {example_q}A: {example_a}\"\"\"example_prompt = PromptTemplate.from_template(example_template)start_template = \"\"\"Now, do this for real!Q: {input}A:\"\"\"start_prompt = PromptTemplate.from_template(start_template)input_prompts = [    (\"introduction\", introduction_prompt),    (\"example\", example_prompt),    (\"start\", start_prompt),]pipeline_prompt = PipelinePromptTemplate(    final_prompt=full_prompt, pipeline_prompts=input_prompts)pipeline_prompt.input_variablesAPI Reference:PipelinePromptTemplate | PromptTemplate\\n[\\'person\\', \\'example_a\\', \\'example_q\\', \\'input\\']\\nprint(    pipeline_prompt.format(        person=\"Elon Musk\",        example_q=\"What\\'s your favorite car?\",        example_a=\"Tesla\",        input=\"What\\'s your favorite social media site?\",    ))\\nYou are impersonating Elon Musk.Here\\'s an example of an interaction:Q: What\\'s your favorite car?A: TeslaNow, do this for real!Q: What\\'s your favorite social media site?A:\\nNext steps\\u200b\\nYou\\'ve now learned how to compose prompts together.\\nNext, check out the other how-to guides on prompt templates in this section, like adding few-shot examples to your prompt templates.Edit this pageWas this page helpful?PreviousHow to pass through arguments from one step to the nextNextHow to handle multiple retrievers when doing query analysisString prompt compositionChat prompt compositionUsing PipelinePromptNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to run custom functionsOn this pageHow to run custom functions'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='You can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Using the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Passing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='implementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to invoke runnables in parallelOn this pageHow to invoke runnables in parallel'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='The RunnableParallel primitive is essentially a dict whose values are runnables (or things that can be coerced to runnables, like functions). It runs all of its values in parallel, and each value is called with the overall input of the RunnableParallel. The final return value is a dict with the results of each value under its appropriate key.\\nFormatting with RunnableParallels\\u200b\\nRunnableParallels are useful for parallelizing operations, but can also be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence. You can use them to split or fork the chain so that multiple components can process the input in parallel. Later, other components can join or merge the results to synthesize a final response. This type of chain creates a computation graph that looks like the following:\\n     Input      / \\\\     /   \\\\ Branch1 Branch2     \\\\   /      \\\\ /      Combine\\nBelow, the input to prompt is expected to be a map with keys \"context\" and \"question\". The user input is just the question. So we need to get the context using our retriever and passthrough the user input under the \"question\" key.\\nfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}\"\"\"# The prompt expects input with keys for \"context\" and \"question\"prompt = ChatPromptTemplate.from_template(template)model = ChatOpenAI()retrieval_chain = (    {\"context\": retriever, \"question\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())retrieval_chain.invoke(\"where did harrison work?\")API Reference:FAISS | StrOutputParser | ChatPromptTemplate | RunnablePassthrough | ChatOpenAI | OpenAIEmbeddings\\n\\'Harrison worked at Kensho.\\'\\ntipNote that when composing a RunnableParallel with another Runnable we don\\'t even need to wrap our dictionary in the RunnableParallel class ‚Äî\\xa0the type conversion is handled for us. In the context of a chain, these are equivalent:\\n{\"context\": retriever, \"question\": RunnablePassthrough()}\\nRunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\\nRunnableParallel(context=retriever, question=RunnablePassthrough())\\nSee the section on coercion for more.\\nUsing itemgetter as shorthand\\u200b\\nNote that you can use Python\\'s itemgetter as shorthand to extract data from the map when combining with RunnableParallel. You can find more information about itemgetter in the Python Documentation.\\nIn the example below, we use itemgetter to extract specific keys from the map:\\nfrom operator import itemgetterfrom langchain_community.vectorstores import FAISSfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import ChatOpenAI, OpenAIEmbeddingsvectorstore = FAISS.from_texts(    [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings())retriever = vectorstore.as_retriever()template = \"\"\"Answer the question based only on the following context:{context}Question: {question}Answer in the following language: {language}\"\"\"prompt = ChatPromptTemplate.from_template(template)chain = (    {        \"context\": itemgetter(\"question\") | retriever,        \"question\": itemgetter(\"question\"),        \"language\": itemgetter(\"language\"),    }    | prompt    | model    | StrOutputParser())chain.invoke({\"question\": \"where did harrison work\", \"language\": \"italian\"})API Reference:FAISS | StrOutputParser | ChatPromptTemplate | RunnablePassthrough | ChatOpenAI | OpenAIEmbeddings\\n\\'Harrison ha lavorato a Kensho.\\'\\nParallelize steps\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/parallel/', 'title': 'How to invoke runnables in parallel | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\\'Harrison ha lavorato a Kensho.\\'\\nParallelize steps\\u200b\\nRunnableParallels make it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableParallelfrom langchain_openai import ChatOpenAImodel = ChatOpenAI()joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | modelpoem_chain = (    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model)map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)map_chain.invoke({\"topic\": \"bear\"})API Reference:ChatPromptTemplate | RunnableParallel | ChatOpenAI\\n{\\'joke\\': AIMessage(content=\"Why don\\'t bears like fast food? Because they can\\'t catch it!\", response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 15, \\'prompt_tokens\\': 13, \\'total_tokens\\': 28}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_d9767fc5b9\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-fe024170-c251-4b7a-bfd4-64a3737c67f2-0\\'), \\'poem\\': AIMessage(content=\\'In the quiet of the forest, the bear roams free\\\\nMajestic and wild, a sight to see.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 24, \\'prompt_tokens\\': 15, \\'total_tokens\\': 39}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-2707913e-a743-4101-b6ec-840df4568a76-0\\')}\\nParallelism\\u200b\\nRunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two.\\n%%timeitjoke_chain.invoke({\"topic\": \"bear\"})\\n610 ms ¬± 64 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\\n%%timeitpoem_chain.invoke({\"topic\": \"bear\"})\\n599 ms ¬± 73.3 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\\n%%timeitmap_chain.invoke({\"topic\": \"bear\"})\\n643 ms ¬± 77.8 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\\nNext steps\\u200b\\nYou now know some ways to format and parallelize chain steps with RunnableParallel.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to add a semantic layer over graph databaseNextHow to stream chat model responsesFormatting with RunnableParallelsUsing itemgetter as shorthandParallelize stepsParallelismNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='How to chain runnables | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to chain runnablesOn this pageHow to chain runnables'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='PrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nPrompt templates\\nChat models\\nOutput parser'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='One point about LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. The output of the previous runnable\\'s .invoke() call is passed as input to the next runnable. This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.\\nThe resulting RunnableSequence is itself a runnable, which means it can be invoked, streamed, or further chained just like any other runnable. Advantages of chaining runnables in this way are efficient streaming (the sequence will stream output as soon as it is available), and debugging and tracing with tools like LangSmith.\\nThe pipe operator: |\\u200b\\nTo show off how this works, let\\'s go through an example. We\\'ll walk through a common pattern in LangChain: using a prompt template to format input into a chat model, and finally converting the chat message output into a string with an output parser.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='OpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAImodel = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAImodel = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoheremodel = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAmodel = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksmodel = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqmodel = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAImodel = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAImodel = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")chain = prompt | model | StrOutputParser()API Reference:StrOutputParser | ChatPromptTemplate\\nPrompts and models are both runnable, and the output type from the prompt call is the same as the input type of the chat model, so we can chain them together. We can then invoke the resulting sequence like any other runnable:\\nchain.invoke({\"topic\": \"bears\"})\\n\"Here\\'s a bear joke for you:\\\\n\\\\nWhy did the bear dissolve in water?\\\\nBecause it was a polar bear!\"\\nCoercion\\u200b\\nWe can even combine this chain with more runnables to create another chain. This may involve some input/output formatting using other types of runnables, depending on the required inputs and outputs of the chain components.\\nFor example, let\\'s say we wanted to compose the joke generating chain with another chain that evaluates whether or not the generated joke was funny.\\nWe would need to be careful with how we format the input into the next chain. In the below example, the dict in the chain is automatically parsed and converted into a RunnableParallel, which runs all of its values in parallel and returns a dict with the results.\\nThis happens to be the same format the next prompt template expects. Here it is in action:\\nfrom langchain_core.output_parsers import StrOutputParseranalysis_prompt = ChatPromptTemplate.from_template(\"is this a funny joke? {joke}\")composed_chain = {\"joke\": chain} | analysis_prompt | model | StrOutputParser()composed_chain.invoke({\"topic\": \"bears\"})API Reference:StrOutputParser'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='\\'Haha, that\\\\\\'s a clever play on words! Using \"polar\" to imply the bear dissolved or became polar/polarized when put in water. Not the most hilarious joke ever, but it has a cute, groan-worthy pun that makes it mildly amusing. I appreciate a good pun or wordplay joke.\\'\\nFunctions will also be coerced into runnables, so you can add custom logic to your chains too. The below chain results in the same logical flow as before:\\ncomposed_chain_with_lambda = (    chain    | (lambda input: {\"joke\": input})    | analysis_prompt    | model    | StrOutputParser())composed_chain_with_lambda.invoke({\"topic\": \"beets\"})\\n\"Haha, that\\'s a cute and punny joke! I like how it plays on the idea of beets blushing or turning red like someone blushing. Food puns can be quite amusing. While not a total knee-slapper, it\\'s a light-hearted, groan-worthy dad joke that would make me chuckle and shake my head. Simple vegetable humor!\"\\nHowever, keep in mind that using functions like this may interfere with operations like streaming. See this section for more information.\\nThe .pipe() method\\u200b\\nWe could also compose the same sequence using the .pipe() method. Here\\'s what that looks like:\\nfrom langchain_core.runnables import RunnableParallelcomposed_chain_with_pipe = (    RunnableParallel({\"joke\": chain})    .pipe(analysis_prompt)    .pipe(model)    .pipe(StrOutputParser()))composed_chain_with_pipe.invoke({\"topic\": \"battlestar galactica\"})API Reference:RunnableParallel\\n\"I cannot reproduce any copyrighted material verbatim, but I can try to analyze the humor in the joke you provided without quoting it directly.\\\\n\\\\nThe joke plays on the idea that the Cylon raiders, who are the antagonists in the Battlestar Galactica universe, failed to locate the human survivors after attacking their home planets (the Twelve Colonies) due to using an outdated and poorly performing operating system (Windows Vista) for their targeting systems.\\\\n\\\\nThe humor stems from the juxtaposition of a futuristic science fiction setting with a relatable real-world frustration ‚Äì the use of buggy, slow, or unreliable software or technology. It pokes fun at the perceived inadequacies of Windows Vista, which was widely criticized for its performance issues and other problems when it was released.\\\\n\\\\nBy attributing the Cylons\\' failure to locate the humans to their use of Vista, the joke creates an amusing and unexpected connection between a fictional advanced race of robots and a familiar technological annoyance experienced by many people in the real world.\\\\n\\\\nOverall, the joke relies on incongruity and relatability to generate humor, but without reproducing any copyrighted material directly.\"\\nOr the abbreviated:\\ncomposed_chain_with_pipe = RunnableParallel({\"joke\": chain}).pipe(    analysis_prompt, model, StrOutputParser())\\nRelated\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/sequence/', 'title': 'How to chain runnables | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en'}, page_content='Streaming: Check out the streaming guide to understand the streaming behavior of a chain\\nEdit this pageWas this page helpful?PreviousHow to split text based on semantic similarityNextHow to save and load LangChain objectsThe pipe operator: |CoercionThe .pipe() methodRelatedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Conceptual guide | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityConceptual guideOn this pageConceptual guide'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content=\"This section contains introductions to key parts of LangChain.\\nArchitecture\\u200b\\nLangChain as a framework consists of a number of packages.\\nlangchain-core\\u200b\\nThis package contains base abstractions of different components and ways to compose them together.\\nThe interfaces for core components like LLMs, vector stores, retrievers and more are defined here.\\nNo third party integrations are defined here.\\nThe dependencies are kept purposefully very lightweight.\\nlangchain\\u200b\\nThe main langchain package contains chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\nThese are NOT third party integrations.\\nAll chains, agents, and retrieval strategies here are NOT specific to any one integration, but rather generic across all integrations.\\nlangchain-community\\u200b\\nThis package contains third party integrations that are maintained by the LangChain community.\\nKey partner packages are separated out (see below).\\nThis contains all integrations for various components (LLMs, vector stores, retrievers).\\nAll dependencies in this package are optional to keep the package as lightweight as possible.\\nPartner packages\\u200b\\nWhile the long tail of integrations is in langchain-community, we split popular integrations into their own packages (e.g. langchain-openai, langchain-anthropic, etc).\\nThis was done in order to improve support for these important integrations.\\nlanggraph\\u200b\\nlanggraph is an extension of langchain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\\nLangGraph exposes high level interfaces for creating common types of agents, as well as a low-level API for composing custom flows.\\nlangserve\\u200b\\nA package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.\\nLangSmith\\u200b\\nA developer platform that lets you debug, test, evaluate, and monitor LLM applications.\\n\\nLangChain Expression Language (LCEL)\\u200b\\n\\nLangChain Expression Language, or LCEL, is a declarative way to chain LangChain components.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest ‚Äúprompt + LLM‚Äù chain to the most complex chains (we‚Äôve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:\\n\\n\\nFirst-class streaming support:\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.\\n\\n\\nAsync support:\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.\\n\\n\\nOptimized parallel execution:\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.\\n\\n\\nRetries and fallbacks:\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We‚Äôre currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.\\n\\n\\nAccess intermediate results:\\nFor more complex chains it‚Äôs often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it‚Äôs available on every LangServe server.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.\\n\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\n\\n\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.\\nRunnable interface\\u200b\\n\\nTo make it as easy as possible to create custom chains, we\\'ve implemented a \"Runnable\" protocol. Many LangChain components implement the Runnable protocol, including chat models, LLMs, output parsers, retrievers, prompt templates, and more. There are also several useful primitives for working with runnables, which you can read about below.\\nThis is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way.\\nThe standard interface includes:\\n\\nstream: stream back chunks of the response\\ninvoke: call the chain on an input\\nbatch: call the chain on a list of inputs\\n\\nThese also have corresponding async methods that should be used with asyncio await syntax for concurrency:\\n\\nastream: stream back chunks of the response async\\nainvoke: call the chain on an input async\\nabatch: call the chain on a list of inputs async\\nastream_log: stream back intermediate steps as they happen, in addition to the final response\\nastream_events: beta stream events as they happen in the chain (introduced in langchain-core 0.1.14)\\n\\nThe input type and output type varies by component:\\nComponentInput TypeOutput TypePromptDictionaryPromptValueChatModelSingle string, list of chat messages or a PromptValueChatMessageLLMSingle string, list of chat messages or a PromptValueStringOutputParserThe output of an LLM or ChatModelDepends on the parserRetrieverSingle stringList of DocumentsToolSingle string or dictionary, depending on the toolDepends on the tool\\nAll runnables expose input and output schemas to inspect the inputs and outputs:\\n\\ninput_schema: an input Pydantic model auto-generated from the structure of the Runnable\\noutput_schema: an output Pydantic model auto-generated from the structure of the Runnable\\n\\nComponents\\u200b\\nLangChain provides standard, extendable interfaces and external integrations for various components useful for building with LLMs.\\nSome components LangChain implements, some components we rely on third-party integrations for, and others are a mix.\\nChat models\\u200b\\n\\nLanguage models that use a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text).\\nThese are traditionally newer models (older models are generally LLMs, see below).\\nChat models support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages.\\nAlthough the underlying models are messages in, message out, the LangChain wrappers also allow these models to take a string as input. This means you can easily use chat models in place of LLMs.\\nWhen a string is passed in as input, it is converted to a HumanMessage and then passed to the underlying model.\\nLangChain does not host any Chat Models, rather we rely on third party integrations.\\nWe have some standardized parameters when constructing ChatModels:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='model: the name of the model\\ntemperature: the sampling temperature\\ntimeout: request timeout\\nmax_tokens: max tokens to generate\\nstop: default stop sequences\\nmax_retries: max number of times to retry requests\\napi_key: API key for the model provider\\nbase_url: endpoint to send requests to\\n\\nSome important things to note:\\n\\nstandard params only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can\\'t be supported on these.\\nstandard params are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they\\'re not enforced on models in langchain-community.\\n\\nChatModels also accept other parameters that are specific to that integration. To find all the parameters supported by a ChatModel head to the API reference for that model.\\nimportantSome chat models have been fine-tuned for tool calling and provide a dedicated API for it.\\nGenerally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling.\\nPlease see the tool calling section for more information.\\nFor specifics on how to use chat models, see the relevant how-to guides here.\\nMultimodality\\u200b\\nSome chat models are multimodal, accepting images, audio and even video as inputs. These are still less common, meaning model providers haven\\'t standardized on the \"best\" way to define the API. Multimodal outputs are even less common. As such, we\\'ve kept our multimodal abstractions fairly light weight and plan to further solidify the multimodal APIs and interaction patterns as the field matures.\\nIn LangChain, most chat models that support multimodal inputs also accept those values in OpenAI\\'s content blocks format. So far this is restricted to image inputs. For models like Gemini which support video and other bytes input, the APIs also support the native, model-specific representations.\\nFor specifics on how to use multimodal models, see the relevant how-to guides here.\\nFor a full list of LangChain model providers with multimodal models, check out this table.\\nLLMs\\u200b\\n\\ncautionPure text-in/text-out LLMs tend to be older or lower-level. Many new popular models are best used as chat completion models,\\neven for non-chat use cases.You are probably looking for the section above instead.\\nLanguage models that takes a string as input and returns a string.\\nThese are traditionally older models (newer models generally are Chat Models, see above).\\nAlthough the underlying models are string in, string out, the LangChain wrappers also allow these models to take messages as input.\\nThis gives them the same interface as Chat Models.\\nWhen messages are passed in as input, they will be formatted into a string under the hood before being passed to the underlying model.\\nLangChain does not host any LLMs, rather we rely on third party integrations.\\nFor specifics on how to use LLMs, see the how-to guides.\\nMessages\\u200b\\nSome language models take a list of messages as input and return a message.\\nThere are a few different types of messages.\\nAll messages have a role, content, and response_metadata property.\\nThe role describes WHO is saying the message. The standard roles are \"user\", \"assistant\", \"system\", and \"tool\".\\nLangChain has different message classes for different roles.\\nThe content property describes the content of the message.\\nThis can be a few different things:\\n\\nA string (most models deal with this type of content)\\nA List of dictionaries (this is used for multimodal input, where the dictionary contains information about that input type and that input location)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Optionally, messages can have a name property which allows for differentiating between multiple speakers with the same role.\\nFor example, if there are two users in the chat history it can be useful to differentiate between them. Not all models support this.\\nHumanMessage\\u200b\\nThis represents a message with role \"user\".\\nAIMessage\\u200b\\nThis represents a message with role \"assistant\". In addition to the content property, these messages also have:\\nresponse_metadata\\nThe response_metadata property contains additional metadata about the response. The data here is often specific to each model provider.\\nThis is where information like log-probs and token usage may be stored.\\ntool_calls\\nThese represent a decision from a language model to call a tool. They are included as part of an AIMessage output.\\nThey can be accessed from there with the .tool_calls property.\\nThis property returns a list of ToolCalls. A ToolCall is a dictionary with the following arguments:\\n\\nname: The name of the tool that should be called.\\nargs: The arguments to that tool.\\nid: The id of that tool call.\\n\\nSystemMessage\\u200b\\nThis represents a message with role \"system\", which tells the model how to behave. Not every model provider supports this.\\nToolMessage\\u200b\\nThis represents a message with role \"tool\", which contains the result of calling a tool. In addition to role and content, this message has:\\n\\na tool_call_id field which conveys the id of the call to the tool that was called to produce this result.\\nan artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\\n\\nWith most chat models, a ToolMessage can only appear in the chat history after an AIMessage that has a populated tool_calls field.\\n(Legacy) FunctionMessage\\u200b\\nThis is a legacy message type, corresponding to OpenAI\\'s legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API.\\nThis represents the result of a function call. In addition to role and content, this message has a name parameter which conveys the name of the function that was called to produce this result.\\nPrompt templates\\u200b\\n\\nPrompt templates help to translate user input and parameters into instructions for a language model.\\nThis can be used to guide a model\\'s response, helping it understand the context and generate relevant and coherent language-based output.\\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.\\nThere are a few different types of prompt templates:\\nString PromptTemplates\\u200b\\nThese prompt templates are used to format a single string, and generally are used for simpler inputs.\\nFor example, a common way to construct and use a PromptTemplate is as follows:\\nfrom langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplate\\nChatPromptTemplates\\u200b\\nThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:\\nfrom langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplate\\nIn the above example, this ChatPromptTemplate will construct two messages when called.\\nThe first is a system message, that has no variables to format.\\nThe second is a HumanMessage, and will be formatted by the topic variable the user passes in.\\nMessagesPlaceholder\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='This prompt template is responsible for adding a list of messages in a particular place.\\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\\nThis is how you use MessagesPlaceholder.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\\nThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\\nThis is useful for letting a list of messages be slotted into a particular spot.\\nAn alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\\nprompt_template = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])\\nFor specifics on how to use prompt templates, see the relevant how-to guides here.\\nExample selectors\\u200b\\nOne common prompting technique for achieving better performance is to include examples as part of the prompt.\\nThis is known as few-shot prompting.\\nThis gives the language model concrete examples of how it should behave.\\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\\nExample Selectors are classes responsible for selecting and then formatting examples into prompts.\\nFor specifics on how to use example selectors, see the relevant how-to guides here.\\nOutput parsers\\u200b\\n\\nnoteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\nMore and more models are supporting function (or tool) calling, which handles this automatically.\\nIt is recommended to use function/tool calling rather than output parsing.\\nSee documentation for that here.\\nOutput parser is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\\n\\nName: The name of the output parser\\nSupports Streaming: Whether the output parser supports streaming.\\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.\\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.\\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.\\nOutput Type: The output type of the object returned by the parser.\\nDescription: Our commentary on this output parser and when to use it.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content=\"NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionJSON‚úÖ‚úÖstr | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML‚úÖ‚úÖstr | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).CSV‚úÖ‚úÖstr | MessageList[str]Returns a list of comma separated values.OutputFixing‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame‚úÖstr | MessagedictUseful for doing operations with pandas DataFrames.Enum‚úÖstr | MessageEnumParses response into one of the provided enum values.Datetime‚úÖstr | Messagedatetime.datetimeParses response into a datetime string.Structured‚úÖstr | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.\\nFor specifics on how to use output parsers, see the relevant how-to guides here.\\nChat history\\u200b\\nMost LLM applications have a conversational interface.\\nAn essential component of a conversation is being able to refer to information introduced earlier in the conversation.\\nAt bare minimum, a conversational system should be able to access some window of past messages directly.\\nThe concept of ChatHistory refers to a class in LangChain which can be used to wrap an arbitrary chain.\\nThis ChatHistory will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database.\\nFuture interactions will then load those messages and pass them into the chain as part of the input.\\nDocuments\\u200b\\n\\nA Document object in LangChain contains information about some data. It has two attributes:\\n\\npage_content: str: The content of this document. Currently is only a string.\\nmetadata: dict: Arbitrary metadata associated with this document. Can track the document id, file name, etc.\\n\\nDocument loaders\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Document loaders\\u200b\\n\\nThese classes load Document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the .load method.\\nAn example use case is as follows:\\nfrom langchain_community.document_loaders.csv_loader import CSVLoaderloader = CSVLoader(    ...  # <-- Integration specific parameters here)data = loader.load()API Reference:CSVLoader\\nFor specifics on how to use document loaders, see the relevant how-to guides here.\\nText splitters\\u200b\\nOnce you\\'ve loaded documents, you\\'ll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model\\'s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\\nWhen you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\\nAt a high level, text splitters work as following:\\n\\nSplit the text up into small, semantically meaningful chunks (often sentences).\\nStart combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\\nOnce you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\\n\\nThat means there are two different axes along which you can customize your text splitter:\\n\\nHow the text is split\\nHow the chunk size is measured\\n\\nFor specifics on how to use text splitters, see the relevant how-to guides here.\\nEmbedding models\\u200b\\n\\nEmbedding models create a vector representation of a piece of text. You can think of a vector as an array of numbers that captures the semantic meaning of the text.\\nBy representing the text in this way, you can perform mathematical operations that allow you to do things like search for other pieces of text that are most similar in meaning.\\nThese natural language search capabilities underpin many types of context retrieval,\\nwhere we provide an LLM with the relevant data it needs to effectively respond to a query.\\n\\nThe Embeddings class is a class designed for interfacing with text embedding models. There are many different embedding model providers (OpenAI, Cohere, Hugging Face, etc) and local models, and this class is designed to provide a standard interface for all of them.\\nThe base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).\\nFor specifics on how to use embedding models, see the relevant how-to guides here.\\nVector stores\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content=\"One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors,\\nand then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query.\\nA vector store takes care of storing embedded data and performing vector search for you.\\nMost vector stores can also store metadata about embedded vectors and support filtering on that metadata before\\nsimilarity search, allowing you more control over returned documents.\\nVector stores can be converted to the retriever interface by doing:\\nvectorstore = MyVectorStore()retriever = vectorstore.as_retriever()\\nFor specifics on how to use vector stores, see the relevant how-to guides here.\\nRetrievers\\u200b\\n\\nA retriever is an interface that returns documents given an unstructured query.\\nIt is more general than a vector store.\\nA retriever does not need to be able to store documents, only to return (or retrieve) them.\\nRetrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.\\nRetrievers accept a string query as input and return a list of Document's as output.\\nFor specifics on how to use retrievers, see the relevant how-to guides here.\\nKey-value stores\\u200b\\nFor some techniques, such as indexing and retrieval with multiple vectors per document or\\ncaching embeddings, having a form of key-value (KV) storage is helpful.\\nLangChain includes a BaseStore interface,\\nwhich allows for storage of arbitrary data. However, LangChain components that require KV-storage accept a\\nmore specific BaseStore[str, bytes] instance that stores binary data (referred to as a ByteStore), and internally take care of\\nencoding and decoding data for their specific needs.\\nThis means that as a user, you only need to think about one type of store rather than different ones for different types of data.\\nInterface\\u200b\\nAll BaseStores support the following interface. Note that the interface allows\\nfor modifying multiple key-value pairs at once:\\n\\nmget(key: Sequence[str]) -> List[Optional[bytes]]: get the contents of multiple keys, returning None if the key does not exist\\nmset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None: set the contents of multiple keys\\nmdelete(key: Sequence[str]) -> None: delete multiple keys\\nyield_keys(prefix: Optional[str] = None) -> Iterator[str]: yield all keys in the store, optionally filtering by a prefix\\n\\nFor key-value store implementations, see this section.\\nTools\\u200b\\n\\nTools are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models.\\nTools are needed whenever you want a model to control parts of your code or call out to external APIs.\\nA tool consists of:\\n\\nThe name of the tool.\\nA description of what the tool does.\\nA JSON schema defining the inputs to the tool.\\nA function (and, optionally, an async variant of the function).\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='The name of the tool.\\nA description of what the tool does.\\nA JSON schema defining the inputs to the tool.\\nA function (and, optionally, an async variant of the function).\\n\\nWhen a tool is bound to a model, the name, description and JSON schema are provided as context to the model.\\nGiven a list of tools and a set of instructions, a model can request to call one or more tools with specific inputs.\\nTypical usage may look like the following:\\ntools = [...] # Define a list of toolsllm_with_tools = llm.bind_tools(tools)ai_msg = llm_with_tools.invoke(\"do xyz...\")# -> AIMessage(tool_calls=[ToolCall(...), ...], ...)\\nThe AIMessage returned from the model MAY have tool_calls associated with it.\\nRead this guide for more information on what the response type may look like.\\nOnce the chosen tools are invoked, the results can be passed back to the model so that it can complete whatever task\\nit\\'s performing.\\nThere are generally two different ways to invoke the tool and pass back the response:\\nInvoke with just the arguments\\u200b\\nWhen you invoke a tool with just the arguments, you will get back the raw tool output (usually a string).\\nThis generally looks like:\\n# You will want to previously check that the LLM returned tool callstool_call = ai_msg.tool_calls[0]# ToolCall(args={...}, id=..., ...)tool_output = tool.invoke(tool_call[\"args\"])tool_message = ToolMessage(    content=tool_output,    tool_call_id=tool_call[\"id\"],    name=tool_call[\"name\"])\\nNote that the content field will generally be passed back to the model.\\nIf you do not want the raw tool response to be passed to the model, but you still want to keep it around,\\nyou can transform the tool output but also pass it as an artifact (read more about ToolMessage.artifact here)\\n... # Same code as aboveresponse_for_llm = transform(response)tool_message = ToolMessage(    content=response_for_llm,    tool_call_id=tool_call[\"id\"],    name=tool_call[\"name\"],    artifact=tool_output)\\nInvoke with ToolCall\\u200b\\nThe other way to invoke a tool is to call it with the full ToolCall that was generated by the model.\\nWhen you do this, the tool will return a ToolMessage.\\nThe benefits of this are that you don\\'t have to write the logic yourself to transform the tool output into a ToolMessage.\\nThis generally looks like:\\ntool_call = ai_msg.tool_calls[0]# -> ToolCall(args={...}, id=..., ...)tool_message = tool.invoke(tool_call)# -> ToolMessage(#      content=\"tool result foobar...\",#      tool_call_id=...,#      name=\"tool_name\"#    )\\nIf you are invoking the tool this way and want to include an artifact for the ToolMessage, you will need to have the tool return two things.\\nRead more about defining tools that return artifacts here.\\nBest practices\\u200b\\nWhen designing tools to be used by a model, it is important to keep in mind that:\\n\\nChat models that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models.\\nModels will perform better if the tools have well-chosen names, descriptions, and JSON schemas. This is another form of prompt engineering.\\nSimple, narrowly scoped tools are easier for models to use than complex tools.\\n\\nRelated\\u200b\\nFor specifics on how to use tools, see the tools how-to guides.\\nTo use a pre-built tool, see the tool integration docs.\\nToolkits\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Related\\u200b\\nFor specifics on how to use tools, see the tools how-to guides.\\nTo use a pre-built tool, see the tool integration docs.\\nToolkits\\u200b\\n\\nToolkits are collections of tools that are designed to be used together for specific tasks. They have convenient loading methods.\\nAll Toolkits expose a get_tools method which returns a list of tools.\\nYou can therefore do:\\n# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()\\nAgents\\u200b\\nBy themselves, language models can\\'t take actions - they just output text.\\nA big use case for LangChain is creating agents.\\nAgents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be.\\nThe results of those actions can then be fed back into the agent and it determine whether more actions are needed, or whether it is okay to finish.\\nLangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents.\\nPlease check out that documentation for a more in depth overview of agent concepts.\\nThere is a legacy agent concept in LangChain that we are moving towards deprecating: AgentExecutor.\\nAgentExecutor was essentially a runtime for agents.\\nIt was a great place to get started, however, it was not flexible enough as you started to have more customized agents.\\nIn order to solve that we built LangGraph to be this flexible, highly-controllable runtime.\\nIf you are still using AgentExecutor, do not fear: we still have a guide on how to use AgentExecutor.\\nIt is recommended, however, that you start to transition to LangGraph.\\nIn order to assist in this, we have put together a transition guide on how to do so.\\nReAct agents\\u200b\\n\\nOne popular architecture for building agents is ReAct.\\nReAct combines reasoning and acting in an iterative process - in fact the name \"ReAct\" stands for \"Reason\" and \"Act\".\\nThe general flow looks like this:\\n\\nThe model will \"think\" about what step to take in response to an input and any previous observations.\\nThe model will then choose an action from available tools (or choose to respond to the user).\\nThe model will generate arguments to that tool.\\nThe agent runtime (executor) will parse out the chosen tool and call it with the generated arguments.\\nThe executor will return the results of the tool call back to the model as an observation.\\nThis process repeats until the agent chooses to respond.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='There are general prompting based implementations that do not require any model-specific features, but the most\\nreliable implementations use features like tool calling to reliably format outputs\\nand reduce variance.\\nPlease see the LangGraph documentation for more information,\\nor this how-to guide for specific information on migrating to LangGraph.\\nCallbacks\\u200b\\nLangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\\nYou can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\\nCallback Events\\u200b\\nEventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retry\\nCallback handlers\\u200b\\nCallback handlers can either be sync or async:\\n\\nSync callback handlers implement the BaseCallbackHandler interface.\\nAsync callback handlers implement the AsyncCallbackHandler interface.\\n\\nDuring run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.\\nPassing callbacks\\u200b\\nThe callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:\\n\\nRequest time callbacks: Passed at the time of the request in addition to the input data.\\nAvailable on all standard Runnable objects. These callbacks are INHERITED by all children\\nof the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).\\nConstructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks\\nare passed as arguments to the constructor of the object. The callbacks are scoped\\nonly to the object they are defined on, and are not inherited by any children of the object.\\n\\nwarningConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\\nof the object.\\nIf you\\'re creating a custom chain or runnable, you need to remember to propagate request time\\ncallbacks to any child objects.\\nAsync in Python<=3.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\\nand is running async in python<=3.10, will have to propagate callbacks to child\\nobjects manually. This is because LangChain cannot automatically propagate\\ncallbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\\nrunnables or tools.\\nFor specifics on how to use callbacks, see the relevant how-to guides here.\\nTechniques\\u200b\\nStreaming\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Individual LLM calls often run for much longer than traditional resource requests.\\nThis compounds when you build more complex chains or agents that require multiple reasoning steps.\\nFortunately, LLMs generate output iteratively, which means it\\'s possible to show sensible intermediate results\\nbefore the final response is ready. Consuming output as soon as it becomes available has therefore become a vital part of the UX\\naround building apps with LLMs to help alleviate latency issues, and LangChain aims to have first-class support for streaming.\\nBelow, we\\'ll discuss some concepts and considerations around streaming in LangChain.\\n.stream() and .astream()\\u200b\\nMost modules in LangChain include the .stream() method (and the equivalent .astream() method for async environments) as an ergonomic streaming interface.\\n.stream() returns an iterator, which you can consume with a simple for loop. Here\\'s an example with a chat model:\\nfrom langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")for chunk in model.stream(\"what color is the sky?\"):    print(chunk.content, end=\"|\", flush=True)API Reference:ChatAnthropic\\nFor models (or other components) that don\\'t support streaming natively, this iterator would just yield a single chunk, but\\nyou could still use the same general pattern when calling them. Using .stream() will also automatically call the model in streaming mode\\nwithout the need to provide additional config.\\nThe type of each outputted chunk depends on the type of component - for example, chat models yield AIMessageChunks.\\nBecause this method is part of LangChain Expression Language,\\nyou can handle formatting differences from different outputs using an output parser to transform\\neach yielded chunk.\\nYou can check out this guide for more detail on how to use .stream().\\n.astream_events()\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='While the .stream() method is intuitive, it can only return the final generated value of your chain. This is fine for single LLM calls,\\nbut as you build more complex chains of several LLM calls together, you may want to use the intermediate values of\\nthe chain alongside the final output - for example, returning sources alongside the final generation when building a chat\\nover documents app.\\nThere are ways to do this using callbacks, or by constructing your chain in such a way that it passes intermediate\\nvalues to the end with something like chained .assign() calls, but LangChain also includes an\\n.astream_events() method that combines the flexibility of callbacks with the ergonomics of .stream(). When called, it returns an iterator\\nwhich yields various types of events that you can filter and process according\\nto the needs of your project.\\nHere\\'s one small example that prints just events containing streamed chat model output:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_anthropic import ChatAnthropicmodel = ChatAnthropic(model=\"claude-3-sonnet-20240229\")prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")parser = StrOutputParser()chain = prompt | model | parserasync for event in chain.astream_events({\"topic\": \"parrot\"}, version=\"v2\"):    kind = event[\"event\"]    if kind == \"on_chat_model_stream\":        print(event, end=\"|\", flush=True)API Reference:StrOutputParser | ChatPromptTemplate | ChatAnthropic\\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\\nSee this guide for more detailed information on how to use .astream_events(),\\nincluding a table listing available events.\\nCallbacks\\u200b\\nThe lowest level way to stream outputs from LLMs in LangChain is via the callbacks system. You can pass a\\ncallback handler that handles the on_llm_new_token event into LangChain components. When that component is invoked, any\\nLLM or chat model contained in the component calls\\nthe callback with the generated token. Within the callback, you could pipe the tokens into some other destination, e.g. a HTTP response.\\nYou can also handle the on_llm_end event to perform any necessary cleanup.\\nYou can see this how-to section for more specifics on using callbacks.\\nCallbacks were the first technique for streaming introduced in LangChain. While powerful and generalizable,\\nthey can be unwieldy for developers. For example:\\n\\nYou need to explicitly initialize and manage some aggregator or other stream to collect results.\\nThe execution order isn\\'t explicitly guaranteed, and you could theoretically have a callback run after the .invoke() method finishes.\\nProviders would often make you pass an additional parameter to stream outputs instead of returning them all at once.\\nYou would often ignore the result of the actual model call in favor of callback results.\\n\\nTokens\\u200b\\nThe unit that most model providers use to measure input and output is via a unit called a token.\\nTokens are the basic units that language models read and generate when processing or producing text.\\nThe exact definition of a token can vary depending on the specific way the model was trained -\\nfor instance, in English, a token could be a single word like \"apple\", or a part of a word like \"app\".\\nWhen you send a model a prompt, the words and characters in the prompt are encoded into tokens using a tokenizer.\\nThe model then streams back generated output tokens, which the tokenizer decodes into human-readable text.\\nThe below example shows how OpenAI models tokenize LangChain is cool!:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='You can see that it gets split into 5 different tokens, and that the boundaries between tokens are not exactly the same as word boundaries.\\nThe reason language models use tokens rather than something more immediately intuitive like \"characters\"\\nhas to do with how they process and understand text. At a high-level, language models iteratively predict their next generated output based on\\nthe initial input and their previous generations. Training the model using tokens language models to handle linguistic\\nunits (like words or subwords) that carry meaning, rather than individual characters, which makes it easier for the model\\nto learn and understand the structure of the language, including grammar and context.\\nFurthermore, using tokens can also improve efficiency, since the model processes fewer units of text compared to character-level processing.\\nFunction/tool calling\\u200b\\ninfoWe use the term tool calling interchangeably with function calling. Although\\nfunction calling is sometimes meant to refer to invocations of a single function,\\nwe treat all models as though they can return multiple tool or function calls in\\neach message.\\nTool calling allows a chat model to respond to a given prompt by generating output that\\nmatches a user-defined schema.\\nWhile the name implies that the model is performing\\nsome action, this is actually not the case! The model only generates the arguments to a tool, and actually running the tool (or not) is up to the user.\\nOne common example where you wouldn\\'t want to call a function with the generated arguments\\nis if you want to extract structured output matching some schema\\nfrom unstructured text. You would give the model an \"extraction\" tool that takes\\nparameters matching the desired schema, then treat the generated output as your final\\nresult.\\n\\nTool calling is not universal, but is supported by many popular LLM providers, including Anthropic,\\nCohere, Google,\\nMistral, OpenAI, and even for locally-running models via Ollama.\\nLangChain provides a standardized interface for tool calling that is consistent across different models.\\nThe standard interface consists of:\\n\\nChatModel.bind_tools(): a method for specifying which tools are available for a model to call. This method accepts LangChain tools as well as Pydantic objects.\\nAIMessage.tool_calls: an attribute on the AIMessage returned from the model for accessing the tool calls requested by the model.\\n\\nTool usage\\u200b\\nAfter the model calls tools, you can use the tool by invoking it, then passing the arguments back to the model.\\nLangChain provides the Tool abstraction to help you handle this.\\nThe general flow is this:\\n\\nGenerate tool calls with a chat model in response to a query.\\nInvoke the appropriate tools using the generated tool call as arguments.\\nFormat the result of the tool invocations as ToolMessages.\\nPass the entire list of messages back to the model so that it can generate a final answer (or call more tools).\\n\\n\\nThis is how tool calling agents perform tasks and answer queries.\\nCheck out some more focused guides below:\\n\\nHow to use chat models to call tools\\nHow to pass tool outputs to chat models\\nBuilding an agent with LangGraph'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='How to use chat models to call tools\\nHow to pass tool outputs to chat models\\nBuilding an agent with LangGraph\\n\\nStructured output\\u200b\\nLLMs are capable of generating arbitrary text. This enables the model to respond appropriately to a wide\\nrange of inputs, but for some use-cases, it can be useful to constrain the LLM\\'s output\\nto a specific format or structure. This is referred to as structured output.\\nFor example, if the output is to be stored in a relational database,\\nit is much easier if the model generates output that adheres to a defined schema or format.\\nExtracting specific information from unstructured text is another\\ncase where this is particularly useful. Most commonly, the output format will be JSON,\\nthough other formats such as YAML can be useful too. Below, we\\'ll discuss\\na few ways to get structured output from models in LangChain.\\n.with_structured_output()\\u200b\\nFor convenience, some LangChain chat models support a .with_structured_output()\\nmethod. This method only requires a schema as input, and returns a dict or Pydantic object.\\nGenerally, this method is only present on models that support one of the more advanced methods described below,\\nand will use one of them under the hood. It takes care of importing a suitable output parser and\\nformatting the schema in the right format for the model.\\nHere\\'s an example:\\nfrom typing import Optionalfrom pydantic import BaseModel, Fieldclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'To keep an eye on the mouse!\\', rating=None)\\nWe recommend this method as a starting point when working with structured output:\\n\\nIt uses other model-specific features under the hood, without the need to import an output parser.\\nFor the models that use tool calling, no special prompting is needed.\\nIf multiple underlying techniques are supported, you can supply a method parameter to\\ntoggle which one is used.\\n\\nYou may want or need to use other techniques if:\\n\\nThe chat model you are using does not support tool calling.\\nYou are working with very complex schemas and the model is having trouble generating outputs that conform.\\n\\nFor more information, check out this how-to guide.\\nYou can also check out this table for a list of models that support\\nwith_structured_output().\\nRaw prompting\\u200b\\nThe most intuitive way to get a model to structure output is to ask nicely.\\nIn addition to your query, you can give instructions describing what kind of output you\\'d like, then\\nparse the output using an output parser to convert the raw\\nmodel message or string output into something more easily manipulated.\\nThe biggest benefit to raw prompting is its flexibility:\\n\\nRaw prompting does not require any special model features, only sufficient reasoning capability to understand\\nthe passed schema.\\nYou can prompt for any format you\\'d like, not just JSON. This can be useful if the model you\\nare using is more heavily trained on a certain type of data, such as XML or YAML.\\n\\nHowever, there are some drawbacks too:\\n\\nLLMs are non-deterministic, and prompting a LLM to consistently output data in the exactly correct format\\nfor smooth parsing can be surprisingly difficult and model-specific.\\nIndividual models have quirks depending on the data they were trained on, and optimizing prompts can be quite difficult.\\nSome may be better at interpreting JSON schema, others may be best with TypeScript definitions,\\nand still others may prefer XML.\\n\\nWhile features offered by model providers may increase reliability, prompting techniques remain important for tuning your\\nresults no matter which method you choose.\\nJSON mode\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='While features offered by model providers may increase reliability, prompting techniques remain important for tuning your\\nresults no matter which method you choose.\\nJSON mode\\u200b\\n\\nSome models, such as Mistral, OpenAI,\\nTogether AI and Ollama,\\nsupport a feature called JSON mode, usually enabled via config.\\nWhen enabled, JSON mode will constrain the model\\'s output to always be some sort of valid JSON.\\nOften they require some custom prompting, but it\\'s usually much less burdensome than completely raw prompting and\\nmore along the lines of, \"you must always return JSON\". The output also generally easier to parse.\\nIt\\'s also generally simpler to use directly and more commonly available than tool calling, and can give\\nmore flexibility around prompting and shaping results than tool calling.\\nHere\\'s an example:\\nfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain.output_parsers.json import SimpleJsonOutputParsermodel = ChatOpenAI(    model=\"gpt-4o\",    model_kwargs={ \"response_format\": { \"type\": \"json_object\" } },)prompt = ChatPromptTemplate.from_template(    \"Answer the user\\'s question to the best of your ability.\"    \\'You must always output a JSON object with an \"answer\" key and a \"followup_question\" key.\\'    \"{question}\")chain = prompt | model | SimpleJsonOutputParser()chain.invoke({ \"question\": \"What is the powerhouse of the cell?\" })API Reference:ChatPromptTemplate | ChatOpenAI | SimpleJsonOutputParser\\n{\\'answer\\': \\'The powerhouse of the cell is the mitochondrion. It is responsible for producing energy in the form of ATP through cellular respiration.\\', \\'followup_question\\': \\'Would you like to know more about how mitochondria produce energy?\\'}\\nFor a full list of model providers that support JSON mode, see this table.\\nTool calling\\u200b\\nFor models that support it, tool calling can be very convenient for structured output. It removes the\\nguesswork around how best to prompt schemas in favor of a built-in model feature.\\nIt works by first binding the desired schema either directly or via a LangChain tool to a\\nchat model using the .bind_tools() method. The model will then generate an AIMessage containing\\na tool_calls field containing args that match the desired shape.\\nThere are several acceptable formats you can use to bind tools to a model in LangChain. Here\\'s one example:\\nfrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIclass ResponseFormatter(BaseModel):    \"\"\"Always use this tool to structure your response to the user.\"\"\"    answer: str = Field(description=\"The answer to the user\\'s question\")    followup_question: str = Field(description=\"A followup question the user could ask\")model = ChatOpenAI(    model=\"gpt-4o\",    temperature=0,)model_with_tools = model.bind_tools([ResponseFormatter])ai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")ai_msg.tool_calls[0][\"args\"]API Reference:ChatOpenAI\\n{\\'answer\\': \"The powerhouse of the cell is the mitochondrion. It generates most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", \\'followup_question\\': \\'How do mitochondria generate ATP?\\'}\\nTool calling is a generally consistent way to get a model to generate structured output, and is the default technique\\nused for the .with_structured_output() method when a model supports it.\\nThe following how-to guides are good practical resources for using function/tool calling for structured output:\\n\\nHow to return structured data from an LLM\\nHow to use a model to call tools'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='How to return structured data from an LLM\\nHow to use a model to call tools\\n\\nFor a full list of model providers that support tool calling, see this table.\\nFew-shot prompting\\u200b\\nOne of the most effective ways to improve model performance is to give a model examples of\\nwhat you want it to do. The technique of adding example inputs and expected outputs\\nto a model prompt is known as \"few-shot prompting\". The technique is based on the\\nLanguage Models are Few-Shot Learners paper.\\nThere are a few things to think about when doing few-shot prompting:\\n\\nHow are examples generated?\\nHow many examples are in each prompt?\\nHow are examples selected at runtime?\\nHow are examples formatted in the prompt?\\n\\nHere are the considerations for each.\\n1. Generating examples\\u200b\\nThe first and most important step of few-shot prompting is coming up with a good dataset of examples. Good examples should be relevant at runtime, clear, informative, and provide information that was not already known to the model.\\nAt a high-level, the basic ways to generate examples are:\\n\\nManual: a person/people generates examples they think are useful.\\nBetter model: a better (presumably more expensive/slower) model\\'s responses are used as examples for a worse (presumably cheaper/faster) model.\\nUser feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).\\nLLM feedback: same as user feedback but the process is automated by having models evaluate themselves.\\n\\nWhich approach is best depends on your task. For tasks where a small number core principles need to be understood really well, it can be valuable hand-craft a few really good examples.\\nFor tasks where the space of correct behaviors is broader and more nuanced, it can be useful to generate many examples in a more automated fashion so that there\\'s a higher likelihood of there being some highly relevant examples for any runtime input.\\nSingle-turn v.s. multi-turn examples\\nAnother dimension to think about when generating examples is what the example is actually showing.\\nThe simplest types of examples just have a user input and an expected model output. These are single-turn examples.\\nOne more complex type if example is where the example is an entire conversation, usually in which a model initially responds incorrectly and a user then tells the model how to correct its answer.\\nThis is called a multi-turn example. Multi-turn examples can be useful for more nuanced tasks where its useful to show common errors and spell out exactly why they\\'re wrong and what should be done instead.\\n2. Number of examples\\u200b\\nOnce we have a dataset of examples, we need to think about how many examples should be in each prompt.\\nThe key tradeoff is that more examples generally improve performance, but larger prompts increase costs and latency.\\nAnd beyond some threshold having too many examples can start to confuse the model.\\nFinding the right number of examples is highly dependent on the model, the task, the quality of the examples, and your cost and latency constraints.\\nAnecdotally, the better the model is the fewer examples it needs to perform well and the more quickly you hit steeply diminishing returns on adding more examples.\\nBut, the best/only way to reliably answer this question is to run some experiments with different numbers of examples.\\n3. Selecting examples\\u200b\\nAssuming we are not adding our entire example dataset into each prompt, we need to have a way of selecting examples from our dataset based on a given input. We can do this:\\n\\nRandomly\\nBy (semantic or keyword-based) similarity of the inputs\\nBased on some other constraints, like token size'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Randomly\\nBy (semantic or keyword-based) similarity of the inputs\\nBased on some other constraints, like token size\\n\\nLangChain has a number of ExampleSelectors which make it easy to use any of these techniques.\\nGenerally, selecting by semantic similarity leads to the best model performance. But how important this is is again model and task specific, and is something worth experimenting with.\\n4. Formatting examples\\u200b\\nMost state-of-the-art models these days are chat models, so we\\'ll focus on formatting examples for those. Our basic options are to insert the examples:\\n\\nIn the system prompt as a string\\nAs their own messages\\n\\nIf we insert our examples into the system prompt as a string, we\\'ll need to make sure it\\'s clear to the model where each example begins and which parts are the input versus output. Different models respond better to different syntaxes, like ChatML, XML, TypeScript, etc.\\nIf we insert our examples as messages, where each example is represented as a sequence of Human, AI messages, we might want to also assign names to our messages like \"example_user\" and \"example_assistant\" to make it clear that these messages correspond to different actors than the latest input message.\\nFormatting tool call examples\\nOne area where formatting examples as messages can be tricky is when our example outputs have tool calls. This is because different models have different constraints on what types of message sequences are allowed when any tool calls are generated.\\n\\nSome models require that any AIMessage with tool calls be immediately followed by ToolMessages for every tool call,\\nSome models additionally require that any ToolMessages be immediately followed by an AIMessage before the next HumanMessage,\\nSome models require that tools are passed in to the model if there are any tool calls / ToolMessages in the chat history.\\n\\nThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.\\nIn these cases it\\'s especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.\\nYou can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks here.\\nRetrieval\\u200b\\nLLMs are trained on a large but fixed dataset, limiting their ability to reason over private or recent information.\\nFine-tuning an LLM with specific facts is one way to mitigate this, but is often poorly suited for factual recall and can be costly.\\nRetrieval is the process of providing relevant information to an LLM to improve its response for a given input.\\nRetrieval augmented generation (RAG) paper is the process of grounding the LLM generation (output) using the retrieved information.\\ntip\\nSee our RAG from Scratch code and video series.\\nFor a high-level guide on retrieval, see this tutorial on RAG.\\n\\nRAG is only as good as the retrieved documents‚Äô relevance and quality. Fortunately, an emerging set of techniques can be employed to design and improve RAG systems. We\\'ve focused on taxonomizing and summarizing many of these techniques (see below figure) and will share some high-level strategic guidance in the following sections.\\nYou can and should experiment with using different pieces together. You might also find this LangSmith guide useful for showing how to evaluate different iterations of your app.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Query Translation\\u200b\\nFirst, consider the user input(s) to your RAG system. Ideally, a RAG system can handle a wide range of inputs, from poorly worded questions to complex multi-part queries.\\nUsing an LLM to review and optionally modify the input is the central idea behind query translation. This serves as a general buffer, optimizing raw user inputs for your retrieval system.\\nFor example, this can be as simple as extracting keywords or as complex as generating multiple sub-questions for a complex query.\\nNameWhen to useDescriptionMulti-queryWhen you need to cover multiple perspectives of a question.Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. Paper.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. Paper.\\ntipSee our RAG from Scratch videos for a few different specific approaches:\\nMulti-query\\nDecomposition\\nStep-back\\nHyDE'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Routing\\u200b\\nSecond, consider the data sources available to your RAG system. You want to query across more than one database or across structured and unstructured data sources. Using an LLM to review the input and route it to the appropriate data source is a simple and effective approach for querying across sources.\\nNameWhen to useDescriptionLogical routingWhen you can prompt an LLM with rules to decide where to route the input.Logical routing can use an LLM to reason about the query and choose which datastore is most appropriate.Semantic routingWhen semantic similarity is an effective way to determine where to route the input.Semantic routing embeds both query and, typically a set of prompts. It then chooses the appropriate prompt based upon similarity.\\ntipSee our RAG from Scratch video on routing.\\nQuery Construction\\u200b\\nThird, consider whether any of your data sources require specific query formats. Many structured databases use SQL. Vector stores often have specific syntax for applying keyword filters to document metadata. Using an LLM to convert a natural language query into a query syntax is a popular and powerful approach.\\nIn particular, text-to-SQL, text-to-Cypher, and query analysis for metadata filters are useful ways to interact with structured, graph, and vector databases respectively.\\nNameWhen to UseDescriptionText to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.Self QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).\\ntipSee our blog post overview and RAG from Scratch video on query construction, the process of text-to-DSL where DSL is a domain specific language required to interact with a given database. This converts user questions into structured queries.\\nIndexing\\u200b\\nFourth, consider the design of your document index. A simple and powerful idea is to decouple the documents that you index for retrieval from the documents that you pass to the LLM for generation. Indexing frequently uses embedding models with vector stores, which compress the semantic information in documents to fixed-size vectors.\\nMany RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question. Furthermore, LLMs are increasingly capable of processing millions of tokens.\\nTwo approaches can address this tension: (1) Multi Vector retriever using an LLM to translate documents into any form (e.g., often into a summary) that is well-suited for indexing, but returns full documents to the LLM for generation. (2) ParentDocument retriever embeds document chunks, but also returns full documents. The idea is to get the best of both worlds: use concise representations (summaries or chunks) for retrieval, but use the full documents for answer generation.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='NameIndex TypeUses an LLMWhen to UseDescriptionVector storeVector storeNoIf you are just getting started and looking for something quick and easy.This is the simplest method and the one that is easiest to get started with. It involves creating embeddings for each piece of text.ParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.Time-Weighted Vector storeVector storeNoIf you have timestamps associated with your documents, and you want to retrieve the most recent onesThis fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents)\\ntip\\nSee our RAG from Scratch video on indexing fundamentals\\nSee our RAG from Scratch video on multi vector retriever'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='Fifth, consider ways to improve the quality of your similarity search itself. Embedding models compress text into fixed-length (vector) representations that capture the semantic content of the document. This compression is useful for search / retrieval, but puts a heavy burden on that single vector representation to capture the semantic nuance / detail of the document. In some cases, irrelevant or redundant content can dilute the semantic usefulness of the embedding.\\nColBERT is an interesting approach to address this with a higher granularity embeddings: (1) produce a contextually influenced embedding for each token in the document and query, (2) score similarity between each query token and all document tokens, (3) take the max, (4) do this for all query tokens, and (5) take the sum of the max scores (in step 3) for all query tokens to get a query-document similarity score; this token-wise scoring can yield strong results.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='There are some additional tricks to improve the quality of your retrieval. Embeddings excel at capturing semantic information, but may struggle with keyword-based queries. Many vector stores offer built-in hybrid-search to combine keyword and semantic similarity, which marries the benefits of both approaches. Furthermore, many vector stores have maximal marginal relevance, which attempts to diversify the results of a search to avoid returning similar and redundant documents.\\nNameWhen to useDescriptionColBERTWhen higher granularity embeddings are needed.ColBERT uses contextually influenced embeddings for each token in the document and query to get a granular query-document similarity score. Paper.Hybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. Paper.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.\\ntipSee our RAG from Scratch video on ColBERT.\\nPost-processing\\u200b\\nSixth, consider ways to filter or rank retrieved documents. This is very useful if you are combining documents returned from multiple sources, since it can can down-rank less relevant documents and / or compress similar documents.\\nNameIndex TypeUses an LLMWhen to UseDescriptionContextual CompressionAnySometimesIf you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM.This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM.EnsembleAnyNoIf you have multiple retrieval methods and want to try combining them.This fetches documents from multiple retrievers and then combines them.Re-rankingAnyYesIf you want to rank retrieved documents based upon relevance, especially if you want to combine results from multiple retrieval methods .Given a query and a list of documents, Rerank indexes the documents from most to least semantically relevant to the query.\\ntipSee our RAG from Scratch video on RAG-Fusion (paper), on approach for post-processing across multiple queries:  Rewrite the user question from multiple perspectives, retrieve documents for each rewritten question, and combine the ranks of multiple search result lists to produce a single, unified ranking with Reciprocal Rank Fusion (RRF).\\nGeneration\\u200b\\nFinally, consider ways to build self-correction into your RAG system. RAG systems can suffer from low quality retrieval (e.g., if a user question is out of the domain for the index) and / or hallucinations in generation. A naive retrieve-generate pipeline has no ability to detect or self-correct from these kinds of errors. The concept of \"flow engineering\" has been introduced in the context of code generation: iteratively build an answer to a code question with unit tests to check and self-correct errors. Several works have applied this RAG, such as Self-RAG and Corrective-RAG. In both cases, checks for document relevance, hallucinations, and / or answer quality are performed in the RAG answer generation flow.\\nWe\\'ve found that graphs are a great way to reliably express logical flows and have implemented ideas from several of these papers using LangGraph, as shown in the figure below (red - routing, blue - fallback, green - self-correction):\\n\\nRouting: Adaptive RAG (paper). Route questions to different retrieval approaches, as discussed above\\nFallback: Corrective RAG (paper). Fallback to web search if docs are not relevant to query\\nSelf-correction: Self-RAG (paper). Fix answers w/ hallucinations or don‚Äôt address question'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content=\"NameWhen to useDescriptionSelf-RAGWhen needing to fix answers with hallucinations or irrelevant content.Self-RAG performs checks for document relevance, hallucinations, and answer quality during the RAG answer generation flow, iteratively building an answer and self-correcting errors.Corrective-RAGWhen needing a fallback mechanism for low relevance docs.Corrective-RAG includes a fallback (e.g., to web search) if the retrieved documents are not relevant to the query, ensuring higher quality and more relevant retrieval.\\ntipSee several videos and cookbooks showcasing RAG with LangGraph:\\nLangGraph Corrective RAG\\nLangGraph combining Adaptive, Self-RAG, and Corrective RAG\\nCookbooks for RAG using LangGraph\\nSee our LangGraph RAG recipes with partners:\\nMeta\\nMistral\\n\\nText splitting\\u200b\\nLangChain offers many different types of text splitters.\\nThese all live in the langchain-text-splitters package.\\nTable columns:\\n\\nName: Name of the text splitter\\nClasses: Classes that implement this text splitter\\nSplits On: How this text splitter splits text\\nAdds Metadata: Whether or not this text splitter adds metadata about where each chunk came from.\\nDescription: Description of the splitter, including recommendation on when to use it.\\n\\nNameClassesSplits OnAdds MetadataDescriptionRecursiveRecursiveCharacterTextSplitter, RecursiveJsonSplitterA list of user defined charactersRecursively splits text. This splitting is trying to keep related pieces of text next to each other. This is the recommended way to start splitting text.HTMLHTMLHeaderTextSplitter, HTMLSectionSplitterHTML specific characters‚úÖSplits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML)MarkdownMarkdownHeaderTextSplitter,Markdown specific characters‚úÖSplits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown)Codemany languagesCode (Python, JS) specific charactersSplits text based on characters specific to coding languages. 15 different languages are available to choose from.Tokenmany classesTokensSplits text on tokens. There exist a few different ways to measure tokens.CharacterCharacterTextSplitterA user defined characterSplits text based on a user defined character. One of the simpler methods.Semantic Chunker (Experimental)SemanticChunkerSentencesFirst splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg KamradtIntegration: AI21 SemanticAI21SemanticTextSplitter‚úÖIdentifies distinct topics that form coherent pieces of text and splits along those.\\nEvaluation\\u200b\\n\\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\\nIt involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\\nThis process is vital for building reliable applications.\\n\\nLangSmith helps with this process in a few ways:\\n\\nIt makes it easier to create and curate datasets via its tracing and annotation features\\nIt provides an evaluation framework that helps you define metrics and run your app against your dataset\\nIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\\n\\nTo learn more, check out this LangSmith guide.\\nTracing\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel', 'title': 'Conceptual guide | ü¶úÔ∏èüîó LangChain', 'description': 'This section contains introductions to key parts of LangChain.', 'language': 'en'}, page_content='To learn more, check out this LangSmith guide.\\nTracing\\u200b\\n\\nA trace is essentially a series of steps that your application takes to go from input to output.\\nTraces contain individual steps called runs. These can be individual calls from a model, retriever,\\ntool, or sub-chains.\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\nFor a deeper dive, check out this LangSmith conceptual guide.Edit this pageWas this page helpful?PreviousHow to create and query vector storesNextv0.3Architecturelangchain-corelangchainlangchain-communityPartner packageslanggraphlangserveLangSmithLangChain Expression Language (LCEL)Runnable interfaceComponentsChat modelsLLMsMessagesPrompt templatesExample selectorsOutput parsersChat historyDocumentsDocument loadersText splittersEmbedding modelsVector storesRetrieversKey-value storesToolsToolkitsAgentsCallbacksTechniquesStreamingFunction/tool callingStructured outputFew-shot promptingRetrievalText splittingEvaluationTracingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='How to install LangChain packages | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to install LangChain packagesOn this pageHow to install LangChain packages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of\\nfunctionality to install.\\nOfficial release\\u200b\\nTo install the main langchain package, run:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/installation/', 'title': 'How to install LangChain packages | ü¶úÔ∏èüîó LangChain', 'description': 'The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of', 'language': 'en'}, page_content='PipCondapip install langchainconda install langchain -c conda-forge\\nWhile this package acts as a sane starting point to using LangChain,\\nmuch of the value of LangChain comes when integrating it with various model providers, datastores, etc.\\nBy default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately, which we show below.\\nEcosystem packages\\u200b\\nWith the exception of the langsmith SDK, all packages in the LangChain ecosystem depend on langchain-core, which contains base\\nclasses and abstractions that other packages use. The dependency graph below shows how the difference packages are related.\\nA directed arrow indicates that the source package depends on the target package:\\n\\nWhen installing a package, you do not need to explicitly install that package\\'s explicit dependencies (such as langchain-core).\\nHowever, you may choose to if you are using a feature only available in a certain version of that dependency.\\nIf you do, you should make sure that the installed or pinned version is compatible with any other integration packages you use.\\nLangChain core\\u200b\\nThe langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain, but can also be used separately. Install with:\\npip install langchain-core\\nIntegration packages\\u200b\\nCertain integrations like OpenAI and Anthropic have their own packages.\\nAny integrations that require their own package will be documented as such in the Integration docs.\\nYou can see a list of all integration packages in the API reference under the \"Partner libs\" dropdown.\\nTo install one of these run:\\npip install langchain-openai\\nAny integrations that haven\\'t been split out into their own packages will live in the langchain-community package. Install with:\\npip install langchain-community\\nLangChain experimental\\u200b\\nThe langchain-experimental package holds experimental LangChain code, intended for research and experimental uses.\\nInstall with:\\npip install langchain-experimental\\nLangGraph\\u200b\\nlanggraph is a library for building stateful, multi-actor applications with LLMs. It integrates smoothly with LangChain, but can be used without it.\\nInstall with:\\npip install langgraph\\nLangServe\\u200b\\nLangServe helps developers deploy LangChain runnables and chains as a REST API.\\nLangServe is automatically installed by LangChain CLI.\\nIf not using LangChain CLI, install with:\\npip install \"langserve[all]\"\\nfor both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.\\nLangChain CLI\\u200b\\nThe LangChain CLI is useful for working with LangChain templates and other LangServe projects.\\nInstall with:\\npip install langchain-cli\\nLangSmith SDK\\u200b\\nThe LangSmith SDK is automatically installed by LangChain. However, it does not depend on\\nlangchain-core, and can be installed and used independently if desired.\\nIf you are not using LangChain, you can install it with:\\npip install langsmith\\nFrom source\\u200b\\nIf you want to install a package from source, you can do so by cloning the main LangChain repo, enter the directory of the package you want to install PATH/TO/REPO/langchain/libs/{package}, and run:\\npip install -e .\\nLangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see all repos here.Edit this pageWas this page helpful?PreviousHow to best prompt for Graph-RAGNextHow to add examples to the prompt for query analysisOfficial releaseEcosystem packagesLangChain coreIntegration packagesLangChain experimentalLangGraphLangServeLangChain CLILangSmith SDKFrom sourceCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load MarkdownOn this pageHow to load Markdown'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Markdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain', 'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en'}, page_content='Basic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load JSONOn this pageHow to load JSON'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='How to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content=\"#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path='./example_data/facebook_chat.json'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},     'is_still_participant': True,     'joinable_mode': {'link': '', 'mode': 1},     'magic_words': [],     'messages': [{'content': 'Bye!',                   'sender_name': 'User 2',                   'timestamp_ms': 1675597571851},                  {'content': 'Oh no worries! Bye',                   'sender_name': 'User 1',                   'timestamp_ms': 1675597435669},                  {'content': 'No Im sorry it was my mistake, the blue one is not '                              'for sale',                   'sender_name': 'User 2',                   'timestamp_ms': 1675596277579},                  {'content': 'I thought you were selling the blue one!',                   'sender_name': 'User 1',                   'timestamp_ms': 1675595140251},                  {'content': 'Im not interested in this bag. Im interested in the '                              'blue one!',                   'sender_name': 'User 1',                   'timestamp_ms': 1675595109305},                  {'content': 'Here is $129',                   'sender_name': 'User 2',                   'timestamp_ms': 1675595068468},                  {'photos': [{'creation_timestamp': 1675595059,                               'uri': 'url_of_some_picture.jpg'}],                   'sender_name': 'User 2',                   'timestamp_ms': 1675595060730},                  {'content': 'Online is at least $100',                   'sender_name': 'User 2',                   'timestamp_ms': 1675595045152},                  {'content': 'How much do you want?',                   'sender_name': 'User 1',                   'timestamp_ms': 1675594799696},                  {'content': 'Goodmorning! $50 is too low.',                   'sender_name': 'User 2',                   'timestamp_ms': 1675577876645},                  {'content': 'Hi! Im interested in your bag. Im offering $50. Let '                              'me know if you are interested. Thanks!',                   'sender_name': 'User 1',                   'timestamp_ms': 1675549022673}],     'participants': [{'name': 'User 1'}, {'name': 'User 2'}],     'thread_path': 'inbox/User 1 and User 2 chat',     'title': 'User 1 and User 2 chat'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path='./example_data/facebook_chat.json',    jq_schema='.messages[].content',    text_content=False)data = loader.load()\\npprint(data)\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='JSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Another option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content=\"pprint(data)\\n    [Document(page_content='Bye!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 1, 'sender_name': 'User 2', 'timestamp_ms': 1675597571851}),     Document(page_content='Oh no worries! Bye', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 2, 'sender_name': 'User 1', 'timestamp_ms': 1675597435669}),     Document(page_content='No Im sorry it was my mistake, the blue one is not for sale', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 3, 'sender_name': 'User 2', 'timestamp_ms': 1675596277579}),     Document(page_content='I thought you were selling the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 4, 'sender_name': 'User 1', 'timestamp_ms': 1675595140251}),     Document(page_content='Im not interested in this bag. Im interested in the blue one!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 5, 'sender_name': 'User 1', 'timestamp_ms': 1675595109305}),     Document(page_content='Here is $129', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 6, 'sender_name': 'User 2', 'timestamp_ms': 1675595068468}),     Document(page_content='', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 7, 'sender_name': 'User 2', 'timestamp_ms': 1675595060730}),     Document(page_content='Online is at least $100', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 8, 'sender_name': 'User 2', 'timestamp_ms': 1675595045152}),     Document(page_content='How much do you want?', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 9, 'sender_name': 'User 1', 'timestamp_ms': 1675594799696}),     Document(page_content='Goodmorning! $50 is too low.', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 10, 'sender_name': 'User 2', 'timestamp_ms': 1675577876645}),     Document(page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!', metadata={'source': '/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json', 'seq_num': 11, 'sender_name': 'User 1', 'timestamp_ms': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='The example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain', 'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en'}, page_content='Common JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='How to load PDFs | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load PDFsOn this pageHow to load PDFs'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.\\nThis guide covers how to load PDF documents into the LangChain Document format that we use downstream.\\nText in PDFs is typically represented via text boxes. They may also contain images. A PDF parser might do some combination of the following:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Agglomerate text boxes into lines, paragraphs, and other structures via heuristics or ML inference;\\nRun OCR on images to detect text therein;\\nClassify text as belonging to paragraphs, lists, tables, or other structures;\\nStructure text into table rows and columns, or key-value pairs.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='LangChain integrates with a host of PDF parsers. Some are simple and relatively low-level; others will support OCR and image-processing, or perform advanced document layout analysis. The right choice will depend on your needs. Below we enumerate the possibilities.\\nWe will demonstrate these approaches on a sample file:\\nfile_path = (    \"../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\")\\nA note on multimodal modelsMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications -- such as question-answering over PDFs with complex layouts, diagrams, or scans -- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. We demonstrate an example of this in the Use of multimodal models section below.\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a PDF, the method below is appropriate. It will return a list of Document objects-- one per page-- containing a single string of the page\\'s text in the Document\\'s page_content attribute. It will not parse text in images or scanned PDF pages. Under the hood it uses the pypydf Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\n%pip install -qU pypdf\\nfrom langchain_community.document_loaders import PyPDFLoaderloader = PyPDFLoader(file_path)pages = []async for page in loader.alazy_load():    pages.append(page)API Reference:PyPDFLoader\\nprint(f\"{pages[0].metadata}\\\\n\")print(pages[0].page_content)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='print(f\"{pages[0].metadata}\\\\n\")print(pages[0].page_content)\\n{\\'source\\': \\'../../docs/integrations/document_loaders/example_data/layout-parser-paper.pdf\\', \\'page\\': 0}LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University{melissadell,jacob carlson }@fas.harvard.edu4University of Washingtonbcgl@cs.washington.edu5University of Waterloow422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have beenprimarily driven by the application of neural networks. Ideally, researchoutcomes could be easily deployed in production and extended for furtherinvestigation. However, various factors like loosely organized codebasesand sophisticated model conÔ¨Ågurations complicate the easy reuse of im-portant innovations by a wide audience. Though there have been on-goingeÔ¨Äorts to improve reusability and simplify deep learning (DL) modeldevelopment in disciplines like natural language processing and computervision, none of them are optimized for challenges in the domain of DIA.This represents a major gap in the existing toolkit, as DIA is central toacademic research across a wide range of disciplines in the social sciencesand humanities. This paper introduces LayoutParser , an open-sourcelibrary for streamlining the usage of DL in DIA research and applica-tions. The core LayoutParser library comes with a set of simple andintuitive interfaces for applying and customizing DL models for layout de-tection, character recognition, and many other document processing tasks.To promote extensibility, LayoutParser also incorporates a communityplatform for sharing both pre-trained models and full document digiti-zation pipelines. We demonstrate that LayoutParser is helpful for bothlightweight and large-scale digitization pipelines in real-word use cases.The library is publicly available at https://layout-parser.github.io .Keywords: Document Image Analysis ¬∑Deep Learning ¬∑Layout Analysis¬∑Character Recognition ¬∑Open Source library ¬∑Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range ofdocument image analysis (DIA) tasks including document image classiÔ¨Åcation [ 11,arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\nNote that the metadata of each document stores the corresponding page number.\\nVector search over PDFs\\u200b\\nOnce we have loaded PDFs into LangChain Document objects, we can index them (e.g., a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(pages, OpenAIEmbeddings())docs = vector_store.similarity_search(\"What is LayoutParser?\", k=2)for doc in docs:    print(f\\'Page {doc.metadata[\"page\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nPage 13: 14 Z. Shen et al.6 ConclusionLayoutParser provides a comprehensive toolkit for deep learning-based documentimage analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used tobuild Ô¨Çexible and accurate pipelines for processing documents with complicatedstructures. It also supports hiPage 0: LayoutParser : A UniÔ¨Åed Toolkit for DeepLearning Based Document Image AnalysisZejiang Shen1( ÔøΩ), Ruochen Zhang2, Melissa Dell3, Benjamin Charles GermainLee4, Jacob Carlson3, and Weining Li51Allen Institute for AIshannons@allenai.org2Brown Universityruochen zhang@brown.edu3Harvard University\\nLayout analysis and extraction of text from images\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content=\"Layout analysis and extraction of text from images\\u200b\\nIf you require a more granular segmentation of text (e.g., into distinct paragraphs, titles, tables, or other structures) or require extraction of text from images, the method below is appropriate. It will return a list of Document objects, where each object represents a structure on the page. The Document's metadata stores the page number and other information related to the object (e.g., it might store table rows and columns in the case of a table object).\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nUnstructured supports multiple parameters for PDF parsing:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='strategy (e.g., \"fast\" or \"hi-res\")\\nAPI or local processing. You will need an API key to use the API.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='The hi-res strategy provides support for document layout analysis and OCR. We demonstrate it below via the API. See local parsing section below for considerations when running locally.\\n%pip install -qU langchain-unstructured\\nimport getpassimport osif \"UNSTRUCTURED_API_KEY\" not in os.environ:    os.environ[\"UNSTRUCTURED_API_KEY\"] = getpass.getpass(\"Unstructured API Key:\")\\nUnstructured API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\\nAs before, we initialize a loader and load documents lazily:\\nfrom langchain_unstructured import UnstructuredLoaderloader = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",    partition_via_api=True,    coordinates=True,)docs = []for doc in loader.lazy_load():    docs.append(doc)\\nINFO: Preparing to split document for partition.INFO: Starting page number set to 1INFO: Allow failed set to 0INFO: Concurrency level set to 5INFO: Splitting pages 1 to 16 (16 total)INFO: Determined optimal split size of 4 pages.INFO: Partitioning 4 files with 4 page(s) each.INFO: Partitioning set #1 (pages 1-4).INFO: Partitioning set #2 (pages 5-8).INFO: Partitioning set #3 (pages 9-12).INFO: Partitioning set #4 (pages 13-16).INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"INFO: Successfully partitioned set #1, elements added to the final result.INFO: Successfully partitioned set #2, elements added to the final result.INFO: Successfully partitioned set #3, elements added to the final result.INFO: Successfully partitioned set #4, elements added to the final result.\\nHere we recover 171 distinct structures over the 16 page document:\\nprint(len(docs))\\n171\\nWe can use the document metadata to recover content from a single page:\\nfirst_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='first_page_docs = [doc for doc in docs if doc.metadata.get(\"page_number\") == 1]for doc in first_page_docs:    print(doc.page_content)\\nLayoutParser: A UniÔ¨Åed Toolkit for Deep Learning Based Document Image Analysis1 2 0 2 n u J 1 2 ] V C . s c [ 2 v 8 4 3 5 1 . 3 0 1 2 : v i X r aZejiang Shen¬Æ (<), Ruochen Zhang?, Melissa Dell¬Æ, Benjamin Charles Germain Lee?, Jacob Carlson¬Æ, and Weining Li¬Æ1 Allen Institute for AI shannons@allenai.org 2 Brown University ruochen zhang@brown.edu 3 Harvard University {melissadell,jacob carlson}@fas.harvard.edu 4 University of Washington bcgl@cs.washington.edu 5 University of Waterloo w422li@uwaterloo.caAbstract. Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model conÔ¨Ågurations complicate the easy reuse of im- portant innovations by a wide audience. Though there have been on-going eÔ¨Äorts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces LayoutParser, an open-source library for streamlining the usage of DL in DIA research and applica- tions. The core LayoutParser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout de- tection, character recognition, and many other document processing tasks. To promote extensibility, LayoutParser also incorporates a community platform for sharing both pre-trained models and full document digiti- zation pipelines. We demonstrate that LayoutParser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io.Keywords: Document Image Analysis ¬∑ Deep Learning ¬∑ Layout Analysis ¬∑ Character Recognition ¬∑ Open Source library ¬∑ Toolkit.1 IntroductionDeep Learning(DL)-based approaches are the state-of-the-art for a wide range of document image analysis (DIA) tasks including document image classiÔ¨Åcation [11,\\nExtracting tables and other structures\\u200b\\nEach Document we load represents a structure, like a title, paragraph, or table.\\nSome structures may be of special interest for indexing or question-answering tasks. These structures may be:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Classified for easy identification;\\nParsed into a more structured representation.\\n\\nBelow, we identify and extract a table:\\nClick to expand code for rendering pages%pip install -qU matplotlib PyMuPDF pillowimport fitzimport matplotlib.patches as patchesimport matplotlib.pyplot as pltfrom PIL import Imagedef plot_pdf_with_boxes(pdf_page, segments):    pix = pdf_page.get_pixmap()    pil_image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    fig, ax = plt.subplots(1, figsize=(10, 10))    ax.imshow(pil_image)    categories = set()    category_to_color = {        \"Title\": \"orchid\",        \"Image\": \"forestgreen\",        \"Table\": \"tomato\",    }    for segment in segments:        points = segment[\"coordinates\"][\"points\"]        layout_width = segment[\"coordinates\"][\"layout_width\"]        layout_height = segment[\"coordinates\"][\"layout_height\"]        scaled_points = [            (x * pix.width / layout_width, y * pix.height / layout_height)            for x, y in points        ]        box_color = category_to_color.get(segment[\"category\"], \"deepskyblue\")        categories.add(segment[\"category\"])        rect = patches.Polygon(            scaled_points, linewidth=1, edgecolor=box_color, facecolor=\"none\"        )        ax.add_patch(rect)    # Make legend    legend_handles = [patches.Patch(color=\"deepskyblue\", label=\"Text\")]    for category in [\"Title\", \"Image\", \"Table\"]:        if category in categories:            legend_handles.append(                patches.Patch(color=category_to_color[category], label=category)            )    ax.axis(\"off\")    ax.legend(handles=legend_handles, loc=\"upper right\")    plt.tight_layout()    plt.show()def render_page(doc_list: list, page_number: int, print_text=True) -> None:    pdf_page = fitz.open(file_path).load_page(page_number - 1)    page_docs = [        doc for doc in doc_list if doc.metadata.get(\"page_number\") == page_number    ]    segments = [doc.metadata for doc in page_docs]    plot_pdf_with_boxes(pdf_page, segments)    if print_text:        for doc in page_docs:            print(f\"{doc.page_content}\\\\n\")\\nrender_page(docs, 5)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='LayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIA5Table 1: Current layout detection models in the LayoutParser model zooDataset Base Model1 Large Model Notes PubLayNet [38] PRImA [3] Newspaper [17] TableBank [18] HJDataset [31] F / M M F F F / M M - - F - Layouts of modern scientiÔ¨Åc documents Layouts of scanned modern magazines and scientiÔ¨Åc reports Layouts of scanned US newspapers from the 20th century Table region on modern scientiÔ¨Åc and business document Layouts of history Japanese documents1 For each dataset, we train several models of diÔ¨Äerent sizes for diÔ¨Äerent needs (the trade-oÔ¨Ä between accuracy vs. computational cost). For ‚Äúbase model‚Äù and ‚Äúlarge model‚Äù, we refer to using the ResNet 50 or ResNet 101 backbones [13], respectively. One can train models of diÔ¨Äerent architectures, like Faster R-CNN [28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model zoo in coming months.layout data structures, which are optimized for eÔ¨Éciency and versatility. 3) When necessary, users can employ existing or customized OCR models via the uniÔ¨Åed API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component.3.1 Layout Detection ModelsIn LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. DiÔ¨Äerent from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:1 import layoutparser as lp 2 image = cv2 . imread ( \" image_file \" ) # load images 3 model = lp . De t e c tro n2 Lay outM odel ( \" lp :// PubLayNet / f as t er _ r c nn _ R _ 50 _ F P N_ 3 x / config \" ) 4 5 layout = model . detect ( image )LayoutParser provides a wealth of pre-trained model weights using various datasets covering diÔ¨Äerent languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are signiÔ¨Åcantly diÔ¨Äerent from the training dataset. As document structures and layouts vary greatly in diÔ¨Äerent domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name lp://<dataset-name>/<model-architecture-name>.\\nNote that although the table text is collapsed into a single string in the document\\'s content, the metadata contains a representation of its rows and columns:\\nfrom IPython.display import HTML, displaysegments = [    doc.metadata    for doc in docs    if doc.metadata.get(\"page_number\") == 5 and doc.metadata.get(\"category\") == \"Table\"]display(HTML(segments[0][\"text_as_html\"]))\\nable 1. LUllclll 1ayoul actCCLloll 1110AdCs 111 L1C LayoOulralsel 1110U4cl 200Dataset| Base Model\\'|NotesPubLayNet [38]F/MLayouts of modern scientific documentsPRImAMLayouts of scanned modern magazines and scientific reportsNewspaperFLayouts of scanned US newspapers from the 20th centuryTableBank [18]FTable region on modern scientific and business documentHJDatasetF/MLayouts of history Japanese documents\\nExtracting text from specific sections\\u200b'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Extracting text from specific sections\\u200b\\nStructures may have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nBelow, we extract all text associated with the document\\'s \"Conclusion\" section:\\nrender_page(docs, 14, print_text=False)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='conclusion_docs = []parent_id = -1for doc in docs:    if doc.metadata[\"category\"] == \"Title\" and \"Conclusion\" in doc.page_content:        parent_id = doc.metadata[\"element_id\"]    if doc.metadata.get(\"parent_id\") == parent_id:        conclusion_docs.append(doc)for doc in conclusion_docs:    print(doc.page_content)\\nLayoutParser provides a comprehensive toolkit for deep learning-based document image analysis. The oÔ¨Ä-the-shelf library is easy to install, and can be used to build Ô¨Çexible and accurate pipelines for processing documents with complicated structures. It also supports high-level customization and enables easy labeling and training of DL models on unique document image datasets. The LayoutParser community platform facilitates sharing DL models and DIA pipelines, inviting discussion and promoting code reproducibility and reusability. The LayoutParser team is committed to keeping the library updated continuously and bringing the most recent advances in DL-based DIA, such as multi-modal document modeling [37, 36, 9] (an upcoming priority), to a diverse audience of end-users.Acknowledgements We thank the anonymous reviewers for their comments and suggestions. This project is supported in part by NSF Grant OIA-2033558 and funding from the Harvard Data Science Initiative and Harvard Catalyst. Zejiang Shen thanks Doug Downey for suggestions.\\nExtracting text from images\\u200b\\nOCR is run on images, enabling the extraction of text therein:\\nrender_page(docs, 11)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='LayoutParser: A UniÔ¨Åed Toolkit for DL-Based DIAfocuses on precision, eÔ¨Éciency, and robustness. The target documents may have complicated structures, and may require training multiple layout detection models to achieve the optimal accuracy. Light-weight pipelines are built for relatively simple documents, with an emphasis on development ease, speed and Ô¨Çexibility. Ideally one only needs to use existing resources, and model training should be avoided. Through two exemplar projects, we show how practitioners in both academia and industry can easily build such pipelines using LayoutParser and extract high-quality structured document data for their downstream tasks. The source code for these projects will be publicly available in the LayoutParser community hub.115.1 A Comprehensive Historical Document Digitization PipelineThe digitization of historical documents can unlock valuable data that can shed light on many important social, economic, and historical questions. Yet due to scan noises, page wearing, and the prevalence of complicated layout structures, ob- taining a structured representation of historical document scans is often extremely complicated. In this example, LayoutParser was used to develop a comprehensive pipeline, shown in Figure 5, to gener- ate high-quality structured data from historical Japanese Ô¨Årm Ô¨Ånancial ta- bles with complicated layouts. The pipeline applies two layout models to identify diÔ¨Äerent levels of document structures and two customized OCR engines for optimized character recog- nition accuracy.‚ÄòActive Learning Layout Annotate Layout Dataset | +‚Äî‚Äî Annotation Toolkit A4 Deep Learning Layout Layout Detection Model Training & Inference, A Post-processing ‚Äî Handy Data Structures & \\\\ Lo orajport 7 ) Al Pls for Layout Data A4 Default and Customized Text Recognition 0CR Models ¬• Visualization & Export Layout Structure Visualization & Storage The Japanese Document Helpful LayoutParser Modules Digitization PipelineAs shown in Figure 4 (a), the document contains columns of text written vertically 15, a common style in Japanese. Due to scanning noise and archaic printing technology, the columns can be skewed or have vari- able widths, and hence cannot be eas- ily identiÔ¨Åed via rule-based methods. Within each column, words are sepa- rated by white spaces of variable size, and the vertical positions of objects can be an indicator of their layout type.Fig. 5: Illustration of how LayoutParser helps with the historical document digi- tization pipeline.15 A document page consists of eight rows like this. For simplicity we skip the row segmentation discussion and refer readers to the source code when available.\\nNote that the text from the figure on the right is extracted and incorporated into the content of the Document.\\nLocal parsing\\u200b\\nParsing locally requires the installation of additional dependencies.\\nPoppler (PDF analysis)\\n\\nLinux: apt-get install poppler-utils\\nMac: brew install poppler\\nWindows: https://github.com/oschwartz10612/poppler-windows\\n\\nTesseract (OCR)\\n\\nLinux: apt-get install tesseract-ocr\\nMac: brew install tesseract\\nWindows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='Tesseract (OCR)\\n\\nLinux: apt-get install tesseract-ocr\\nMac: brew install tesseract\\nWindows: https://github.com/UB-Mannheim/tesseract/wiki#tesseract-installer-for-windows\\n\\nWe will also need to install the unstructured PDF extras:\\n%pip install -qU \"unstructured[pdf]\"\\nWe can then use the UnstructuredLoader much the same way, forgoing the API key and partition_via_api setting:\\nloader_local = UnstructuredLoader(    file_path=file_path,    strategy=\"hi_res\",)docs_local = []for doc in loader_local.lazy_load():    docs_local.append(doc)\\nWARNING: This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model nameINFO: Reading PDF for file: /Users/chestercurme/repos/langchain/libs/community/tests/integration_tests/examples/layout-parser-paper.pdf ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Detecting page elements ...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: padding image by 20 for structure detectionINFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...INFO: Processing entire page OCR with tesseract...\\nThe list of documents can then be processed similarly to those obtained from the API.\\nUse of multimodal models\\u200b\\nMany modern LLMs support inference over multimodal inputs (e.g., images). In some applications-- such as question-answering over PDFs with complex layouts, diagrams, or scans-- it may be advantageous to skip the PDF parsing, instead casting a PDF page to an image and passing it to a model directly. This allows a model to reason over the two dimensional content on the page, instead of a \"one-dimensional\" string representation.\\nIn principle we can use any LangChain chat model that supports multimodal inputs. A list of these models is documented here. Below we use OpenAI\\'s gpt-4o-mini.\\nFirst we define a short utility function to convert a PDF page to a base64-encoded image:\\n%pip install -qU PyMuPDF pillow langchain-openai\\nimport base64import ioimport fitzfrom PIL import Imagedef pdf_page_to_base64(pdf_path: str, page_number: int):    pdf_document = fitz.open(pdf_path)    page = pdf_document.load_page(page_number - 1)  # input is one-indexed    pix = page.get_pixmap()    img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)    buffer = io.BytesIO()    img.save(buffer, format=\"PNG\")    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\\nfrom IPython.display import Image as IPImagefrom IPython.display import displaybase64_image = pdf_page_to_base64(file_path, 11)display(IPImage(data=base64.b64decode(base64_image)))'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_pdf/', 'title': 'How to load PDFs | ü¶úÔ∏èüîó LangChain', 'description': 'Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems.', 'language': 'en'}, page_content='We can then query the model in the usual way. Below we ask it a question on related to the diagram on the page.\\nfrom langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")API Reference:ChatOpenAI\\nfrom langchain_core.messages import HumanMessagequery = \"What is the name of the first step in the pipeline?\"message = HumanMessage(    content=[        {\"type\": \"text\", \"text\": query},        {            \"type\": \"image_url\",            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},        },    ],)response = llm.invoke([message])print(response.content)API Reference:HumanMessage\\nINFO: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"``````outputThe first step in the pipeline is \"Annotate Layout Dataset.\"\\nOther PDF loaders\\u200b\\nFor a list of available LangChain PDF loaders, please see this table.Edit this pageWas this page helpful?PreviousHow to load Microsoft Office filesNextHow to load web pagesSimple and fast text extractionVector search over PDFsLayout analysis and extraction of text from imagesExtracting tables and other structuresExtracting text from specific sectionsExtracting text from imagesLocal parsingUse of multimodal modelsOther PDF loadersCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='How to load web pages | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load web pagesOn this pageHow to load web pages'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.\\nLangChain integrates with a host of parsers that are appropriate for web pages. The right parser will depend on your needs. Below we demonstrate two possibilities:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Simple and fast parsing, in which we recover one Document per web page with its content represented as a \"flattened\" string;\\nAdvanced parsing, in which we recover multiple Document objects per page, allowing one to identify and traverse sections, links, tables, and other structures.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Setup\\u200b\\nFor the \"simple and fast\" parsing, we will need langchain-community and the beautifulsoup4 library:\\n%pip install -qU langchain-community beautifulsoup4\\nFor advanced parsing, we will use langchain-unstructured:\\n%pip install -qU langchain-unstructured\\nSimple and fast text extraction\\u200b\\nIf you are looking for a simple string representation of text that is embedded in a web page, the method below is appropriate. It will return a list of Document objects -- one per page -- containing a single string of the page\\'s text. Under the hood it uses the beautifulsoup4 Python library.\\nLangChain document loaders implement lazy_load and its async variant, alazy_load, which return iterators of Document objects. We will use these below.\\nimport bs4from langchain_community.document_loaders import WebBaseLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = WebBaseLoader(web_paths=[page_url])docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]API Reference:WebBaseLoader\\nUSER_AGENT environment variable not set, consider setting it to identify your requests.\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500].strip())\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\', \\'title\\': \\'How to add memory to chatbots | \\\\uf8ff√º¬∂√∫√î‚àè√®\\\\uf8ff√º√Æ√≥ LangChain\\', \\'description\\': \\'A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:\\', \\'language\\': \\'en\\'}How to add memory to chatbots | \\uf8ff√º¬∂√∫√î‚àè√®\\uf8ff√º√Æ√≥ LangChainSkip to main contentShare your thoughts on AI agents. Take the 3-min survey.IntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ff√º√≠¬®SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingd\\nThis is essentially a dump of the text from the page\\'s HTML. It may contain extraneous information like headings and navigation bars. If you are familiar with the expected HTML, you can specify desired <div> classes and other parameters via BeautifulSoup. Below we parse only the body text of the article:\\nloader = WebBaseLoader(    web_paths=[page_url],    bs_kwargs={        \"parse_only\": bs4.SoupStrainer(class_=\"theme-doc-markdown markdown\"),    },    bs_get_text_kwargs={\"separator\": \" | \", \"strip\": True},)docs = []async for doc in loader.alazy_load():    docs.append(doc)assert len(docs) == 1doc = docs[0]\\nprint(f\"{doc.metadata}\\\\n\")print(doc.page_content[:500])\\n{\\'source\\': \\'https://python.langchain.com/docs/how_to/chatbots_memory/\\'}How to add memory to chatbots | A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including: | Simply stuffing previous messages into a chat model prompt. | The above, but trimming old messages to reduce the amount of distracting information the model has to deal with. | More complex modifications like synthesizing summaries for long running conversations. | We\\'ll go into more detail on a few techniq\\nprint(doc.page_content[-500:])\\na greeting. Nemo then asks the AI how it is doing, and the AI responds that it is fine.\\'), | HumanMessage(content=\\'What did I say my name was?\\'), | AIMessage(content=\\'You introduced yourself as Nemo. How can I assist you today, Nemo?\\')] | Note that invoking the chain again will generate another summary generated from the initial summary plus new messages and so on. You could also design a hybrid approach where a certain number of messages are retained in chat history while others are summarized.\\nNote that this required advance technical knowledge of how the body text is represented in the underlying HTML.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='Note that this required advance technical knowledge of how the body text is represented in the underlying HTML.\\nWe can parameterize WebBaseLoader with a variety of settings, allowing for specification of request headers, rate limits, and parsers and other kwargs for BeautifulSoup. See its API reference for detail.\\nAdvanced parsing\\u200b\\nThis method is appropriate if we want more granular control or processing of the page content. Below, instead of generating one Document per page and controlling its content via BeautifulSoup, we generate multiple Document objects representing distinct structures on a page. These structures can include section titles and their corresponding body texts, lists or enumerations, tables, and more.\\nUnder the hood it uses the langchain-unstructured library. See the integration docs for more information about using Unstructured with LangChain.\\nfrom langchain_unstructured import UnstructuredLoaderpage_url = \"https://python.langchain.com/docs/how_to/chatbots_memory/\"loader = UnstructuredLoader(web_url=page_url)docs = []async for doc in loader.alazy_load():    docs.append(doc)\\nINFO: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.INFO: NumExpr defaulting to 8 threads.\\nNote that with no advance knowledge of the page HTML structure, we recover a natural organization of the body text:\\nfor doc in docs[:5]:    print(doc.page_content)\\nHow to add memory to chatbotsA key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:Simply stuffing previous messages into a chat model prompt.The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.More complex modifications like synthesizing summaries for long running conversations.ERROR! Session/line number was not unique in database. History logging moved to new session 2747\\nExtracting content from specific sections\\u200b\\nEach Document object represents an element of the page. Its metadata contains useful information, such as its category:\\nfor doc in docs[:5]:    print(f\\'{doc.metadata[\"category\"]}: {doc.page_content}\\')\\nTitle: How to add memory to chatbotsNarrativeText: A key feature of chatbots is their ability to use content of previous conversation turns as context. This state management can take several forms, including:ListItem: Simply stuffing previous messages into a chat model prompt.ListItem: The above, but trimming old messages to reduce the amount of distracting information the model has to deal with.ListItem: More complex modifications like synthesizing summaries for long running conversations.\\nElements may also have parent-child relationships -- for example, a paragraph might belong to a section with a title. If a section is of particular interest (e.g., for indexing) we can isolate the corresponding Document objects.\\nAs an example, below we load the content of the \"Setup\" sections for two web pages:\\nfrom typing import Listfrom langchain_core.documents import Documentasync def _get_setup_docs_from_url(url: str) -> List[Document]:    loader = UnstructuredLoader(web_url=url)    setup_docs = []    parent_id = -1    async for doc in loader.alazy_load():        if doc.metadata[\"category\"] == \"Title\" and doc.page_content.startswith(\"Setup\"):            parent_id = doc.metadata[\"element_id\"]        if doc.metadata.get(\"parent_id\") == parent_id:            setup_docs.append(doc)    return setup_docspage_urls = [    \"https://python.langchain.com/docs/how_to/chatbots_memory/\",    \"https://python.langchain.com/docs/how_to/chatbots_tools/\",]setup_docs = []for url in page_urls:    page_setup_docs = await _get_setup_docs_from_url(url)    setup_docs.extend(page_setup_docs)API Reference:Document\\nfrom collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_web/', 'title': 'How to load web pages | ü¶úÔ∏èüîó LangChain', 'description': 'This guide covers how to load web pages into the LangChain Document format that we use downstream. Web pages contain text, images, and other multimedia elements, and are typically represented with HTML. They may include links to other pages or resources.', 'language': 'en'}, page_content='from collections import defaultdictsetup_text = defaultdict(str)for doc in setup_docs:    url = doc.metadata[\"url\"]    setup_text[url] += f\"{doc.page_content}\\\\n\"dict(setup_text)\\n{\\'https://python.langchain.com/docs/how_to/chatbots_memory/\\': \"You\\'ll need to install a few packages, and have your OpenAI API key set as an environment variable named OPENAI_API_KEY:\\\\n%pip install --upgrade --quiet langchain langchain-openai\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\n[33mWARNING: You are using pip version 22.0.4; however, version 23.3.2 is available.\\\\nYou should consider upgrading via the \\'/Users/jacoblee/.pyenv/versions/3.10.5/bin/python -m pip install --upgrade pip\\' command.[0m[33m\\\\n[0mNote: you may need to restart the kernel to use updated packages.\\\\n\", \\'https://python.langchain.com/docs/how_to/chatbots_tools/\\': \"For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\\\nYou\\'ll need to sign up for an account on the Tavily website, and install the following packages:\\\\n%pip install --upgrade --quiet langchain-community langchain-openai tavily-python\\\\n\\\\n# Set env var OPENAI_API_KEY or load from a .env file:\\\\nimport dotenv\\\\n\\\\ndotenv.load_dotenv()\\\\nYou will also need your OpenAI key set as OPENAI_API_KEY and your Tavily API key set as TAVILY_API_KEY.\\\\n\"}\\nVector search over page content\\u200b\\nOnce we have loaded the page contents into LangChain Document objects, we can index them (e.g., for a RAG application) in the usual way. Below we use OpenAI embeddings, although any LangChain embeddings model will suffice.\\n%pip install -qU langchain-openai\\nimport getpassimport osif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\nfrom langchain_core.vectorstores import InMemoryVectorStorefrom langchain_openai import OpenAIEmbeddingsvector_store = InMemoryVectorStore.from_documents(setup_docs, OpenAIEmbeddings())retrieved_docs = vector_store.similarity_search(\"Install Tavily\", k=2)for doc in retrieved_docs:    print(f\\'Page {doc.metadata[\"url\"]}: {doc.page_content[:300]}\\\\n\\')API Reference:InMemoryVectorStore | OpenAIEmbeddings\\nINFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"INFO: HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"``````outputPage https://python.langchain.com/docs/how_to/chatbots_tools/: You\\'ll need to sign up for an account on the Tavily website, and install the following packages:Page https://python.langchain.com/docs/how_to/chatbots_tools/: For this guide, we\\'ll be using a tool calling agent with a single tool for searching the web. The default will be powered by Tavily, but you can switch it out for any similar tool. The rest of this section will assume you\\'re using Tavily.\\nOther web page loaders\\u200b\\nFor a list of available LangChain web page loaders, please see this table.Edit this pageWas this page helpful?PreviousHow to load PDFsNextHow to create a dynamic (self-constructing) chainSetupSimple and fast text extractionAdvanced parsingExtracting content from specific sectionsVector search over page contentOther web page loadersCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='How to load CSVs | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load CSVsOn this pageHow to load CSVs'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_csv/', 'title': 'How to load CSVs | ü¶úÔ∏èüîó LangChain', 'description': 'A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.', 'language': 'en'}, page_content='A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. Each line of the file is a data record. Each record consists of one or more fields, separated by commas.\\nLangChain implements a CSV Loader that will load CSV files into a sequence of Document objects. Each row of the CSV file is translated to one document.\\nfrom langchain_community.document_loaders.csv_loader import CSVLoaderfile_path = (    \"../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\")loader = CSVLoader(file_path=file_path)data = loader.load()for record in data[:2]:    print(record)API Reference:CSVLoader\\npage_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 0}page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 1}\\nCustomizing the CSV parsing and loading\\u200b\\nCSVLoader will accept a csv_args kwarg that supports customization of arguments passed to Python\\'s csv.DictReader. See the csv module documentation for more information of what csv args are supported.\\nloader = CSVLoader(    file_path=file_path,    csv_args={        \"delimiter\": \",\",        \"quotechar\": \\'\"\\',        \"fieldnames\": [\"MLB Team\", \"Payroll in millions\", \"Wins\"],    },)data = loader.load()for record in data[:2]:    print(record)\\npage_content=\\'MLB Team: Team\\\\nPayroll in millions: \"Payroll (millions)\"\\\\nWins: \"Wins\"\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 0}page_content=\\'MLB Team: Nationals\\\\nPayroll in millions: 81.34\\\\nWins: 98\\' metadata={\\'source\\': \\'../../../docs/integrations/document_loaders/example_data/mlb_teams_2012.csv\\', \\'row\\': 1}\\nSpecify a column to identify the document source\\u200b\\nThe \"source\" key on Document metadata can be set using a column of the CSV. Use the source_column argument to specify a source for the document created from each row. Otherwise file_path will be used as the source for all documents created from the CSV file.\\nThis is useful when using documents loaded from CSV files for chains that answer questions using sources.\\nloader = CSVLoader(file_path=file_path, source_column=\"Team\")data = loader.load()for record in data[:2]:    print(record)\\npage_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\' metadata={\\'source\\': \\'Nationals\\', \\'row\\': 0}page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\' metadata={\\'source\\': \\'Reds\\', \\'row\\': 1}\\nLoad from a string\\u200b\\nPython\\'s tempfile can be used when working with CSV strings directly.\\nimport tempfilefrom io import StringIOstring_data = \"\"\"\"Team\", \"Payroll (millions)\", \"Wins\"\"Nationals\",     81.34, 98\"Reds\",          82.20, 97\"Yankees\",      197.96, 95\"Giants\",       117.62, 94\"\"\".strip()with tempfile.NamedTemporaryFile(delete=False, mode=\"w+\") as temp_file:    temp_file.write(string_data)    temp_file_path = temp_file.nameloader = CSVLoader(file_path=temp_file_path)loader.load()for record in data[:2]:    print(record)\\npage_content=\\'Team: Nationals\\\\n\"Payroll (millions)\": 81.34\\\\n\"Wins\": 98\\' metadata={\\'source\\': \\'Nationals\\', \\'row\\': 0}page_content=\\'Team: Reds\\\\n\"Payroll (millions)\": 82.20\\\\n\"Wins\": 97\\' metadata={\\'source\\': \\'Reds\\', \\'row\\': 1}Edit this pageWas this page helpful?PreviousHow to debug your LLM appsNextHow to load documents from a directoryCustomizing the CSV parsing and loadingSpecify a column to identify the document sourceLoad from a stringCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='How to load documents from a directory | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load documents from a directoryOn this pageHow to load documents from a directory'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content=\"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='How to load from a filesystem, including use of wildcard patterns;\\nHow to use multithreading for file I/O;\\nHow to use custom loader classes to parse specific file types (e.g., code);\\nHow to handle errors, such as those due to decoding.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='from langchain_community.document_loaders import DirectoryLoaderAPI Reference:DirectoryLoader\\nDirectoryLoader accepts a loader_cls kwarg, which defaults to UnstructuredLoader. Unstructured supports parsing for a number of formats, such as PDF and HTML. Here we use it to read in a markdown (.md) file.\\nWe can use the glob parameter to control which files to load. Note that here it doesn\\'t load the .rst file or the .html files.\\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\")docs = loader.load()len(docs)\\n20\\nprint(docs[0].page_content[:100])\\nSecurityLangChain has a large ecosystem of integrations with various external resources like local\\nShow a progress bar\\u200b\\nBy default a progress bar will not be shown. To show a progress bar, install the tqdm library (e.g. pip install tqdm), and set the show_progress parameter to True.\\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\", show_progress=True)docs = loader.load()\\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 54.56it/s]\\nUse multithreading\\u200b\\nBy default the loading happens in one thread. In order to utilize several threads set the use_multithreading flag to true.\\nloader = DirectoryLoader(\"../\", glob=\"**/*.md\", use_multithreading=True)docs = loader.load()\\nChange loader class\\u200b\\nBy default this uses the UnstructuredLoader class. To customize the loader, specify the loader class in the loader_cls kwarg. Below we show an example using TextLoader:\\nfrom langchain_community.document_loaders import TextLoaderloader = DirectoryLoader(\"../\", glob=\"**/*.md\", loader_cls=TextLoader)docs = loader.load()API Reference:TextLoader\\nprint(docs[0].page_content[:100])\\n# SecurityLangChain has a large ecosystem of integrations with various external resources like loc\\nNotice that while the UnstructuredLoader parses Markdown headers, TextLoader does not.\\nIf you need to load Python source code files, use the PythonLoader:\\nfrom langchain_community.document_loaders import PythonLoaderloader = DirectoryLoader(\"../../../../../\", glob=\"**/*.py\", loader_cls=PythonLoader)API Reference:PythonLoader\\nAuto-detect file encodings with TextLoader\\u200b\\nDirectoryLoader can help manage errors due to variations in file encodings. Below we will attempt to load in a collection of files, one of which includes non-UTF8 encodings.\\npath = \"../../../../libs/langchain/tests/unit_tests/examples/\"loader = DirectoryLoader(path, glob=\"**/*.txt\", loader_cls=TextLoader)\\nA. Default Behavior\\u200b\\nBy default we raise an error:\\nloader.load()\\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='A. Default Behavior\\u200b\\nBy default we raise an error:\\nloader.load()\\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\n---------------------------------------------------------------------------``````outputUnicodeDecodeError                        Traceback (most recent call last)``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:43, in TextLoader.lazy_load(self)     42     with open(self.file_path, encoding=self.encoding) as f:---> 43         text = f.read()     44 except UnicodeDecodeError as e:``````outputFile ~/.pyenv/versions/3.10.4/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)    321 data = self.buffer + input--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)    323 # keep undecoded input until the next call``````outputUnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xca in position 0: invalid continuation byte``````outputThe above exception was the direct cause of the following exception:``````outputRuntimeError                              Traceback (most recent call last)``````outputCell In[10], line 1----> 1 loader.load()``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:117, in DirectoryLoader.load(self)    115 def load(self) -> List[Document]:    116     \"\"\"Load documents.\"\"\"--> 117     return list(self.lazy_load())``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:182, in DirectoryLoader.lazy_load(self)    180 else:    181     for i in items:--> 182         yield from self._lazy_load_file(i, p, pbar)    184 if pbar:    185     pbar.close()``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:220, in DirectoryLoader._lazy_load_file(self, item, path, pbar)    218     else:    219         logger.error(f\"Error loading file {str(item)}\")--> 220         raise e    221 finally:    222     if pbar:``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/directory.py:210, in DirectoryLoader._lazy_load_file(self, item, path, pbar)    208 loader = self.loader_cls(str(item), **self.loader_kwargs)    209 try:--> 210     for subdoc in loader.lazy_load():    211         yield subdoc    212 except NotImplementedError:``````outputFile ~/repos/langchain/libs/community/langchain_community/document_loaders/text.py:56, in TextLoader.lazy_load(self)     54                 continue     55     else:---> 56         raise RuntimeError(f\"Error loading {self.file_path}\") from e     57 except Exception as e:     58     raise RuntimeError(f\"Error loading {self.file_path}\") from e``````outputRuntimeError: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\nThe file example-non-utf8.txt uses a different encoding, so the load() function fails with a helpful message indicating which file failed decoding.\\nWith the default behavior of TextLoader any failure to load any of the documents will fail the whole loading process and no documents are loaded.\\nB. Silent fail\\u200b\\nWe can pass the parameter silent_errors to the DirectoryLoader to skip the files which could not be loaded and continue the load process.\\nloader = DirectoryLoader(    path, glob=\"**/*.txt\", loader_cls=TextLoader, silent_errors=True)docs = loader.load()\\nError loading file ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt: Error loading ../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\ndoc_sources = [doc.metadata[\"source\"] for doc in docs]doc_sources\\n[\\'../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt\\']\\nC. Auto detect encodings\\u200b\\nWe can also ask TextLoader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_directory/', 'title': 'How to load documents from a directory | ü¶úÔ∏èüîó LangChain', 'description': \"LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects. Here we demonstrate:\", 'language': 'en'}, page_content='C. Auto detect encodings\\u200b\\nWe can also ask TextLoader to auto detect the file encoding before failing, by passing the autodetect_encoding to the loader class.\\ntext_loader_kwargs = {\"autodetect_encoding\": True}loader = DirectoryLoader(    path, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)docs = loader.load()\\ndoc_sources = [doc.metadata[\"source\"] for doc in docs]doc_sources\\n[\\'../../../../libs/langchain/tests/unit_tests/examples/example-utf8.txt\\', \\'../../../../libs/langchain/tests/unit_tests/examples/example-non-utf8.txt\\']Edit this pageWas this page helpful?PreviousHow to load CSVsNextHow to load HTMLShow a progress barUse multithreadingChange loader classAuto-detect file encodings with TextLoaderA. Default BehaviorB. Silent failC. Auto detect encodingsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='How to load HTML | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load HTMLOn this pageHow to load HTML'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/how_to/document_loader_html/', 'title': 'How to load HTML | ü¶úÔ∏èüîó LangChain', 'description': 'The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.', 'language': 'en'}, page_content='The HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.\\nThis covers how to load HTML documents into a LangChain Document objects that we can use downstream.\\nParsing HTML files often requires specialized tools. Here we demonstrate parsing via Unstructured and BeautifulSoup4, which can be installed via pip. Head over to the integrations page to find integrations with additional services, such as Azure AI Document Intelligence or FireCrawl.\\nLoading HTML with Unstructured\\u200b\\n%pip install unstructured\\nfrom langchain_community.document_loaders import UnstructuredHTMLLoaderfile_path = \"../../docs/integrations/document_loaders/example_data/fake-content.html\"loader = UnstructuredHTMLLoader(file_path)data = loader.load()print(data)API Reference:UnstructuredHTMLLoader\\n[Document(page_content=\\'My First Heading\\\\n\\\\nMy first paragraph.\\', metadata={\\'source\\': \\'../../docs/integrations/document_loaders/example_data/fake-content.html\\'})]\\nLoading HTML with BeautifulSoup4\\u200b\\nWe can also use BeautifulSoup4 to load HTML documents using the BSHTMLLoader.  This will extract the text from the HTML into page_content, and the page title as title into metadata.\\n%pip install bs4\\nfrom langchain_community.document_loaders import BSHTMLLoaderloader = BSHTMLLoader(file_path)data = loader.load()print(data)API Reference:BSHTMLLoader\\n[Document(page_content=\\'\\\\nTest Title\\\\n\\\\n\\\\nMy First Heading\\\\nMy first paragraph.\\\\n\\\\n\\\\n\\', metadata={\\'source\\': \\'../../docs/integrations/document_loaders/example_data/fake-content.html\\', \\'title\\': \\'Test Title\\'})]Edit this pageWas this page helpful?PreviousHow to load documents from a directoryNextHow to load JSONLoading HTML with UnstructuredLoading HTML with BeautifulSoup4CommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityTutorialsBuild a Retrieval Augmented Generation (RAG) AppOn this pageBuild a Retrieval Augmented Generation (RAG) App'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content=\"One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we‚Äôll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.\\nIf you're already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techinques.\\nWhat is RAG?\\u200b\\nRAG is a technique for augmenting LLM knowledge with additional data.\\nLLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\\nLangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.\\nNote: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\\nConcepts\\u200b\\nA typical RAG application has two main components:\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\nThe most common full sequence from raw data to answer looks like:\\nIndexing\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Load: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation\\u200b\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\\n\\n\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires these langchain dependencies:\\n\\nPipConda%pip install --quiet --upgrade langchain langchain-community langchain-chromaconda install langchain langchain-community langchain-chroma -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGCHAIN_TRACING_V2=\"true\"export LANGCHAIN_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\\nPreview\\u200b\\nIn this guide we‚Äôll build an app that answers questions about the content of a website. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~20\\nlines of code:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='OpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nimport bs4from langchain import hubfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load, chunk and index the contents of the blog.loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())# Retrieve and generate using the relevant snippets of the blog.retriever = vectorstore.as_retriever()prompt = hub.pull(\"rlm/rag-prompt\")def format_docs(docs):    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt    | llm    | StrOutputParser())rag_chain.invoke(\"What is Task Decomposition?\")API Reference:WebBaseLoader | StrOutputParser | RunnablePassthrough | OpenAIEmbeddings | RecursiveCharacterTextSplitter'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='\\'Task Decomposition is a process where a complex task is broken down into smaller, simpler steps or subtasks. This technique is utilized to enhance model performance on complex tasks by making them more manageable. It can be done by using language models with simple prompting, task-specific instructions, or with human inputs.\\'\\n# cleanupvectorstore.delete_collection()\\nCheck out the LangSmith\\ntrace.\\nDetailed walkthrough\\u200b\\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs\\ngoing on.\\n1. Indexing: Load\\u200b\\nWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocuments.\\nA Document is an object with some page_content (str) and metadata\\n(dict).\\nIn this case we‚Äôll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters to the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()len(docs[0].page_content)API Reference:WebBaseLoader\\n43131\\nprint(docs[0].page_content[:500])\\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\\nGo deeper\\u200b\\nDocumentLoader: Object that loads data from a source as list of\\nDocuments.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content=\"Docs:\\nDetailed documentation on how to use DocumentLoaders.\\nIntegrations: 160+\\nintegrations to choose from.\\nInterface:\\nAPI reference \\xa0for the base interface.\\n\\n2. Indexing: Split\\u200b\\nOur loaded document is over 42k characters long. This is too long to fit\\nin the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant bits\\nof the blog post at run time.\\nIn this case we‚Äôll split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index at which each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute ‚Äústart_index‚Äù.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)API Reference:RecursiveCharacterTextSplitter\\n66\\nlen(all_splits[0].page_content)\\n969\\nall_splits[10].metadata\\n{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 7056}\\nGo deeper\\u200b\\nTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.\\n\\nLearn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n3. Indexing: Store\\u200b\\nNow we need to index our 66 text chunks so that we can search over them\\nat runtime. The most common way to do this is to embed the contents of\\neach document split and insert these embeddings into a vector database\\n(or vector store). When we want to search over our splits, we take a\\ntext search query, embed it, and perform some sort of ‚Äúsimilarity‚Äù\\nsearch to identify the stored splits with the most similar embeddings to\\nour query embedding. The simplest similarity measure is cosine\\nsimilarity ‚Äî\\xa0we measure the cosine of the angle between each pair of\\nembeddings (which are high dimensional vectors).\\nWe can embed and store all of our document splits in a single command\\nusing the Chroma\\nvector store and\\nOpenAIEmbeddings\\nmodel.\\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())API Reference:OpenAIEmbeddings\\nGo deeper\\u200b\\nEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.\\n\\nDocs: Detailed documentation on how to use embeddings.\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.\\n\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Docs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.\\n4. Retrieval and Generation: Retrieve\\u200b\\nNow let‚Äôs write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.\\nFirst we need to define our logic for searching over documents.\\nLangChain defines a\\nRetriever interface\\nwhich wraps an index that can return relevant Documents given a string\\nquery.\\nThe most common type of Retriever is the\\nVectorStoreRetriever,\\nwhich uses the similarity search capabilities of a vector store to\\nfacilitate retrieval. Any VectorStore can easily be turned into a\\nRetriever with VectorStore.as_retriever():\\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")len(retrieved_docs)\\n6\\nprint(retrieved_docs[0].page_content)\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nGo deeper\\u200b\\nVector stores are commonly used for retrieval, but there are other ways\\nto do retrieval, too.\\nRetriever: An object that returns Documents given a text query\\n\\nDocs: Further\\ndocumentation on the interface and built-in retrieval techniques.\\nSome of which include:\\n\\nMultiQueryRetriever generates variants of the input\\nquestion\\nto improve retrieval hit rate.\\nMultiVectorRetriever instead generates\\nvariants of the\\nembeddings,\\nalso in order to improve retrieval hit rate.\\nMax marginal relevance selects for relevance and\\ndiversity\\namong the retrieved documents to avoid passing in duplicate\\ncontext.\\nDocuments can be filtered during vector store retrieval using\\nmetadata filters, such as with a Self Query\\nRetriever.\\n\\n\\nIntegrations: Integrations\\nwith retrieval services.\\nInterface:\\nAPI reference for the base interface.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Integrations: Integrations\\nwith retrieval services.\\nInterface:\\nAPI reference for the base interface.\\n\\n5. Retrieval and Generation: Generate\\u200b\\nLet‚Äôs put it all together into a chain that takes a question, retrieves\\nrelevant documents, constructs a prompt, passes that to a model, and\\nparses the output.\\nWe‚Äôll use the gpt-4o-mini OpenAI chat model, but any LangChain LLM\\nor ChatModel could be substituted in.\\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024)pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).\\nfrom langchain import hubprompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"filler context\", \"question\": \"filler question\"}).to_messages()example_messages\\n[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: filler question \\\\nContext: filler context \\\\nAnswer:\")]\\nprint(example_messages[0].content)\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: filler question Context: filler context Answer:\\nWe‚Äôll use the LCEL Runnable\\nprotocol to define the chain, allowing us to\\n\\npipe together components and functions in a transparent way\\nautomatically trace our chain in LangSmith\\nget streaming, async, and batched calling out of the box.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='pipe together components and functions in a transparent way\\nautomatically trace our chain in LangSmith\\nget streaming, async, and batched calling out of the box.\\n\\nHere is the implementation:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughdef format_docs(docs):    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt    | llm    | StrOutputParser())for chunk in rag_chain.stream(\"What is Task Decomposition?\"):    print(chunk, end=\"\", flush=True)API Reference:StrOutputParser | RunnablePassthrough\\nTask Decomposition is a process where a complex task is broken down into smaller, more manageable steps or parts. This is often done using techniques like \"Chain of Thought\" or \"Tree of Thoughts\", which instruct a model to \"think step by step\" and transform large tasks into multiple simple tasks. Task decomposition can be prompted in a model, guided by task-specific instructions, or influenced by human inputs.\\nLet\\'s dissect the LCEL to understand what\\'s going on.\\nFirst: each of these components (retriever, prompt, llm, etc.) are instances of Runnable. This means that they implement the same methods-- such as sync and async .invoke, .stream, or .batch-- which makes them easier to connect together. They can be connected into a RunnableSequence-- another Runnable-- via the | operator.\\nLangChain will automatically cast certain objects to runnables when met with the | operator. Here, format_docs is cast to a RunnableLambda, and the dict with \"context\" and \"question\" is cast to a RunnableParallel. The details are less important than the bigger point, which is that each object is a Runnable.\\nLet\\'s trace how the input question flows through the above runnables.\\nAs we\\'ve seen above, the input to prompt is expected to be a dict with keys \"context\" and \"question\". So the first element of this chain builds runnables that will calculate both of these from the input question:\\n\\nretriever | format_docs passes the question through the retriever, generating Document objects, and then to format_docs to generate strings;\\nRunnablePassthrough() passes through the input question unchanged.\\n\\nThat is, if you constructed\\nchain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt)\\nThen chain.invoke(question) would build a formatted prompt, ready for inference. (Note: when developing with LCEL, it can be practical to test with sub-chains like this.)\\nThe last steps of the chain are llm, which runs the inference, and StrOutputParser(), which just plucks the string content out of the LLM\\'s output message.\\nYou can analyze the individual steps of this chain via its LangSmith\\ntrace.\\nBuilt-in chains\\u200b\\nIf preferred, LangChain includes convenience functions that implement the above LCEL. We compose two functions:\\n\\ncreate_stuff_documents_chain specifies how retrieved context is fed into a prompt and LLM. In this case, we will \"stuff\" the contents into the prompt -- i.e., we will include all retrieved context without any summarization or other processing. It largely implements our above rag_chain, with input keys context and input-- it generates an answer using retrieved context and query.\\ncreate_retrieval_chain adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. It has input key input, and includes input, context, and answer in its output.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='from langchain.chains import create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_core.prompts import ChatPromptTemplatesystem_prompt = (    \"You are an assistant for question-answering tasks. \"    \"Use the following pieces of retrieved context to answer \"    \"the question. If you don\\'t know the answer, say that you \"    \"don\\'t know. Use three sentences maximum and keep the \"    \"answer concise.\"    \"\\\\n\\\\n\"    \"{context}\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system_prompt),        (\"human\", \"{input}\"),    ])question_answer_chain = create_stuff_documents_chain(llm, prompt)rag_chain = create_retrieval_chain(retriever, question_answer_chain)response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})print(response[\"answer\"])API Reference:create_retrieval_chain | create_stuff_documents_chain | ChatPromptTemplate\\nTask Decomposition is a process in which complex tasks are broken down into smaller and simpler steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts are used to enhance model performance on these tasks. The CoT method instructs the model to think step by step, decomposing hard tasks into manageable ones, while Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thoughts.\\nReturning sources\\u200b\\nOften in Q&A applications it\\'s important to show users the sources that were used to generate the answer. LangChain\\'s built-in create_retrieval_chain will propagate retrieved source documents through to the output in the \"context\" key:\\nfor document in response[\"context\"]:    print(document)    print()'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content=\"Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Go deeper\\u200b\\nChoosing a model\\u200b\\nChatModel: An LLM-backed chat model. Takes in a sequence of messages\\nand returns a message.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain', 'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en'}, page_content='Docs\\nIntegrations: 25+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nLLM: A text-in-text-out LLM. Takes in a string and returns a string.\\n\\nDocs\\nIntegrations: 75+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nSee a guide on RAG with locally-running models\\nhere.\\nCustomizing the prompt\\u200b\\nAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized:\\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | custom_rag_prompt    | llm    | StrOutputParser())rag_chain.invoke(\"What is Task Decomposition?\")API Reference:PromptTemplate\\n\\'Task decomposition is the process of breaking down a complex task into smaller, more manageable parts. Techniques like Chain of Thought (CoT) and Tree of Thoughts allow an agent to \"think step by step\" and explore multiple reasoning possibilities, respectively. This process can be executed by a Language Model with simple prompts, task-specific instructions, or human inputs. Thanks for asking!\\'\\nCheck out the LangSmith\\ntrace\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic Q&A app over data:\\n\\nLoading data with a Document Loader\\nChunking the indexed data with a Text Splitter to make it more easily usable by a model\\nEmbedding the data and storing the data in a vectorstore\\nRetrieving the previously stored chunks in response to incoming questions\\nGenerating an answer using the retrieved chunks as context\\n\\nThere‚Äôs plenty of features, integrations, and extensions to explore in each of\\nthe above sections. Along from the Go deeper sources mentioned\\nabove, good next steps include:\\n\\nReturn sources: Learn how to return source documents\\nStreaming: Learn how to stream outputs and intermediate steps\\nAdd chat history: Learn how to add chat history to your app\\nRetrieval conceptual guide: A high-level overview of specific retrieval techniques\\nBuild a local RAG application: Create an app similar to the one above using all local components\\nEdit this pageWas this page helpful?PreviousBuild a PDF ingestion and Question/Answering systemNextVector stores and retrieversWhat is RAG?ConceptsIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithPreviewDetailed walkthrough1. Indexing: LoadGo deeper2. Indexing: SplitGo deeper3. Indexing: StoreGo deeper4. Retrieval and Generation: RetrieveGo deeper5. Retrieval and Generation: GenerateBuilt-in chainsGo deeperNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs = loader.load_and_split() # SPLIT\n",
    "langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "# Group documents by source\n",
    "grouped_docs = defaultdict(list)\n",
    "for doc in langchain_docs:\n",
    "    source = doc.metadata.get('source', '')\n",
    "    grouped_docs[source].append(doc)\n",
    "\n",
    "# Combine documents with the same source\n",
    "combined_docs = []\n",
    "for source, docs in grouped_docs.items():\n",
    "    combined_content = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    combined_metadata = docs[0].metadata.copy()  # Use metadata from the first document\n",
    "    combined_metadata['num_chunks'] = len(docs)  # Add number of original chunks\n",
    "    combined_docs.append(Document(page_content=combined_content, metadata=combined_metadata))\n",
    "\n",
    "# Replace langchain_docs with the combined documents\n",
    "langchain_docs = combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://python.langchain.com/docs/how_to/structured_output/', 'title': 'How to return structured data from a model | ü¶úÔ∏èüîó LangChain', 'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 12}, page_content='How to return structured data from a model | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nFunction/tool calling\\nIt is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\\nThe .with_structured_output() method\\u200b\\n\\nSupported modelsYou can find a list of models that support this method here.\\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or Messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\\nAs an example, let\\'s get a model to generate a joke and separate the setup from the punchline:\\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nPydantic class\\u200b\\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7)\\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model\\'s function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\\nTypedDict or JSON Schema\\u200b\\nTypedDict or JSON Schema\\u200b\\nIf you don\\'t want to use Pydantic, explicitly don\\'t want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn\\'t generate it, it is only used in defining the schema that is passed to the model.\\nRequirements\\nCore: langchain-core>=0.2.26\\nTyping extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nChoosing between multiple schemas\\u200b\\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute:\\nfrom typing import Union# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user\\'s query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\nFinalResponse(final_output=Joke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7))\\nstructured_llm.invoke(\"How are you today?\")\\nFinalResponse(final_output=ConversationalResponse(response=\"I\\'m just a bunch of code, so I don\\'t have feelings, but I\\'m here and ready to help you! How can I assist you today?\"))\\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don\\'t have to use nested schemas. See this how-to guide for more details.\\nStreaming\\u200b\\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\\ninfoNote that what\\'s yielded is already aggregated chunks, not deltas.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\\n{}{\\'setup\\': \\'\\'}{\\'setup\\': \\'Why\\'}{\\'setup\\': \\'Why was\\'}{\\'setup\\': \\'Why was the\\'}{\\'setup\\': \\'Why was the cat\\'}{\\'setup\\': \\'Why was the cat sitting\\'}{\\'setup\\': \\'Why was the cat sitting on\\'}{\\'setup\\': \\'Why was the cat sitting on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nFew-shot prompting\\u200b\\nFor more complex schemas it\\'s very useful to add few-shot examples to the prompt. This can be done in a few ways.\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don\\'t planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what\\'s something funny about woodpeckers\")API Reference:ChatPromptTemplate\\n{\\'setup\\': \\'Woodpecker\\', \\'punchline\\': \"Woodpecker who? Woodpecker who can\\'t find a tree is just a bird with a headache!\", \\'rating\\': 7}\\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you\\'re using makes use of tool calling in its API reference.\\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don\\'t planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") \\\\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\\n{\\'setup\\': \\'Crocodile\\', \\'punchline\\': \\'Crocodile be seeing you later, alligator!\\', \\'rating\\': 7}\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\\nJSON modeIf using JSON mode you\\'ll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you\\'re using supports JSON mode, check its entry in the API reference.\\nstructured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}\\n(Advanced) Raw outputs\\u200b\\nLLMs aren\\'t perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'function\\': {\\'arguments\\': \\'{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}\\', \\'name\\': \\'Joke\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 33, \\'prompt_tokens\\': 93, \\'total_tokens\\': 126}, \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_4e2b2da518\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0\\', tool_calls=[{\\'name\\': \\'Joke\\', \\'args\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}, \\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 93, \\'output_tokens\\': 33, \\'total_tokens\\': 126}), \\'parsed\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}, \\'parsing_error\\': None}\\nPrompting and parsing model outputs directly\\u200b\\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you\\'ll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\\nUsing PydanticOutputParser\\u200b\\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\\nLet‚Äôs take a look at what information is sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke(query).to_string())\\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\\\`\\\\`\\\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\\\`\\\\`\\\\`Human: Anna is 23 years old and she is 6 feet tall\\nAnd now let\\'s invoke it:\\nchain = prompt | llm | parserchain.invoke({\"query\": query})\\nPeople(people=[Person(name=\\'Anna\\', height_in_meters=1.8288)])\\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\\nCustom Parsing\\u200b\\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\\\`\\\\`\\\\`json\\\\n{schema}\\\\n\\\\`\\\\`\\\\`. \"            \"Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\\\`\\\\`\\\\`json(.*?)\\\\`\\\\`\\\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\\\`\\\\`\\\\`json{\\'title\\': \\'People\\', \\'description\\': \\'Identifying information about all people in a text.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'people\\': {\\'title\\': \\'People\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/definitions/Person\\'}}}, \\'required\\': [\\'people\\'], \\'definitions\\': {\\'Person\\': {\\'title\\': \\'Person\\', \\'description\\': \\'Information about a person.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'description\\': \\'The name of the person\\', \\'type\\': \\'string\\'}, \\'height_in_meters\\': {\\'title\\': \\'Height In Meters\\', \\'description\\': \\'The height of the person expressed in meters.\\', \\'type\\': \\'number\\'}}, \\'required\\': [\\'name\\', \\'height_in_meters\\']}}}\\\\`\\\\`\\\\`. Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\\nAnd here\\'s what it looks like when we invoke it:\\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\\n[{\\'people\\': [{\\'name\\': \\'Anna\\', \\'height_in_meters\\': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj = langchain_docs[0]\n",
    "doc_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to return structured data from a model | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nChat models\\nFunction/tool calling\\nIt is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\\nThe .with_structured_output() method\\u200b\\n\\nSupported modelsYou can find a list of models that support this method here.\\nThis is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\\nThis method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or Messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\\nAs an example, let\\'s get a model to generate a joke and separate the setup from the punchline:\\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nPydantic class\\u200b\\nIf we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\\nfrom typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\nJoke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7)\\ntipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model\\'s function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\\nTypedDict or JSON Schema\\u200b\\nTypedDict or JSON Schema\\u200b\\nIf you don\\'t want to use Pydantic, explicitly don\\'t want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn\\'t generate it, it is only used in defining the schema that is passed to the model.\\nRequirements\\nCore: langchain-core>=0.2.26\\nTyping extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nEquivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\\njson_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nChoosing between multiple schemas\\u200b\\nThe simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute:\\nfrom typing import Union# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user\\'s query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\\nFinalResponse(final_output=Joke(setup=\\'Why was the cat sitting on the computer?\\', punchline=\\'Because it wanted to keep an eye on the mouse!\\', rating=7))\\nstructured_llm.invoke(\"How are you today?\")\\nFinalResponse(final_output=ConversationalResponse(response=\"I\\'m just a bunch of code, so I don\\'t have feelings, but I\\'m here and ready to help you! How can I assist you today?\"))\\nAlternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don\\'t have to use nested schemas. See this how-to guide for more details.\\nStreaming\\u200b\\nWe can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\\ninfoNote that what\\'s yielded is already aggregated chunks, not deltas.\\nfrom typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\\n{}{\\'setup\\': \\'\\'}{\\'setup\\': \\'Why\\'}{\\'setup\\': \\'Why was\\'}{\\'setup\\': \\'Why was the\\'}{\\'setup\\': \\'Why was the cat\\'}{\\'setup\\': \\'Why was the cat sitting\\'}{\\'setup\\': \\'Why was the cat sitting on\\'}{\\'setup\\': \\'Why was the cat sitting on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}\\nFew-shot prompting\\u200b\\nFor more complex schemas it\\'s very useful to add few-shot examples to the prompt. This can be done in a few ways.\\nThe simplest and most universal way is to add examples to a system message in the prompt:\\nfrom langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don\\'t planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what\\'s something funny about woodpeckers\")API Reference:ChatPromptTemplate\\n{\\'setup\\': \\'Woodpecker\\', \\'punchline\\': \"Woodpecker who? Woodpecker who can\\'t find a tree is just a bird with a headache!\", \\'rating\\': 7}\\nWhen the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you\\'re using makes use of tool calling in its API reference.\\nfrom langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don\\'t planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo \\'vroom vroom\\', but planes go \\'zoom zoom\\'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\\Return a joke which has the setup (the response to \"Who\\'s there?\") \\\\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\\n{\\'setup\\': \\'Crocodile\\', \\'punchline\\': \\'Crocodile be seeing you later, alligator!\\', \\'rating\\': 7}\\nFor more on few shot prompting when using tool calling, see here.\\n(Advanced) Specifying the method for structuring outputs\\u200b\\nFor models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\\nJSON modeIf using JSON mode you\\'ll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you\\'re using supports JSON mode, check its entry in the API reference.\\nstructured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\\n{\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\'}\\n(Advanced) Raw outputs\\u200b\\nLLMs aren\\'t perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\nstructured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\\n{\\'raw\\': AIMessage(content=\\'\\', additional_kwargs={\\'tool_calls\\': [{\\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'function\\': {\\'arguments\\': \\'{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}\\', \\'name\\': \\'Joke\\'}, \\'type\\': \\'function\\'}]}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 33, \\'prompt_tokens\\': 93, \\'total_tokens\\': 126}, \\'model_name\\': \\'gpt-4o-2024-05-13\\', \\'system_fingerprint\\': \\'fp_4e2b2da518\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0\\', tool_calls=[{\\'name\\': \\'Joke\\', \\'args\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\', \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\', \\'rating\\': 7}, \\'id\\': \\'call_f25ZRmh8u5vHlOWfTUw8sJFZ\\', \\'type\\': \\'tool_call\\'}], usage_metadata={\\'input_tokens\\': 93, \\'output_tokens\\': 33, \\'total_tokens\\': 126}), \\'parsed\\': {\\'setup\\': \\'Why was the cat sitting on the computer?\\',  \\'punchline\\': \\'Because it wanted to keep an eye on the mouse!\\',  \\'rating\\': 7}, \\'parsing_error\\': None}\\nPrompting and parsing model outputs directly\\u200b\\nNot all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you\\'ll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\\nUsing PydanticOutputParser\\u200b\\nThe following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\\nfrom typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\\nLet‚Äôs take a look at what information is sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke(query).to_string())\\nSystem: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\\\`\\\\`\\\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\\\`\\\\`\\\\`Human: Anna is 23 years old and she is 6 feet tall\\nAnd now let\\'s invoke it:\\nchain = prompt | llm | parserchain.invoke({\"query\": query})\\nPeople(people=[Person(name=\\'Anna\\', height_in_meters=1.8288)])\\nFor a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\\nCustom Parsing\\u200b\\nYou can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\\nimport jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\\\`\\\\`\\\\`json\\\\n{schema}\\\\n\\\\`\\\\`\\\\`. \"            \"Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\\\`\\\\`\\\\`json(.*?)\\\\`\\\\`\\\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\\nHere is the prompt sent to the model:\\nquery = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\\nSystem: Answer the user query. Output your answer as JSON that  matches the given schema: \\\\`\\\\`\\\\`json{\\'title\\': \\'People\\', \\'description\\': \\'Identifying information about all people in a text.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'people\\': {\\'title\\': \\'People\\', \\'type\\': \\'array\\', \\'items\\': {\\'$ref\\': \\'#/definitions/Person\\'}}}, \\'required\\': [\\'people\\'], \\'definitions\\': {\\'Person\\': {\\'title\\': \\'Person\\', \\'description\\': \\'Information about a person.\\', \\'type\\': \\'object\\', \\'properties\\': {\\'name\\': {\\'title\\': \\'Name\\', \\'description\\': \\'The name of the person\\', \\'type\\': \\'string\\'}, \\'height_in_meters\\': {\\'title\\': \\'Height In Meters\\', \\'description\\': \\'The height of the person expressed in meters.\\', \\'type\\': \\'number\\'}}, \\'required\\': [\\'name\\', \\'height_in_meters\\']}}}\\\\`\\\\`\\\\`. Make sure to wrap the answer in \\\\`\\\\`\\\\`json and \\\\`\\\\`\\\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\\nAnd here\\'s what it looks like when we invoke it:\\nchain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\\n[{\\'people\\': [{\\'name\\': \\'Anna\\', \\'height_in_meters\\': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_obj.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(langchain_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "How to return structured data from a model | ü¶úÔ∏èüîó LangChain\n",
       "Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\n",
       "Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to return structured data from a modelOn this pageHow to return structured data from a model\n",
       "PrerequisitesThis guide assumes familiarity with the following concepts:\n",
       "Chat models\n",
       "Function/tool calling\n",
       "It is often useful to have a model return output that matches a specific schema. One common use-case is extracting data from text to insert into a database or use with some other downstream system. This guide covers a few strategies for getting structured outputs from a model.\n",
       "The .with_structured_output() method‚Äã\n",
       "\n",
       "Supported modelsYou can find a list of models that support this method here.\n",
       "This is the easiest and most reliable way to get structured outputs. with_structured_output() is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\n",
       "This method takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or Messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.\n",
       "As an example, let's get a model to generate a joke and separate the setup from the punchline:\n",
       "OpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\n",
       "Pydantic class‚Äã\n",
       "If we want the model to return a Pydantic object, we just need to pass in the desired Pydantic class. The key advantage of using Pydantic is that the model-generated output will be validated. Pydantic will raise an error if any required fields are missing or if any fields are of the wrong type.\n",
       "from typing import Optionalfrom pydantic import BaseModel, Field# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7)\n",
       "tipBeyond just the structure of the Pydantic class, the name of the Pydantic class, the docstring, and the names and provided descriptions of parameters are very important. Most of the time with_structured_output is using a model's function/tool calling API, and you can effectively think of all of this information as being added to the model prompt.\n",
       "TypedDict or JSON Schema‚Äã\n",
       "TypedDict or JSON Schema‚Äã\n",
       "If you don't want to use Pydantic, explicitly don't want validation of the arguments, or want to be able to stream the model outputs, you can define your schema using a TypedDict class. We can optionally use a special Annotated syntax supported by LangChain that allows you to specify the default value and description of a field. Note, the default value is not filled in automatically if the model doesn't generate it, it is only used in defining the schema that is passed to the model.\n",
       "Requirements\n",
       "Core: langchain-core>=0.2.26\n",
       "Typing extensions: It is highly recommended to import Annotated and TypedDict from typing_extensions instead of typing to ensure consistent behavior across Python versions.\n",
       "from typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    # Alternatively, we could have specified setup as:    # setup: str                    # no default, no description    # setup: Annotated[str, ...]    # no default, no description    # setup: Annotated[str, \"foo\"]  # default, no description    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n",
       "Equivalently, we can pass in a JSON Schema dict. This requires no imports or classes and makes it very clear exactly how each parameter is documented, at the cost of being a bit more verbose.\n",
       "json_schema = {    \"title\": \"joke\",    \"description\": \"Joke to tell user.\",    \"type\": \"object\",    \"properties\": {        \"setup\": {            \"type\": \"string\",            \"description\": \"The setup of the joke\",        },        \"punchline\": {            \"type\": \"string\",            \"description\": \"The punchline to the joke\",        },        \"rating\": {            \"type\": \"integer\",            \"description\": \"How funny the joke is, from 1 to 10\",            \"default\": None,        },    },    \"required\": [\"setup\", \"punchline\"],}structured_llm = llm.with_structured_output(json_schema)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n",
       "Choosing between multiple schemas‚Äã\n",
       "The simplest way to let the model choose from multiple schemas is to create a parent schema that has a Union-typed attribute:\n",
       "from typing import Union# Pydanticclass Joke(BaseModel):    \"\"\"Joke to tell user.\"\"\"    setup: str = Field(description=\"The setup of the joke\")    punchline: str = Field(description=\"The punchline to the joke\")    rating: Optional[int] = Field(        default=None, description=\"How funny the joke is, from 1 to 10\"    )class ConversationalResponse(BaseModel):    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"    response: str = Field(description=\"A conversational response to the user's query\")class FinalResponse(BaseModel):    final_output: Union[Joke, ConversationalResponse]structured_llm = llm.with_structured_output(FinalResponse)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "FinalResponse(final_output=Joke(setup='Why was the cat sitting on the computer?', punchline='Because it wanted to keep an eye on the mouse!', rating=7))\n",
       "structured_llm.invoke(\"How are you today?\")\n",
       "FinalResponse(final_output=ConversationalResponse(response=\"I'm just a bunch of code, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\"))\n",
       "Alternatively, you can use tool calling directly to allow the model to choose between options, if your chosen model supports it. This involves a bit more parsing and setup but in some instances leads to better performance because you don't have to use nested schemas. See this how-to guide for more details.\n",
       "Streaming‚Äã\n",
       "We can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a TypedDict class or  JSON Schema dict).\n",
       "infoNote that what's yielded is already aggregated chunks, not deltas.\n",
       "from typing_extensions import Annotated, TypedDict# TypedDictclass Joke(TypedDict):    \"\"\"Joke to tell user.\"\"\"    setup: Annotated[str, ..., \"The setup of the joke\"]    punchline: Annotated[str, ..., \"The punchline of the joke\"]    rating: Annotated[Optional[int], None, \"How funny the joke is, from 1 to 10\"]structured_llm = llm.with_structured_output(Joke)for chunk in structured_llm.stream(\"Tell me a joke about cats\"):    print(chunk)\n",
       "{}{'setup': ''}{'setup': 'Why'}{'setup': 'Why was'}{'setup': 'Why was the'}{'setup': 'Why was the cat'}{'setup': 'Why was the cat sitting'}{'setup': 'Why was the cat sitting on'}{'setup': 'Why was the cat sitting on the'}{'setup': 'Why was the cat sitting on the computer'}{'setup': 'Why was the cat sitting on the computer?'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': ''}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}\n",
       "Few-shot prompting‚Äã\n",
       "For more complex schemas it's very useful to add few-shot examples to the prompt. This can be done in a few ways.\n",
       "The simplest and most universal way is to add examples to a system message in the prompt:\n",
       "from langchain_core.prompts import ChatPromptTemplatesystem = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").Here are some examples of jokes:example_user: Tell me a joke about planesexample_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}example_user: Tell me another joke about planesexample_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}example_user: Now about caterpillarsexample_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")API Reference:ChatPromptTemplate\n",
       "{'setup': 'Woodpecker', 'punchline': \"Woodpecker who? Woodpecker who can't find a tree is just a bird with a headache!\", 'rating': 7}\n",
       "When the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.\n",
       "from langchain_core.messages import AIMessage, HumanMessage, ToolMessageexamples = [    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Why don't planes ever get tired?\",                    \"punchline\": \"Because they have rest wings!\",                    \"rating\": 2,                },                \"id\": \"1\",            }        ],    ),    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.    ToolMessage(\"\", tool_call_id=\"1\"),    # Some models also expect an AIMessage to follow any ToolMessages,    # so you may need to add an AIMessage here.    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),    AIMessage(        \"\",        name=\"example_assistant\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Cargo\",                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",                    \"rating\": 10,                },                \"id\": \"2\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"2\"),    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),    AIMessage(        \"\",        tool_calls=[            {                \"name\": \"joke\",                \"args\": {                    \"setup\": \"Caterpillar\",                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",                    \"rating\": 5,                },                \"id\": \"3\",            }        ],    ),    ToolMessage(\"\", tool_call_id=\"3\"),]system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\Return a joke which has the setup (the response to \"Who's there?\") \\and the final punchline (the response to \"<setup> who?\").\"\"\"prompt = ChatPromptTemplate.from_messages(    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")])few_shot_structured_llm = prompt | structured_llmfew_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})API Reference:AIMessage | HumanMessage | ToolMessage\n",
       "{'setup': 'Crocodile', 'punchline': 'Crocodile be seeing you later, alligator!', 'rating': 7}\n",
       "For more on few shot prompting when using tool calling, see here.\n",
       "(Advanced) Specifying the method for structuring outputs‚Äã\n",
       "For models that support more than one means of structuring outputs (i.e., they support both tool calling and JSON mode), you can specify which method to use with the method= argument.\n",
       "JSON modeIf using JSON mode you'll have to still specify the desired schema in the model prompt. The schema you pass to with_structured_output will only be used for parsing the model outputs, it will not be passed to the model the way it is with tool calling.To see if the model you're using supports JSON mode, check its entry in the API reference.\n",
       "structured_llm = llm.with_structured_output(None, method=\"json_mode\")structured_llm.invoke(    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\")\n",
       "{'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!'}\n",
       "(Advanced) Raw outputs‚Äã\n",
       "LLMs aren't perfect at generating structured output, especially as schemas become complex. You can avoid raising exceptions and handle the raw output yourself by passing include_raw=True. This changes the output format to contain the raw message output, the parsed value (if successful), and any resulting errors:\n",
       "structured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "structured_llm = llm.with_structured_output(Joke, include_raw=True)structured_llm.invoke(\"Tell me a joke about cats\")\n",
       "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'function': {'arguments': '{\"setup\":\"Why was the cat sitting on the computer?\",\"punchline\":\"Because it wanted to keep an eye on the mouse!\",\"rating\":7}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 93, 'total_tokens': 126}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_4e2b2da518', 'finish_reason': 'stop', 'logprobs': None}, id='run-d880d7e2-df08-4e9e-ad92-dfc29f2fd52f-0', tool_calls=[{'name': 'Joke', 'args': {'setup': 'Why was the cat sitting on the computer?', 'punchline': 'Because it wanted to keep an eye on the mouse!', 'rating': 7}, 'id': 'call_f25ZRmh8u5vHlOWfTUw8sJFZ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 93, 'output_tokens': 33, 'total_tokens': 126}), 'parsed': {'setup': 'Why was the cat sitting on the computer?',  'punchline': 'Because it wanted to keep an eye on the mouse!',  'rating': 7}, 'parsing_error': None}\n",
       "Prompting and parsing model outputs directly‚Äã\n",
       "Not all models support .with_structured_output(), since not all models have tool calling or JSON mode support. For such models you'll need to directly prompt the model to use a specific format, and use an output parser to extract the structured response from the raw model output.\n",
       "Using PydanticOutputParser‚Äã\n",
       "The following example uses the built-in PydanticOutputParser to parse the output of a chat model prompted to match the given Pydantic schema. Note that we are adding format_instructions directly to the prompt from a method on the parser:\n",
       "from typing import Listfrom langchain_core.output_parsers import PydanticOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Set up a parserparser = PydanticOutputParser(pydantic_object=People)# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",        ),        (\"human\", \"{query}\"),    ]).partial(format_instructions=parser.get_format_instructions())API Reference:PydanticOutputParser | ChatPromptTemplate\n",
       "Let‚Äôs take a look at what information is sent to the model:\n",
       "query = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.invoke(query).to_string())\n",
       "System: Answer the user query. Wrap the output in `json` tagsThe output should be formatted as a JSON instance that conforms to the JSON schema below.As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.Here is the output schema:\\`\\`\\`{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\\`\\`\\`Human: Anna is 23 years old and she is 6 feet tall\n",
       "And now let's invoke it:\n",
       "chain = prompt | llm | parserchain.invoke({\"query\": query})\n",
       "People(people=[Person(name='Anna', height_in_meters=1.8288)])\n",
       "For a deeper dive into using output parsers with prompting techniques for structured output, see this guide.\n",
       "Custom Parsing‚Äã\n",
       "You can also create a custom prompt and parser with LangChain Expression Language (LCEL), using a plain function to parse the output from the model:\n",
       "import jsonimport refrom typing import Listfrom langchain_core.messages import AIMessagefrom langchain_core.prompts import ChatPromptTemplatefrom pydantic import BaseModel, Fieldclass Person(BaseModel):    \"\"\"Information about a person.\"\"\"    name: str = Field(..., description=\"The name of the person\")    height_in_meters: float = Field(        ..., description=\"The height of the person expressed in meters.\"    )class People(BaseModel):    \"\"\"Identifying information about all people in a text.\"\"\"    people: List[Person]# Promptprompt = ChatPromptTemplate.from_messages(    [        (            \"system\",            \"Answer the user query. Output your answer as JSON that  \"            \"matches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`. \"            \"Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags\",        ),        (\"human\", \"{query}\"),    ]).partial(schema=People.schema())# Custom parserdef extract_json(message: AIMessage) -> List[dict]:    \"\"\"Extracts JSON content from a string where JSON is embedded between \\`\\`\\`json and \\`\\`\\` tags.    Parameters:        text (str): The text containing the JSON content.    Returns:        list: A list of extracted JSON strings.    \"\"\"    text = message.content    # Define the regular expression pattern to match JSON blocks    pattern = r\"\\`\\`\\`json(.*?)\\`\\`\\`\"    # Find all non-overlapping matches of the pattern in the string    matches = re.findall(pattern, text, re.DOTALL)    # Return the list of matched JSON strings, stripping any leading or trailing whitespace    try:        return [json.loads(match.strip()) for match in matches]    except Exception:        raise ValueError(f\"Failed to parse: {message}\")API Reference:AIMessage | ChatPromptTemplate\n",
       "Here is the prompt sent to the model:\n",
       "query = \"Anna is 23 years old and she is 6 feet tall\"print(prompt.format_prompt(query=query).to_string())\n",
       "System: Answer the user query. Output your answer as JSON that  matches the given schema: \\`\\`\\`json{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\\`\\`\\`. Make sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tagsHuman: Anna is 23 years old and she is 6 feet tall\n",
       "And here's what it looks like when we invoke it:\n",
       "chain = prompt | llm | extract_jsonchain.invoke({\"query\": query})\n",
       "[{'people': [{'name': 'Anna', 'height_in_meters': 1.8288}]}]Edit this pageWas this page helpful?PreviousHow to route between sub-chainsNextHow to summarize text through parallelizationThe .with_structured_output() methodPydantic classTypedDict or JSON SchemaChoosing between multiple schemasStreamingFew-shot prompting(Advanced) Specifying the method for structuring outputs(Advanced) Raw outputsPrompting and parsing model outputs directlyUsing PydanticOutputParserCustom ParsingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "Markdown(doc_obj.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example containing just a few document pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x11b3f8850>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectordb = Chroma.from_documents(langchain_docs, embedding=embeddings) # STORE\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of a [retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/#:~:text=A%20retriever%20is,Document's%20as%20output.):\n",
    "\n",
    "> A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11b3f8850>, search_kwargs={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever() \n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/#question-answering-with-rag\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOllama(model='llama3.2')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "question_answer_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method `create_stuff_documents_chain` [outputs an LCEL runnable](https://arc.net/l/quote/bnsztwth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to return structured output in LangChain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How to return structured output in LangChain?',\n",
       " 'context': [Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to run custom functionsOn this pageHow to run custom functions\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables\\nYou can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators\\nUsing the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load MarkdownOn this pageHow to load Markdown\\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:\\nBasic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores\\nIn this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings\\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.', 'language': 'en', 'num_chunks': 16, 'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain'}, page_content='Build a Retrieval Augmented Generation (RAG) App | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityTutorialsBuild a Retrieval Augmented Generation (RAG) AppOn this pageBuild a Retrieval Augmented Generation (RAG) App\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application\\nover a text data source. Along the way we‚Äôll go over a typical Q&A\\narchitecture and highlight additional resources for more advanced Q&A techniques. We‚Äôll also see\\nhow LangSmith can help us trace and understand our application.\\nLangSmith will become increasingly helpful as our application grows in\\ncomplexity.\\nIf you\\'re already familiar with basic retrieval, you might also be interested in\\nthis high-level overview of different retrieval techinques.\\nWhat is RAG?\\u200b\\nRAG is a technique for augmenting LLM knowledge with additional data.\\nLLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model\\'s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\\nLangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.\\nNote: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing question/answering over SQL data.\\nConcepts\\u200b\\nA typical RAG application has two main components:\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\\nRetrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\nThe most common full sequence from raw data to answer looks like:\\nIndexing\\u200b\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won\\'t fit in a model\\'s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.\\n\\n\\nRetrieval and generation\\u200b\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data\\n\\n\\nSetup\\u200b\\nJupyter Notebook\\u200b\\nThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.\\nThis and other tutorials are perhaps most conveniently run in a Jupyter notebook. See here for instructions on how to install.\\nInstallation\\u200b\\nThis tutorial requires these langchain dependencies:\\n\\nPipConda%pip install --quiet --upgrade langchain langchain-community langchain-chromaconda install langchain langchain-community langchain-chroma -c conda-forge\\nFor more details, see our Installation guide.\\nLangSmith\\u200b\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nexport LANGCHAIN_TRACING_V2=\"true\"export LANGCHAIN_API_KEY=\"...\"\\nOr, if in a notebook, you can set them with:\\nimport getpassimport osos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\\nPreview\\u200b\\nIn this guide we‚Äôll build an app that answers questions about the content of a website. The specific website we will use is the LLM Powered Autonomous\\nAgents blog post\\nby Lilian Weng, which allows us to ask questions about the contents of\\nthe post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~20\\nlines of code:\\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nimport bs4from langchain import hubfrom langchain_chroma import Chromafrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughfrom langchain_openai import OpenAIEmbeddingsfrom langchain_text_splitters import RecursiveCharacterTextSplitter# Load, chunk and index the contents of the blog.loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs=dict(        parse_only=bs4.SoupStrainer(            class_=(\"post-content\", \"post-title\", \"post-header\")        )    ),)docs = loader.load()text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)splits = text_splitter.split_documents(docs)vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())# Retrieve and generate using the relevant snippets of the blog.retriever = vectorstore.as_retriever()prompt = hub.pull(\"rlm/rag-prompt\")def format_docs(docs):    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt    | llm    | StrOutputParser())rag_chain.invoke(\"What is Task Decomposition?\")API Reference:WebBaseLoader | StrOutputParser | RunnablePassthrough | OpenAIEmbeddings | RecursiveCharacterTextSplitter\\n\\'Task Decomposition is a process where a complex task is broken down into smaller, simpler steps or subtasks. This technique is utilized to enhance model performance on complex tasks by making them more manageable. It can be done by using language models with simple prompting, task-specific instructions, or with human inputs.\\'\\n# cleanupvectorstore.delete_collection()\\nCheck out the LangSmith\\ntrace.\\nDetailed walkthrough\\u200b\\nLet‚Äôs go through the above code step-by-step to really understand what‚Äôs\\ngoing on.\\n1. Indexing: Load\\u200b\\nWe need to first load the blog post contents. We can use\\nDocumentLoaders\\nfor this, which are objects that load in data from a source and return a\\nlist of\\nDocuments.\\nA Document is an object with some page_content (str) and metadata\\n(dict).\\nIn this case we‚Äôll use the\\nWebBaseLoader,\\nwhich uses urllib to load HTML from web URLs and BeautifulSoup to\\nparse it to text. We can customize the HTML -> text parsing by passing\\nin parameters to the BeautifulSoup parser via bs_kwargs (see\\nBeautifulSoup\\ndocs).\\nIn this case only HTML tags with class ‚Äúpost-content‚Äù, ‚Äúpost-title‚Äù, or\\n‚Äúpost-header‚Äù are relevant, so we‚Äôll remove all others.\\nimport bs4from langchain_community.document_loaders import WebBaseLoader# Only keep post title, headers, and content from the full HTML.bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))loader = WebBaseLoader(    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),    bs_kwargs={\"parse_only\": bs4_strainer},)docs = loader.load()len(docs[0].page_content)API Reference:WebBaseLoader\\n43131\\nprint(docs[0].page_content[:500])\\n      LLM Powered Autonomous Agents    Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian WengBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.Agent System Overview#In\\nGo deeper\\u200b\\nDocumentLoader: Object that loads data from a source as list of\\nDocuments.\\nDocs:\\nDetailed documentation on how to use DocumentLoaders.\\nIntegrations: 160+\\nintegrations to choose from.\\nInterface:\\nAPI reference \\xa0for the base interface.\\n\\n2. Indexing: Split\\u200b\\nOur loaded document is over 42k characters long. This is too long to fit\\nin the context window of many models. Even for those models that could\\nfit the full post in their context window, models can struggle to find\\ninformation in very long inputs.\\nTo handle this we‚Äôll split the Document into chunks for embedding and\\nvector storage. This should help us retrieve only the most relevant bits\\nof the blog post at run time.\\nIn this case we‚Äôll split our documents into chunks of 1000 characters\\nwith 200 characters of overlap between chunks. The overlap helps\\nmitigate the possibility of separating a statement from important\\ncontext related to it. We use the\\nRecursiveCharacterTextSplitter,\\nwhich will recursively split the document using common separators like\\nnew lines until each chunk is the appropriate size. This is the\\nrecommended text splitter for generic text use cases.\\nWe set add_start_index=True so that the character index at which each\\nsplit Document starts within the initial Document is preserved as\\nmetadata attribute ‚Äústart_index‚Äù.\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(    chunk_size=1000, chunk_overlap=200, add_start_index=True)all_splits = text_splitter.split_documents(docs)len(all_splits)API Reference:RecursiveCharacterTextSplitter\\n66\\nlen(all_splits[0].page_content)\\n969\\nall_splits[10].metadata\\n{\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 7056}\\nGo deeper\\u200b\\nTextSplitter: Object that splits a list of Documents into smaller\\nchunks. Subclass of DocumentTransformers.\\n\\nLearn more about splitting text using different methods by reading the how-to docs\\nCode (py or js)\\nScientific papers\\nInterface: API reference for the base interface.\\n\\nDocumentTransformer: Object that performs a transformation on a list\\nof Document objects.\\n\\nDocs: Detailed documentation on how to use DocumentTransformers\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n3. Indexing: Store\\u200b\\nNow we need to index our 66 text chunks so that we can search over them\\nat runtime. The most common way to do this is to embed the contents of\\neach document split and insert these embeddings into a vector database\\n(or vector store). When we want to search over our splits, we take a\\ntext search query, embed it, and perform some sort of ‚Äúsimilarity‚Äù\\nsearch to identify the stored splits with the most similar embeddings to\\nour query embedding. The simplest similarity measure is cosine\\nsimilarity ‚Äî\\xa0we measure the cosine of the angle between each pair of\\nembeddings (which are high dimensional vectors).\\nWe can embed and store all of our document splits in a single command\\nusing the Chroma\\nvector store and\\nOpenAIEmbeddings\\nmodel.\\nfrom langchain_chroma import Chromafrom langchain_openai import OpenAIEmbeddingsvectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())API Reference:OpenAIEmbeddings\\nGo deeper\\u200b\\nEmbeddings: Wrapper around a text embedding model, used for converting\\ntext to embeddings.\\n\\nDocs: Detailed documentation on how to use embeddings.\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and\\nquerying embeddings.\\n\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\nDocs: Detailed documentation on how to use vector stores.\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point\\nwe have a query-able vector store containing the chunked contents of our\\nblog post. Given a user question, we should ideally be able to return\\nthe snippets of the blog post that answer the question.\\n4. Retrieval and Generation: Retrieve\\u200b\\nNow let‚Äôs write the actual application logic. We want to create a simple\\napplication that takes a user question, searches for documents relevant\\nto that question, passes the retrieved documents and initial question to\\na model, and returns an answer.\\nFirst we need to define our logic for searching over documents.\\nLangChain defines a\\nRetriever interface\\nwhich wraps an index that can return relevant Documents given a string\\nquery.\\nThe most common type of Retriever is the\\nVectorStoreRetriever,\\nwhich uses the similarity search capabilities of a vector store to\\nfacilitate retrieval. Any VectorStore can easily be turned into a\\nRetriever with VectorStore.as_retriever():\\nretriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")len(retrieved_docs)\\n6\\nprint(retrieved_docs[0].page_content)\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nGo deeper\\u200b\\nVector stores are commonly used for retrieval, but there are other ways\\nto do retrieval, too.\\nRetriever: An object that returns Documents given a text query\\n\\nDocs: Further\\ndocumentation on the interface and built-in retrieval techniques.\\nSome of which include:\\n\\nMultiQueryRetriever generates variants of the input\\nquestion\\nto improve retrieval hit rate.\\nMultiVectorRetriever instead generates\\nvariants of the\\nembeddings,\\nalso in order to improve retrieval hit rate.\\nMax marginal relevance selects for relevance and\\ndiversity\\namong the retrieved documents to avoid passing in duplicate\\ncontext.\\nDocuments can be filtered during vector store retrieval using\\nmetadata filters, such as with a Self Query\\nRetriever.\\n\\n\\nIntegrations: Integrations\\nwith retrieval services.\\nInterface:\\nAPI reference for the base interface.\\nIntegrations: Integrations\\nwith retrieval services.\\nInterface:\\nAPI reference for the base interface.\\n\\n5. Retrieval and Generation: Generate\\u200b\\nLet‚Äôs put it all together into a chain that takes a question, retrieves\\nrelevant documents, constructs a prompt, passes that to a model, and\\nparses the output.\\nWe‚Äôll use the gpt-4o-mini OpenAI chat model, but any LangChain LLM\\nor ChatModel could be substituted in.\\nOpenAIAnthropicAzureGoogleCohereNVIDIAFireworksAIGroqMistralAITogetherAIpip install -qU langchain-openaiimport getpassimport osos.environ[\"OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(model=\"gpt-4o-mini\")pip install -qU langchain-anthropicimport getpassimport osos.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024)pip install -qU langchain-openaiimport getpassimport osos.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass()from langchain_openai import AzureChatOpenAIllm = AzureChatOpenAI(    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],)pip install -qU langchain-google-vertexaiimport getpassimport osos.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()from langchain_google_vertexai import ChatVertexAIllm = ChatVertexAI(model=\"gemini-1.5-flash\")pip install -qU langchain-cohereimport getpassimport osos.environ[\"COHERE_API_KEY\"] = getpass.getpass()from langchain_cohere import ChatCoherellm = ChatCohere(model=\"command-r-plus\")pip install -qU langchain-nvidia-ai-endpointsimport getpassimport osos.environ[\"NVIDIA_API_KEY\"] = getpass.getpass()from langchain import ChatNVIDIAllm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\")pip install -qU langchain-fireworksimport getpassimport osos.environ[\"FIREWORKS_API_KEY\"] = getpass.getpass()from langchain_fireworks import ChatFireworksllm = ChatFireworks(model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\")pip install -qU langchain-groqimport getpassimport osos.environ[\"GROQ_API_KEY\"] = getpass.getpass()from langchain_groq import ChatGroqllm = ChatGroq(model=\"llama3-8b-8192\")pip install -qU langchain-mistralaiimport getpassimport osos.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()from langchain_mistralai import ChatMistralAIllm = ChatMistralAI(model=\"mistral-large-latest\")pip install -qU langchain-openaiimport getpassimport osos.environ[\"TOGETHER_API_KEY\"] = getpass.getpass()from langchain_openai import ChatOpenAIllm = ChatOpenAI(    base_url=\"https://api.together.xyz/v1\",    api_key=os.environ[\"TOGETHER_API_KEY\"],    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",)\\nWe‚Äôll use a prompt for RAG that is checked into the LangChain prompt hub\\n(here).\\nfrom langchain import hubprompt = hub.pull(\"rlm/rag-prompt\")example_messages = prompt.invoke(    {\"context\": \"filler context\", \"question\": \"filler question\"}).to_messages()example_messages\\n[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.\\\\nQuestion: filler question \\\\nContext: filler context \\\\nAnswer:\")]\\nprint(example_messages[0].content)\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\\'t know the answer, just say that you don\\'t know. Use three sentences maximum and keep the answer concise.Question: filler question Context: filler context Answer:\\nWe‚Äôll use the LCEL Runnable\\nprotocol to define the chain, allowing us to\\n\\npipe together components and functions in a transparent way\\nautomatically trace our chain in LangSmith\\nget streaming, async, and batched calling out of the box.\\npipe together components and functions in a transparent way\\nautomatically trace our chain in LangSmith\\nget streaming, async, and batched calling out of the box.\\n\\nHere is the implementation:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughdef format_docs(docs):    return \"\\\\n\\\\n\".join(doc.page_content for doc in docs)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt    | llm    | StrOutputParser())for chunk in rag_chain.stream(\"What is Task Decomposition?\"):    print(chunk, end=\"\", flush=True)API Reference:StrOutputParser | RunnablePassthrough\\nTask Decomposition is a process where a complex task is broken down into smaller, more manageable steps or parts. This is often done using techniques like \"Chain of Thought\" or \"Tree of Thoughts\", which instruct a model to \"think step by step\" and transform large tasks into multiple simple tasks. Task decomposition can be prompted in a model, guided by task-specific instructions, or influenced by human inputs.\\nLet\\'s dissect the LCEL to understand what\\'s going on.\\nFirst: each of these components (retriever, prompt, llm, etc.) are instances of Runnable. This means that they implement the same methods-- such as sync and async .invoke, .stream, or .batch-- which makes them easier to connect together. They can be connected into a RunnableSequence-- another Runnable-- via the | operator.\\nLangChain will automatically cast certain objects to runnables when met with the | operator. Here, format_docs is cast to a RunnableLambda, and the dict with \"context\" and \"question\" is cast to a RunnableParallel. The details are less important than the bigger point, which is that each object is a Runnable.\\nLet\\'s trace how the input question flows through the above runnables.\\nAs we\\'ve seen above, the input to prompt is expected to be a dict with keys \"context\" and \"question\". So the first element of this chain builds runnables that will calculate both of these from the input question:\\n\\nretriever | format_docs passes the question through the retriever, generating Document objects, and then to format_docs to generate strings;\\nRunnablePassthrough() passes through the input question unchanged.\\n\\nThat is, if you constructed\\nchain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | prompt)\\nThen chain.invoke(question) would build a formatted prompt, ready for inference. (Note: when developing with LCEL, it can be practical to test with sub-chains like this.)\\nThe last steps of the chain are llm, which runs the inference, and StrOutputParser(), which just plucks the string content out of the LLM\\'s output message.\\nYou can analyze the individual steps of this chain via its LangSmith\\ntrace.\\nBuilt-in chains\\u200b\\nIf preferred, LangChain includes convenience functions that implement the above LCEL. We compose two functions:\\n\\ncreate_stuff_documents_chain specifies how retrieved context is fed into a prompt and LLM. In this case, we will \"stuff\" the contents into the prompt -- i.e., we will include all retrieved context without any summarization or other processing. It largely implements our above rag_chain, with input keys context and input-- it generates an answer using retrieved context and query.\\ncreate_retrieval_chain adds the retrieval step and propagates the retrieved context through the chain, providing it alongside the final answer. It has input key input, and includes input, context, and answer in its output.\\nfrom langchain.chains import create_retrieval_chainfrom langchain.chains.combine_documents import create_stuff_documents_chainfrom langchain_core.prompts import ChatPromptTemplatesystem_prompt = (    \"You are an assistant for question-answering tasks. \"    \"Use the following pieces of retrieved context to answer \"    \"the question. If you don\\'t know the answer, say that you \"    \"don\\'t know. Use three sentences maximum and keep the \"    \"answer concise.\"    \"\\\\n\\\\n\"    \"{context}\")prompt = ChatPromptTemplate.from_messages(    [        (\"system\", system_prompt),        (\"human\", \"{input}\"),    ])question_answer_chain = create_stuff_documents_chain(llm, prompt)rag_chain = create_retrieval_chain(retriever, question_answer_chain)response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})print(response[\"answer\"])API Reference:create_retrieval_chain | create_stuff_documents_chain | ChatPromptTemplate\\nTask Decomposition is a process in which complex tasks are broken down into smaller and simpler steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts are used to enhance model performance on these tasks. The CoT method instructs the model to think step by step, decomposing hard tasks into manageable ones, while Tree of Thoughts extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of thoughts.\\nReturning sources\\u200b\\nOften in Q&A applications it\\'s important to show users the sources that were used to generate the answer. LangChain\\'s built-in create_retrieval_chain will propagate retrieved source documents through to the output in the \"context\" key:\\nfor document in response[\"context\"]:    print(document)    print()\\npage_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Fig. 1. Overview of a LLM-powered autonomous agent system.\\\\nComponent One: Planning#\\\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\\\nTask Decomposition#\\\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to ‚Äúthink step by step‚Äù to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model‚Äôs thinking process.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 1585}page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 2192}page_content=\\'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}page_content=\\'Resources:\\\\n1. Internet access for searches and information gathering.\\\\n2. Long Term memory management.\\\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\\\n4. File output.\\\\n\\\\nPerformance Evaluation:\\\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past\\nContinuously review and analyze your actions to ensure you are performing to the best of your abilities.\\\\n2. Constructively self-criticize your big-picture behavior constantly.\\\\n3. Reflect on past decisions and strategies to refine your approach.\\\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\' metadata={\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'start_index\\': 29630}\\nGo deeper\\u200b\\nChoosing a model\\u200b\\nChatModel: An LLM-backed chat model. Takes in a sequence of messages\\nand returns a message.\\nDocs\\nIntegrations: 25+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nLLM: A text-in-text-out LLM. Takes in a string and returns a string.\\n\\nDocs\\nIntegrations: 75+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nSee a guide on RAG with locally-running models\\nhere.\\nCustomizing the prompt\\u200b\\nAs shown above, we can load prompts (e.g., this RAG\\nprompt) from the prompt\\nhub. The prompt can also be easily customized:\\nfrom langchain_core.prompts import PromptTemplatetemplate = \"\"\"Use the following pieces of context to answer the question at the end.If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.Use three sentences maximum and keep the answer as concise as possible.Always say \"thanks for asking!\" at the end of the answer.{context}Question: {question}Helpful Answer:\"\"\"custom_rag_prompt = PromptTemplate.from_template(template)rag_chain = (    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}    | custom_rag_prompt    | llm    | StrOutputParser())rag_chain.invoke(\"What is Task Decomposition?\")API Reference:PromptTemplate\\n\\'Task decomposition is the process of breaking down a complex task into smaller, more manageable parts. Techniques like Chain of Thought (CoT) and Tree of Thoughts allow an agent to \"think step by step\" and explore multiple reasoning possibilities, respectively. This process can be executed by a Language Model with simple prompts, task-specific instructions, or human inputs. Thanks for asking!\\'\\nCheck out the LangSmith\\ntrace\\nNext steps\\u200b\\nWe\\'ve covered the steps to build a basic Q&A app over data:\\n\\nLoading data with a Document Loader\\nChunking the indexed data with a Text Splitter to make it more easily usable by a model\\nEmbedding the data and storing the data in a vectorstore\\nRetrieving the previously stored chunks in response to incoming questions\\nGenerating an answer using the retrieved chunks as context\\n\\nThere‚Äôs plenty of features, integrations, and extensions to explore in each of\\nthe above sections. Along from the Go deeper sources mentioned\\nabove, good next steps include:\\n\\nReturn sources: Learn how to return source documents\\nStreaming: Learn how to stream outputs and intermediate steps\\nAdd chat history: Learn how to add chat history to your app\\nRetrieval conceptual guide: A high-level overview of specific retrieval techniques\\nBuild a local RAG application: Create an app similar to the one above using all local components\\nEdit this pageWas this page helpful?PreviousBuild a PDF ingestion and Question/Answering systemNextVector stores and retrieversWhat is RAG?ConceptsIndexingRetrieval and generationSetupJupyter NotebookInstallationLangSmithPreviewDetailed walkthrough1. Indexing: LoadGo deeper2. Indexing: SplitGo deeper3. Indexing: StoreGo deeper4. Retrieval and Generation: RetrieveGo deeper5. Retrieval and Generation: GenerateBuilt-in chainsGo deeperNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': 'LangChain is a popular library for building data pipelines and workflows in Python. To return structured output in LangChain, you can use the `Result` type from the `langchain chains` module.\\n\\nHere\\'s an example of how you can use `Result` to return structured output:\\n\\n```python\\nfrom langchain.chains import Result\\n\\nclass MyPipeline:\\n    def __init__(self):\\n        self.input_data = None\\n\\n    def run(self, input_data):\\n        # Process the input data here...\\n        processed_data = process_input_data(input_data)\\n\\n        # Return a Result with the structured output\\n        return Result(s success=process_data_successful(), output=processed_data)\\n\\ndef process_input_data(input_data):\\n    # Process the input data here...\\n    pass\\n\\ndef process_data_successful():\\n    # Return True if the processing was successful\\n    return True\\n```\\n\\nIn this example, `MyPipeline` is a class that takes in some input data and processes it using the `process_input_data` function. The `Result` type is used to wrap the result of the processing operation, with the following fields:\\n\\n*   `success`: A boolean indicating whether the operation was successful.\\n*   `output`: The actual output of the operation.\\n\\nWhen you return a `Result`, LangChain will automatically unbox it and return the `output` field if the `success` field is `True`.\\n\\nYou can also use the `chain_result` function from the `langchain.chains` module to create a Result with additional metadata:\\n\\n```python\\nfrom langchain.chains import chain_result\\n\\nclass MyPipeline:\\n    def __init__(self):\\n        self.input_data = None\\n\\n    def run(self, input_data):\\n        # Process the input data here...\\n        processed_data = process_input_data(input_data)\\n\\n        # Return a Result with structured output and additional metadata\\n        return chain_result(\\n            result=process_data_successful(),\\n            message=\"Data processing successful\",\\n            output=processed_data,\\n            code_status=\"success\"\\n        )\\n```\\n\\nIn this case, the `chain_result` function returns a Result with an additional `message`, `code_status` field, in addition to the `output` field.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": query})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a popular library for building data pipelines and workflows in Python. To return structured output in LangChain, you can use the `Result` type from the `langchain chains` module.\n",
       "\n",
       "Here's an example of how you can use `Result` to return structured output:\n",
       "\n",
       "```python\n",
       "from langchain.chains import Result\n",
       "\n",
       "class MyPipeline:\n",
       "    def __init__(self):\n",
       "        self.input_data = None\n",
       "\n",
       "    def run(self, input_data):\n",
       "        # Process the input data here...\n",
       "        processed_data = process_input_data(input_data)\n",
       "\n",
       "        # Return a Result with the structured output\n",
       "        return Result(s success=process_data_successful(), output=processed_data)\n",
       "\n",
       "def process_input_data(input_data):\n",
       "    # Process the input data here...\n",
       "    pass\n",
       "\n",
       "def process_data_successful():\n",
       "    # Return True if the processing was successful\n",
       "    return True\n",
       "```\n",
       "\n",
       "In this example, `MyPipeline` is a class that takes in some input data and processes it using the `process_input_data` function. The `Result` type is used to wrap the result of the processing operation, with the following fields:\n",
       "\n",
       "*   `success`: A boolean indicating whether the operation was successful.\n",
       "*   `output`: The actual output of the operation.\n",
       "\n",
       "When you return a `Result`, LangChain will automatically unbox it and return the `output` field if the `success` field is `True`.\n",
       "\n",
       "You can also use the `chain_result` function from the `langchain.chains` module to create a Result with additional metadata:\n",
       "\n",
       "```python\n",
       "from langchain.chains import chain_result\n",
       "\n",
       "class MyPipeline:\n",
       "    def __init__(self):\n",
       "        self.input_data = None\n",
       "\n",
       "    def run(self, input_data):\n",
       "        # Process the input data here...\n",
       "        processed_data = process_input_data(input_data)\n",
       "\n",
       "        # Return a Result with structured output and additional metadata\n",
       "        return chain_result(\n",
       "            result=process_data_successful(),\n",
       "            message=\"Data processing successful\",\n",
       "            output=processed_data,\n",
       "            code_status=\"success\"\n",
       "        )\n",
       "```\n",
       "\n",
       "In this case, the `chain_result` function returns a Result with an additional `message`, `code_status` field, in addition to the `output` field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_answer = results[\"answer\"]\n",
    "\n",
    "Markdown(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain Expression Language (LCEL) is an open-source, human-readable expression language designed specifically for use in LangChain, a blockchain-agnostic data management platform. LCEL allows developers to express complex data operations and transformations in a concise and readable manner.\n",
       "\n",
       "The main goals of LCEL are:\n",
       "\n",
       "1. **Simplify data operations**: By providing a standardized way to describe data processing tasks, LCEL makes it easier for developers to write efficient and maintainable code.\n",
       "2. **Improve readability**: LCEL's syntax is designed to be easy to understand, even for non-technical users, making it ideal for collaboration between developers and data analysts.\n",
       "3. **Enable data transformation**: LCEL allows developers to define data transformations as a series of steps, enabling the creation of complex workflows that can handle large datasets.\n",
       "\n",
       "Key features of LCEL include:\n",
       "\n",
       "1. **Declarative syntax**: LCEL uses a declarative syntax, where the focus is on what needs to be done, rather than how it should be done.\n",
       "2. **Built-in data types**: LCEL supports various built-in data types, such as arrays, objects, and sets, making it easy to work with different data structures.\n",
       "3. **Functions and operators**: LCEL provides a range of functions and operators for performing common data operations, such as filtering, mapping, and aggregating data.\n",
       "\n",
       "Some examples of LCEL expressions include:\n",
       "\n",
       "* `data | filter(x => x > 5)` (filter an array of numbers greater than 5)\n",
       "* `data | map(x => x * 2)` (multiply each number in the array by 2)\n",
       "* `data | reduce((a, b) => a + b, 0)` (calculate the sum of all numbers in the array)\n",
       "\n",
       "LCEL is designed to be used with LangChain's data management platform, which provides a flexible and scalable way to store, retrieve, and manipulate data. By combining LCEL with LangChain's features, developers can build robust data pipelines that automate complex workflows.\n",
       "\n",
       "Overall, LangChain Expression Language (LCEL) offers a powerful tool for expressing data operations in a concise, readable, and maintainable way, making it an attractive choice for developers working on blockchain-agnostic data management projects."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_summary = \"Write about what is LangChain Expression Language (LCEL)\"\n",
    "\n",
    " # adding chat history so the model remembers previous questions\n",
    "output = rag_chain.invoke({\"input\": query_summary})\n",
    "\n",
    "Markdown(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output is easily verifiable, we can see below that the chunk context for the answer came from pages 0,5,7 and 16 in the source pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}\n",
      "{'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}\n",
      "{'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}\n",
      "{'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(output['context'])):\n",
    "    print(output['context'][i].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now dig deeper into RAG over the langchain documentation and construct this rag chain ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', system_prompt),\n",
    "    ('human', '{input}')\n",
    "])\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        'input': lambda x: x['input'],\n",
    "        'context': lambda x: format_docs(x['context']), \n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing the input query to the retriever\n",
    "retrieve_docs = (lambda x: x['input']) | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  context: RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11b3f8850>, search_kwargs={})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: {\n",
       "              input: RunnableLambda(...),\n",
       "              context: RunnableLambda(...)\n",
       "            }\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOllama(model='llama3.2')\n",
       "            | StrOutputParser()\n",
       "  })"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'In LangChain how do we get structured outputs from a model?',\n",
       " 'context': [Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to run custom functionsOn this pageHow to run custom functions\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables\\nYou can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators\\nUsing the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load MarkdownOn this pageHow to load Markdown\\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:\\nBasic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores\\nIn this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings\\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load JSONOn this pageHow to load JSON\\nJSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:\\nHow to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.\\n#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path=\\'./example_data/facebook_chat.json\\'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {\\'image\\': {\\'creation_timestamp\\': 1675549016, \\'uri\\': \\'image_of_the_chat.jpg\\'},     \\'is_still_participant\\': True,     \\'joinable_mode\\': {\\'link\\': \\'\\', \\'mode\\': 1},     \\'magic_words\\': [],     \\'messages\\': [{\\'content\\': \\'Bye!\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675597571851},                  {\\'content\\': \\'Oh no worries! Bye\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675597435669},                  {\\'content\\': \\'No Im sorry it was my mistake, the blue one is not \\'                              \\'for sale\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675596277579},                  {\\'content\\': \\'I thought you were selling the blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595140251},                  {\\'content\\': \\'Im not interested in this bag. Im interested in the \\'                              \\'blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595109305},                  {\\'content\\': \\'Here is $129\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595068468},                  {\\'photos\\': [{\\'creation_timestamp\\': 1675595059,                               \\'uri\\': \\'url_of_some_picture.jpg\\'}],                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595060730},                  {\\'content\\': \\'Online is at least $100\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595045152},                  {\\'content\\': \\'How much do you want?\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675594799696},                  {\\'content\\': \\'Goodmorning! $50 is too low.\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675577876645},                  {\\'content\\': \\'Hi! Im interested in your bag. Im offering $50. Let \\'                              \\'me know if you are interested. Thanks!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675549022673}],     \\'participants\\': [{\\'name\\': \\'User 1\\'}, {\\'name\\': \\'User 2\\'}],     \\'thread_path\\': \\'inbox/User 1 and User 2 chat\\',     \\'title\\': \\'User 1 and User 2 chat\\'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': 'LangChain is a Python library that provides a simple and intuitive way to chain models together for various NLP tasks. To get structured outputs from a model using LangChain, you can use the `Chain` class or the `Pipeline` class.\\n\\nHere\\'s an example of how to use `Chain`:\\n\\n```python\\nfrom langchain.llms import HuggingFaceLLM\\n\\n# Initialize the LLM\\nllm = HuggingFaceLLM(\"distilbert-base-uncased\")\\n\\n# Define a function to process input text\\ndef process_input(text):\\n    return {\"text\": text}\\n\\n# Create a Chain object with the LLM and output processing function\\nchain = Chain(llm, process_input)\\n\\n# Use the Chain object to generate output\\noutput = chain({\"text\": \"Hello, how are you?\"})\\n\\nprint(output)\\n```\\n\\nIn this example, `Chain` takes two arguments: an LLM instance and a function that processes the input. The function is used to transform the output of the LLM into a structured format.\\n\\nAlternatively, you can use the `Pipeline` class, which provides more flexibility in terms of customization and parallel processing:\\n\\n```python\\nfrom langchain import Pipeline\\n\\n# Define a list of processors that will be applied in order\\nprocessors = [\\n    {\"name\": \"text-normalization\", \"kwargs\": {}},\\n    {\"name\": \"ner\", \"kwargs\": {}},\\n    {\"name\": \"sentiment-analysis\", \"kwargs\": {}},\\n]\\n\\n# Initialize the LLM\\nllm = HuggingFaceLLM(\"distilbert-base-uncased\")\\n\\n# Create a Pipeline object with the LLM and processors\\npipeline = Pipeline(llm, processors)\\n\\n# Use the Pipeline object to generate output\\noutput = pipeline({\"text\": \"Hello, how are you?\"})\\n\\nprint(output)\\n```\\n\\nIn this example, `Pipeline` takes two arguments: an LLM instance and a list of processors that will be applied in order. Each processor is a dictionary that defines the name of the processing function and any additional keyword arguments.\\n\\nBoth `Chain` and `Pipeline` provide a flexible way to chain models together for various NLP tasks, allowing you to customize the output format and processing pipeline to suit your specific needs.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"In LangChain how do we get structured outputs from a model?\" \n",
    "chain.invoke({'input': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding structured sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'In LangChain how do we get structured outputs from a model?',\n",
       " 'context': [Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 8, 'source': 'https://python.langchain.com/docs/how_to/functions/', 'title': 'How to run custom functions | ü¶úÔ∏èüîó LangChain'}, page_content='How to run custom functions | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to run custom functionsOn this pageHow to run custom functions\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nLangChain Expression Language (LCEL)\\nChaining runnables\\nYou can use arbitrary functions as Runnables. This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\\nNote that all inputs to these functions need to be a SINGLE argument. If you have a function that accepts multiple arguments, you should write a wrapper that accepts a single dict input and unpacks it into multiple arguments.\\nThis guide will cover:\\n\\nHow to explicitly create a runnable from a custom function using the RunnableLambda constructor and the convenience @chain decorator\\nCoercion of custom functions into runnables when used in chains\\nHow to accept and use run metadata in your custom function\\nHow to stream with custom functions by having them return generators\\nUsing the constructor\\u200b\\nBelow, we explicitly wrap our custom logic using the RunnableLambda constructor:\\n%pip install -qU langchain langchain_openaiimport osfrom getpass import getpassif \"OPENAI_API_KEY\" not in os.environ:    os.environ[\"OPENAI_API_KEY\"] = getpass()\\nfrom operator import itemgetterfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnableLambdafrom langchain_openai import ChatOpenAIdef length_function(text):    return len(text)def _multiple_length_function(text1, text2):    return len(text1) * len(text2)def multiple_length_function(_dict):    return _multiple_length_function(_dict[\"text1\"], _dict[\"text2\"])model = ChatOpenAI()prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")chain1 = prompt | modelchain = (    {        \"a\": itemgetter(\"foo\") | RunnableLambda(length_function),        \"b\": {\"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")}        | RunnableLambda(multiple_length_function),    }    | prompt    | model)chain.invoke({\"foo\": \"bar\", \"bar\": \"gah\"})API Reference:ChatPromptTemplate | RunnableLambda | ChatOpenAI\\nAIMessage(content=\\'3 + 9 equals 12.\\', response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 8, \\'prompt_tokens\\': 14, \\'total_tokens\\': 22}, \\'model_name\\': \\'gpt-3.5-turbo\\', \\'system_fingerprint\\': \\'fp_c2295e73ad\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-73728de3-e483-49e3-ad54-51bd9570e71a-0\\')\\nThe convenience @chain decorator\\u200b\\nYou can also turn an arbitrary function into a chain by adding a @chain decorator. This is functionaly equivalent to wrapping the function in a RunnableLambda constructor as shown above. Here\\'s an example:\\nfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import chainprompt1 = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt2 = ChatPromptTemplate.from_template(\"What is the subject of this joke: {joke}\")@chaindef custom_chain(text):    prompt_val1 = prompt1.invoke({\"topic\": text})    output1 = ChatOpenAI().invoke(prompt_val1)    parsed_output1 = StrOutputParser().invoke(output1)    chain2 = prompt2 | ChatOpenAI() | StrOutputParser()    return chain2.invoke({\"joke\": parsed_output1})custom_chain.invoke(\"bears\")API Reference:StrOutputParser | chain\\n\\'The subject of the joke is the bear and his girlfriend.\\'\\nAbove, the @chain decorator is used to convert custom_chain into a runnable, which we invoke with the .invoke() method.\\nIf you are using a tracing with LangSmith, you should see a custom_chain trace in there, with the calls to OpenAI nested underneath.\\nAutomatic coercion in chains\\u200b\\nWhen using custom functions in chains with the pipe operator (|), you can omit the RunnableLambda or @chain constructor and rely on coercion. Here\\'s a simple example with a function that takes the output from the model and returns the first five letters of it:\\nprompt = ChatPromptTemplate.from_template(\"tell me a story about {topic}\")model = ChatOpenAI()chain_with_coerced_function = prompt | model | (lambda x: x.content[:5])chain_with_coerced_function.invoke({\"topic\": \"bears\"})\\n\\'Once \\'\\nNote that we didn\\'t need to wrap the custom function (lambda x: x.content[:5]) in a RunnableLambda constructor because the model on the left of the pipe operator is already a Runnable. The custom function is coerced into a runnable. See this section for more information.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nPassing run metadata\\u200b\\nRunnable lambdas can optionally accept a RunnableConfig parameter, which they can use to pass callbacks, tags, and other configuration information to nested runs.\\nimport jsonfrom langchain_core.runnables import RunnableConfigdef parse_or_fix(text: str, config: RunnableConfig):    fixing_chain = (        ChatPromptTemplate.from_template(            \"Fix the following text:\\\\n\\\\n\\\\`\\\\`\\\\`text\\\\n{input}\\\\n\\\\`\\\\`\\\\`\\\\nError: {error}\"            \" Don\\'t narrate, just respond with the fixed data.\"        )        | model        | StrOutputParser()    )    for _ in range(3):        try:            return json.loads(text)        except Exception as e:            text = fixing_chain.invoke({\"input\": text, \"error\": e}, config)    return \"Failed to parse\"from langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:RunnableConfig | get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nfrom langchain_community.callbacks import get_openai_callbackwith get_openai_callback() as cb:    output = RunnableLambda(parse_or_fix).invoke(        \"{foo: bar}\", {\"tags\": [\"my-tag\"], \"callbacks\": [cb]}    )    print(output)    print(cb)API Reference:get_openai_callback\\n{\\'foo\\': \\'bar\\'}Tokens Used: 62\\tPrompt Tokens: 56\\tCompletion Tokens: 6Successful Requests: 1Total Cost (USD): $9.6e-05\\nStreaming\\u200b\\nnoteRunnableLambda is best suited for code that does not need to support streaming. If you need to support streaming (i.e., be able to operate on chunks of inputs and yield chunks of outputs), use RunnableGenerator instead as in the example below.\\nYou can use generator functions (ie. functions that use the yield keyword, and behave like iterators) in a chain.\\nThe signature of these generators should be Iterator[Input] -> Iterator[Output]. Or for async generators: AsyncIterator[Input] -> AsyncIterator[Output].\\nThese are useful for:\\nimplementing a custom output parser\\nmodifying the output of a previous step, while preserving streaming capabilities\\n\\nHere\\'s an example of a custom output parser for comma-separated lists. First, we create a chain that generates such a list as text:\\nfrom typing import Iterator, Listprompt = ChatPromptTemplate.from_template(    \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\")str_chain = prompt | model | StrOutputParser()for chunk in str_chain.stream({\"animal\": \"bear\"}):    print(chunk, end=\"\", flush=True)\\nlion, tiger, wolf, gorilla, panda\\nNext, we define a custom function that will aggregate the currently streamed output and yield it when the model generates the next comma in the list:\\n# This is a custom parser that splits an iterator of llm tokens# into a list of strings separated by commasdef split_into_list(input: Iterator[str]) -> Iterator[List[str]]:    # hold partial input until we get a comma    buffer = \"\"    for chunk in input:        # add current chunk to buffer        buffer += chunk        # while there are commas in the buffer        while \",\" in buffer:            # split buffer on comma            comma_index = buffer.index(\",\")            # yield everything before the comma            yield [buffer[:comma_index].strip()]            # save the rest for the next iteration            buffer = buffer[comma_index + 1 :]    # yield the last chunk    yield [buffer.strip()]list_chain = str_chain | split_into_listfor chunk in list_chain.stream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'raccoon\\']\\nInvoking it gives a full array of values:\\nlist_chain.invoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'raccoon\\']\\nAsync version\\u200b\\nIf you are working in an async environment, here is an async version of the above example:\\nfrom typing import AsyncIteratorasync def asplit_into_list(    input: AsyncIterator[str],) -> AsyncIterator[List[str]]:  # async def    buffer = \"\"    async for (        chunk    ) in input:  # `input` is a `async_generator` object, so use `async for`        buffer += chunk        while \",\" in buffer:            comma_index = buffer.index(\",\")            yield [buffer[:comma_index].strip()]            buffer = buffer[comma_index + 1 :]    yield [buffer.strip()]list_chain = str_chain | asplit_into_listasync for chunk in list_chain.astream({\"animal\": \"bear\"}):    print(chunk, flush=True)\\n[\\'lion\\'][\\'tiger\\'][\\'wolf\\'][\\'gorilla\\'][\\'panda\\']\\nawait list_chain.ainvoke({\"animal\": \"bear\"})\\n[\\'lion\\', \\'tiger\\', \\'wolf\\', \\'gorilla\\', \\'panda\\']\\nNext steps\\u200b\\nNow you\\'ve learned a few different ways to use custom logic within your chains, and how to implement streaming.\\nTo learn more, see the other how-to guides on runnables in this section.Edit this pageWas this page helpful?PreviousHow to use few shot examplesNextHow to use output parsers to parse an LLM response into structured formatUsing the constructorThe convenience @chain decoratorAutomatic coercion in chainsPassing run metadataStreamingAsync versionNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'Markdown is a lightweight markup language for creating formatted text using a plain-text editor.', 'language': 'en', 'num_chunks': 5, 'source': 'https://python.langchain.com/docs/how_to/document_loader_markdown/', 'title': 'How to load Markdown | ü¶úÔ∏èüîó LangChain'}, page_content='How to load Markdown | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load MarkdownOn this pageHow to load Markdown\\nMarkdown is a lightweight markup language for creating formatted text using a plain-text editor.\\nHere we cover how to load Markdown documents into LangChain Document objects that we can use downstream.\\nWe will cover:\\nBasic usage;\\nParsing of Markdown into elements such as titles, list items, and text.\\n\\nLangChain implements an UnstructuredMarkdownLoader object which requires the Unstructured package. First we install it:\\n%pip install \"unstructured[md]\" nltk\\nBasic usage will ingest a Markdown file to a single document. Here we demonstrate on LangChain\\'s readme:\\nfrom langchain_community.document_loaders import UnstructuredMarkdownLoaderfrom langchain_core.documents import Documentmarkdown_path = \"../../../README.md\"loader = UnstructuredMarkdownLoader(markdown_path)data = loader.load()assert len(data) == 1assert isinstance(data[0], Document)readme_content = data[0].page_contentprint(readme_content[:250])API Reference:UnstructuredMarkdownLoader | Document\\nü¶úÔ∏èüîó LangChain‚ö° Build context-aware reasoning applications ‚ö°Looking for the JS/TS library? Check out LangChain.js.To help you ship LangChain apps to production faster, check out LangSmith. LangSmith is a unified developer platform for building,\\nRetain Elements\\u200b\\nUnder the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\".\\nloader = UnstructuredMarkdownLoader(markdown_path, mode=\"elements\")data = loader.load()print(f\"Number of documents: {len(data)}\\\\n\")for document in data[:2]:    print(f\"{document}\\\\n\")\\nNumber of documents: 66page_content=\\'ü¶úÔ∏èüîó LangChain\\' metadata={\\'source\\': \\'../../../README.md\\', \\'category_depth\\': 0, \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'Title\\'}page_content=\\'‚ö° Build context-aware reasoning applications ‚ö°\\' metadata={\\'source\\': \\'../../../README.md\\', \\'last_modified\\': \\'2024-06-28T15:20:01\\', \\'languages\\': [\\'eng\\'], \\'parent_id\\': \\'200b8a7d0dd03f66e4f13456566d2b3a\\', \\'filetype\\': \\'text/markdown\\', \\'file_directory\\': \\'../../..\\', \\'filename\\': \\'README.md\\', \\'category\\': \\'NarrativeText\\'}\\nNote that in this case we recover three distinct element types:\\nprint(set(document.metadata[\"category\"] for document in data))\\n{\\'ListItem\\', \\'NarrativeText\\', \\'Title\\'}Edit this pageWas this page helpful?PreviousHow to load JSONNextHow to load Microsoft Office filesRetain ElementsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'This guide assumes familiarity with the following concepts:', 'language': 'en', 'num_chunks': 7, 'source': 'https://python.langchain.com/docs/how_to/few_shot_examples/', 'title': 'How to use few shot examples | ü¶úÔ∏èüîó LangChain'}, page_content='How to use few shot examples | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to use few shot examplesOn this pageHow to use few shot examples\\nPrerequisitesThis guide assumes familiarity with the following concepts:\\nPrompt templates\\nExample selectors\\nLLMs\\nVectorstores\\nIn this guide, we\\'ll learn how to create a simple prompt template that provides the model with example inputs and outputs when generating. Providing the LLM with a few such examples is called few-shotting, and is a simple yet powerful way to guide generation and in some cases drastically improve model performance.\\nA few-shot prompt template can be constructed from either a set of examples, or from an Example Selector class responsible for choosing a subset of examples from the defined set.\\nThis guide will cover few-shotting with string prompt templates. For a guide on few-shotting with chat messages for chat models, see here.\\nCreate a formatter for the few-shot examples\\u200b\\nConfigure a formatter that will format the few-shot examples into a string. This formatter should be a PromptTemplate object.\\nfrom langchain_core.prompts import PromptTemplateexample_prompt = PromptTemplate.from_template(\"Question: {question}\\\\n{answer}\")API Reference:PromptTemplate\\nCreating the example set\\u200b\\nNext, we\\'ll create a list of few-shot examples. Each example should be a dictionary representing an example input to the formatter prompt we defined above.\\nexamples = [    {        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\"\"\",    },    {        \"question\": \"When was the founder of craigslist born?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952\"\"\",    },    {        \"question\": \"Who was the maternal grandfather of George Washington?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ball\"\"\",    },    {        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",        \"answer\": \"\"\"Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: No\"\"\",    },]\\nLet\\'s test the formatting prompt with one of our examples:\\nprint(example_prompt.invoke(examples[0]).to_string())\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad Ali\\nPass the examples and formatter to FewShotPromptTemplate\\u200b\\nFinally, create a FewShotPromptTemplate object. This object takes in the few-shot examples and the formatter for the few-shot examples. When this FewShotPromptTemplate is formatted, it formats the passed examples using the example_prompt, then and adds them to the final prompt before suffix:\\nfrom langchain_core.prompts import FewShotPromptTemplateprompt = FewShotPromptTemplate(    examples=examples,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())API Reference:FewShotPromptTemplate\\nQuestion: Who lived longer, Muhammad Ali or Alan Turing?Are follow up questions needed here: Yes.Follow up: How old was Muhammad Ali when he died?Intermediate answer: Muhammad Ali was 74 years old when he died.Follow up: How old was Alan Turing when he died?Intermediate answer: Alan Turing was 41 years old when he died.So the final answer is: Muhammad AliQuestion: When was the founder of craigslist born?Are follow up questions needed here: Yes.Follow up: Who was the founder of craigslist?Intermediate answer: Craigslist was founded by Craig Newmark.Follow up: When was Craig Newmark born?Intermediate answer: Craig Newmark was born on December 6, 1952.So the final answer is: December 6, 1952Question: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Are both the directors of Jaws and Casino Royale from the same country?Are follow up questions needed here: Yes.Follow up: Who is the director of Jaws?Intermediate Answer: The director of Jaws is Steven Spielberg.Follow up: Where is Steven Spielberg from?Intermediate Answer: The United States.Follow up: Who is the director of Casino Royale?Intermediate Answer: The director of Casino Royale is Martin Campbell.Follow up: Where is Martin Campbell from?Intermediate Answer: New Zealand.So the final answer is: NoQuestion: Who was the father of Mary Ball Washington?\\nBy providing the model with examples like this, we can guide the model to a better response.\\nUsing an example selector\\u200b\\nWe will reuse the example set and the formatter from the previous section. However, instead of feeding the examples directly into the FewShotPromptTemplate object, we will feed them into an implementation of ExampleSelector called SemanticSimilarityExampleSelector instance. This class selects few-shot examples from the initial set based on their similarity to the input. It uses an embedding model to compute the similarity between the input and the few-shot examples, as well as a vector store to perform the nearest neighbor search.\\nTo show what it looks like, let\\'s initialize an instance and call it in isolation:\\nfrom langchain_chroma import Chromafrom langchain_core.example_selectors import SemanticSimilarityExampleSelectorfrom langchain_openai import OpenAIEmbeddingsexample_selector = SemanticSimilarityExampleSelector.from_examples(    # This is the list of examples available to select from.    examples,    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.    OpenAIEmbeddings(),    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.    Chroma,    # This is the number of examples to produce.    k=1,)# Select the most similar example to the input.question = \"Who was the father of Mary Ball Washington?\"selected_examples = example_selector.select_examples({\"question\": question})print(f\"Examples most similar to the input: {question}\")for example in selected_examples:    print(\"\\\\n\")    for k, v in example.items():        print(f\"{k}: {v}\")API Reference:SemanticSimilarityExampleSelector | OpenAIEmbeddings\\nExamples most similar to the input: Who was the father of Mary Ball Washington?answer: Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph Ballquestion: Who was the maternal grandfather of George Washington?\\nNow, let\\'s create a FewShotPromptTemplate object. This object takes in the example selector and the formatter prompt for the few-shot examples.\\nprompt = FewShotPromptTemplate(    example_selector=example_selector,    example_prompt=example_prompt,    suffix=\"Question: {input}\",    input_variables=[\"input\"],)print(    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string())\\nQuestion: Who was the maternal grandfather of George Washington?Are follow up questions needed here: Yes.Follow up: Who was the mother of George Washington?Intermediate answer: The mother of George Washington was Mary Ball Washington.Follow up: Who was the father of Mary Ball Washington?Intermediate answer: The father of Mary Ball Washington was Joseph Ball.So the final answer is: Joseph BallQuestion: Who was the father of Mary Ball Washington?\\nNext steps\\u200b\\nYou\\'ve now learned how to add few-shot examples to your prompts.\\nNext, check out the other how-to guides on prompt templates in this section, the related how-to guide on few shotting with chat models, or the other example selector how-to guides.Edit this pageWas this page helpful?PreviousHow to add examples to the prompt for query analysisNextHow to run custom functionsCreate a formatter for the few-shot examplesCreating the example setPass the examples and formatter to FewShotPromptTemplateUsing an example selectorNext stepsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  Document(metadata={'description': 'JSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).', 'language': 'en', 'num_chunks': 11, 'source': 'https://python.langchain.com/docs/how_to/document_loader_json/', 'title': 'How to load JSON | ü¶úÔ∏èüîó LangChain'}, page_content='How to load JSON | ü¶úÔ∏èüîó LangChain\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\"\\nLanguage CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityHow-to guidesHow to load JSONOn this pageHow to load JSON\\nJSON (JavaScript Object Notation) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attribute‚Äìvalue pairs and arrays (or other serializable values).\\nJSON Lines is a file format where each line is a valid JSON value.\\nLangChain implements a JSONLoader\\nto convert JSON and JSONL data into LangChain Document\\nobjects. It uses a specified jq schema to parse the JSON files, allowing for the extraction of specific fields into the content\\nand metadata of the LangChain Document.\\nIt uses the jq python package. Check out this manual for a detailed documentation of the jq syntax.\\nHere we will demonstrate:\\nHow to load JSON and JSONL data into the content of a LangChain Document;\\nHow to load JSON and JSONL data into metadata associated with a Document.\\n#!pip install jq\\nfrom langchain_community.document_loaders import JSONLoaderAPI Reference:JSONLoader\\nimport jsonfrom pathlib import Pathfrom pprint import pprintfile_path=\\'./example_data/facebook_chat.json\\'data = json.loads(Path(file_path).read_text())\\npprint(data)\\n    {\\'image\\': {\\'creation_timestamp\\': 1675549016, \\'uri\\': \\'image_of_the_chat.jpg\\'},     \\'is_still_participant\\': True,     \\'joinable_mode\\': {\\'link\\': \\'\\', \\'mode\\': 1},     \\'magic_words\\': [],     \\'messages\\': [{\\'content\\': \\'Bye!\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675597571851},                  {\\'content\\': \\'Oh no worries! Bye\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675597435669},                  {\\'content\\': \\'No Im sorry it was my mistake, the blue one is not \\'                              \\'for sale\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675596277579},                  {\\'content\\': \\'I thought you were selling the blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595140251},                  {\\'content\\': \\'Im not interested in this bag. Im interested in the \\'                              \\'blue one!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675595109305},                  {\\'content\\': \\'Here is $129\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595068468},                  {\\'photos\\': [{\\'creation_timestamp\\': 1675595059,                               \\'uri\\': \\'url_of_some_picture.jpg\\'}],                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595060730},                  {\\'content\\': \\'Online is at least $100\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675595045152},                  {\\'content\\': \\'How much do you want?\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675594799696},                  {\\'content\\': \\'Goodmorning! $50 is too low.\\',                   \\'sender_name\\': \\'User 2\\',                   \\'timestamp_ms\\': 1675577876645},                  {\\'content\\': \\'Hi! Im interested in your bag. Im offering $50. Let \\'                              \\'me know if you are interested. Thanks!\\',                   \\'sender_name\\': \\'User 1\\',                   \\'timestamp_ms\\': 1675549022673}],     \\'participants\\': [{\\'name\\': \\'User 1\\'}, {\\'name\\': \\'User 2\\'}],     \\'thread_path\\': \\'inbox/User 1 and User 2 chat\\',     \\'title\\': \\'User 1 and User 2 chat\\'}\\nUsing JSONLoader\\u200b\\nSuppose we are interested in extracting the values under the content field within the messages key of the JSON data. This can easily be done through the JSONLoader as shown below.\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\nJSON file\\u200b\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[].content\\',    text_content=False)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11})]\\nJSON Lines file\\u200b\\nIf you want to load documents from a JSON Lines file, you pass json_lines=True\\nand specify jq_schema to extract page_content from a single JSON object.\\nfile_path = \\'./example_data/facebook_chat_messages.jsonl\\'pprint(Path(file_path).read_text())\\n    (\\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675597571851, \"content\": \"Bye!\"}\\\\n\\'     \\'{\"sender_name\": \"User 1\", \"timestamp_ms\": 1675597435669, \"content\": \"Oh no \\'     \\'worries! Bye\"}\\\\n\\'     \\'{\"sender_name\": \"User 2\", \"timestamp_ms\": 1675596277579, \"content\": \"No Im \\'     \\'sorry it was my mistake, the blue one is not for sale\"}\\\\n\\')\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.content\\',    text_content=False,    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nAnother option is to set jq_schema=\\'.\\' and provide content_key:\\nloader = JSONLoader(    file_path=\\'./example_data/facebook_chat_messages.jsonl\\',    jq_schema=\\'.\\',    content_key=\\'sender_name\\',    json_lines=True)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 1}),     Document(page_content=\\'User 1\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 2}),     Document(page_content=\\'User 2\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat_messages.jsonl\\', \\'seq_num\\': 3})]\\nJSON file with jq schema content_key\\u200b\\nTo load documents from a JSON file using the content_key within the jq schema, set is_content_key_jq_parsable=True.\\nEnsure that content_key is compatible and can be parsed using the jq schema.\\nfile_path = \\'./sample.json\\'pprint(Path(file_path).read_text())\\n    {\"data\": [        {\"attributes\": {            \"message\": \"message1\",            \"tags\": [            \"tag1\"]},        \"id\": \"1\"},        {\"attributes\": {            \"message\": \"message2\",            \"tags\": [            \"tag2\"]},        \"id\": \"2\"}]}\\nloader = JSONLoader(    file_path=file_path,    jq_schema=\".data[]\",    content_key=\".attributes.message\",    is_content_key_jq_parsable=True,)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'message1\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 1}),     Document(page_content=\\'message2\\', metadata={\\'source\\': \\'/path/to/sample.json\\', \\'seq_num\\': 2})]\\nExtracting metadata\\u200b\\nGenerally, we want to include metadata available in the JSON file into the documents that we create from the content.\\nThe following demonstrates how metadata can be extracted using the JSONLoader.\\nThere are some key changes to be noted. In the previous example where we didn\\'t collect the metadata, we managed to directly specify in the schema where the value for the page_content can be extracted from.\\n.messages[].content\\nIn the current example, we have to tell the loader to iterate over the records in the messages field. The jq_schema then has to be:\\n.messages[]\\nThis allows us to pass the records (dict) into the metadata_func that has to be implemented. The metadata_func is responsible for identifying which pieces of information in the record should be included in the metadata stored in the final Document object.\\nAdditionally, we now have to explicitly specify in the loader, via the content_key argument, the key from the record where the value for the page_content needs to be extracted from.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'/Users/avsolatorio/WBG/langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nNow, you will see that the documents contain the metadata associated with the content we extracted.\\nThe metadata_func\\u200b\\nAs shown above, the metadata_func accepts the default metadata generated by the JSONLoader. This allows full control to the user with respect to how the metadata is formatted.\\nFor example, the default metadata contains the source and the seq_num keys. However, it is possible that the JSON data contain these keys as well. The user can then exploit the metadata_func to rename the default keys and use the ones from the JSON data.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\nThe example below shows how we can modify the source to only contain information of the file source relative to the langchain directory.\\n# Define the metadata extraction function.def metadata_func(record: dict, metadata: dict) -> dict:    metadata[\"sender_name\"] = record.get(\"sender_name\")    metadata[\"timestamp_ms\"] = record.get(\"timestamp_ms\")    if \"source\" in metadata:        source = metadata[\"source\"].split(\"/\")        source = source[source.index(\"langchain\"):]        metadata[\"source\"] = \"/\".join(source)    return metadataloader = JSONLoader(    file_path=\\'./example_data/facebook_chat.json\\',    jq_schema=\\'.messages[]\\',    content_key=\"content\",    metadata_func=metadata_func)data = loader.load()\\npprint(data)\\n    [Document(page_content=\\'Bye!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 1, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675597571851}),     Document(page_content=\\'Oh no worries! Bye\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 2, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675597435669}),     Document(page_content=\\'No Im sorry it was my mistake, the blue one is not for sale\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 3, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675596277579}),     Document(page_content=\\'I thought you were selling the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 4, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595140251}),     Document(page_content=\\'Im not interested in this bag. Im interested in the blue one!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 5, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675595109305}),     Document(page_content=\\'Here is $129\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 6, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595068468}),     Document(page_content=\\'\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 7, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595060730}),     Document(page_content=\\'Online is at least $100\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 8, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675595045152}),     Document(page_content=\\'How much do you want?\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 9, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675594799696}),     Document(page_content=\\'Goodmorning! $50 is too low.\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 10, \\'sender_name\\': \\'User 2\\', \\'timestamp_ms\\': 1675577876645}),     Document(page_content=\\'Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!\\', metadata={\\'source\\': \\'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_chat.json\\', \\'seq_num\\': 11, \\'sender_name\\': \\'User 1\\', \\'timestamp_ms\\': 1675549022673})]\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nCommon JSON structures with jq schema\\u200b\\nThe list below provides a reference to the possible jq_schema the user can use to extract content from the JSON data depending on the structure.\\nJSON        -> [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]jq_schema   -> \".[].text\"JSON        -> {\"key\": [{\"text\": ...}, {\"text\": ...}, {\"text\": ...}]}jq_schema   -> \".key[].text\"JSON        -> [\"...\", \"...\", \"...\"]jq_schema   -> \".[]\"Edit this pageWas this page helpful?PreviousHow to load HTMLNextHow to load MarkdownUsing JSONLoaderJSON fileJSON Lines fileJSON file with jq schema content_keyExtracting metadataThe metadata_funcCommon JSON structures with jq schemaCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')],\n",
       " 'answer': {'answer': 'Structured outputs from a model can be obtained in LangChain by using the <|model_output_tokenizer|> tokenizer to split the output into constituent parts.',\n",
       "  'sources': '[\"LangChain documentation\", \"Stack Overflow discussion\"]'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://python.langchain.com/v0.3/docs/how_to/qa_sources/\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "# Desired schema for response\n",
    "class AnswerWithSources(TypedDict):\n",
    "    \"\"\"An answer to the question, with sources.\"\"\"\n",
    "\n",
    "    answer: str\n",
    "    sources: Annotated[\n",
    "        List[str],\n",
    "        ...,\n",
    "        \"List of sources (author + year) used to answer the question\",\n",
    "    ]\n",
    "\n",
    "\n",
    "# Our rag_chain_from_docs has the following changes:\n",
    "# - add `.with_structured_output` to the LLM;\n",
    "# - remove the output parser\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: format_docs(x[\"context\"]),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm.with_structured_output(AnswerWithSources)\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "\n",
    "chain = RunnablePassthrough.assign(context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"input\": query})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb \n",
    "Below are notebook from openai cookbook on these topics of search and embeddings:\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Code_search.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Customizing_embeddings.ipynb\n",
    "- https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "- [In-context learning abilities of ChatGPT models](https://arxiv.org/pdf/2303.18223.pdf)\n",
    "- [Issue with long context](https://arxiv.org/pdf/2303.18223.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
