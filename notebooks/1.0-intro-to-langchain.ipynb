{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'oreilly-langchain' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n oreilly-langchain ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain \n",
    "\n",
    "Working with LLMs involves in one way or another working with a specific type of abstraction: \"Prompts\".\n",
    "\n",
    "However, in the practical context of day-to-day tasks we expect LLMs to perform, these prompts won't be some static and dead type of abstraction. Instead we'll work with dynamic prompts re-usable prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework that allows you to connect LLMs together by allowing you to work with modular components like prompt templates and chains giving you immense flexibility in creating tailored solutions powered by the capabilities of large language models.\n",
    "\n",
    "\n",
    "Its main features are:\n",
    "- **Components**: abstractions for working with LMs\n",
    "- **Off-the-shelf chains**: assembly of components for accomplishing certain higher-level tasks\n",
    "\n",
    "LangChain facilitates the creation of complex pipelines that leverage the connection of components like chains, prompt templates, output parsers and others to compose intricate pipelines that give you everything you need to solve a wide variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of LangChain, we have the following elements:\n",
    "\n",
    "- Models\n",
    "- Prompts\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "Models are nothing more than abstractions over the LLM APIs like the ChatGPT API.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"]=\"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\"\n",
    "chat_model = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict outputs from both LLMs and ChatModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "chat_model.predict(\"hi!\")\n",
    "# Output: \"Hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic components are:\n",
    "\n",
    "- Models\n",
    "- Prompt tempaltes\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Show me 5 examples of this concept: {concept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['concept'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['concept'], template='Show me 5 examples of this concept: {concept}'))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Show me 5 examples of this concept: animal'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(concept=\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. A lion is a powerful animal known for its strength and hunting abilities.\\n2. A giraffe is a tall animal with a long neck and spotted coat.\\n3. A dolphin is an intelligent and playful marine animal known for its acrobatic abilities.\\n4. A koala is a cute and cuddly animal native to Australia known for its love of eucalyptus leaves.\\n5. A bald eagle is a majestic bird of prey and a symbol of the United States.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat_model\n",
    "\n",
    "output = chain.invoke({\"concept\": \"animal\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. A lion is a powerful animal known for its strength and hunting abilities.\\n2. A giraffe is a tall animal with a long neck and spotted coat.\\n3. A dolphin is an intelligent and playful marine animal known for its acrobatic abilities.\\n4. A koala is a cute and cuddly animal native to Australia known for its love of eucalyptus leaves.\\n5. A bald eagle is a majestic bird of prey and a symbol of the United States.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That sounds interesting! LLMs, or Language Model Models, are a fascinating topic. They are a type of machine learning model that can generate human-like text based on the input it receives. It's a rapidly evolving field with a lot of potential applications. What specific aspects of LLMs will you be covering in your training?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict(\"I am teaching a live-training about LLMs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the predict method over a string input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Snooze or Dozer'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"What would be a good name for a dog that loves to nap??\"\n",
    "\n",
    "\n",
    "chat_model.predict(text)\n",
    "# Output: \"Snuggles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the `predict_messages` method over a list of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Snooze, Dozer, Napper, Lazy, Dreamer, Slumber')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good dog name for a dog that loves to nap?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point let's stop and take a look at what this code would look like if we were using the openai api directly instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand what is going on.\n",
    "\n",
    "Instead of writing down the human message dictionary for the openai API as you would do normally using the the original API, langchain is giving you an abstraction over that message through the class\n",
    "`HumanMessage()`, as well as an abstraction over the loop for multiple predictions through the .`predict_messages()` method.\n",
    "\n",
    "Now, why is that an useful thing?\n",
    "\n",
    "Because it allows you to work at a higher level of experimentation and orchestration with the blocks of that make up a workflow using LLMs.\n",
    "\n",
    "By making it easier to create predictions of multiple messages for example, you can experiment with different human message prompts faster and therefore get to better and more efficient results faster without having to write a lot of boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**\n",
    "\n",
    "The same works for prompts. Now, prompts are pieces of text we feed to LLMs, and LangChain allows you to work with prompt templates.\n",
    "\n",
    "Prompt Templates are useful abstractions for reusing prompts and they are used to provide context for the specific task that the language model needs to complete. \n",
    "\n",
    "A simple example is a `PromptTemplate` that formats a string into a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good dog name for a dog that loves to sleeping?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "prompt.format(activity=\"sleeping\")\n",
    "# Output: \"What is a good dog name for a dog that loves to nap?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parsers**\n",
    "\n",
    "OutputParsers convert the raw output from an LLM into a format that can be used downstream. Here is an example of an OutputParser that converts a comma-separated list into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
    "# Output: ['hi', 'bye']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are the basics of langchain. But how can we leverage these abstraction capabilities inside our LLM app application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to put everything together LangChain allows you to build something called \"chains\", which are components that connect prompts, llms and output parsers into a building block that allows you to create more interesting and complex functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are a few suggestions for a dog that loves to sleep:\\n\\n1. Snooze\\n2. Dozer\\n3. Slumber\\n4. Napper\\n5. Zzz\\n6. Dreamer\\n7. Paws\\n8. Snuggles\\n9. Lazy\\n10. Snore'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=prompt,\n",
    ")\n",
    "chain.run(\"sleep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what the chain is doing is connecting these basic components (the LLM and the prompt template) into\n",
    "a block that can be run separately. The chain allows you to turn workflows using LLLMs into this modular process of composing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the newer versions of LangChain have a new representation language to create these chains (and more) known as LCEL or LangChain expression language, which is a declarative way to easily compose chains together. The same example as above expressed in this LCEL format would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A good dog name for a dog that loves to sleep could be \"Snuggles\" or \"Snooze.\"')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "chain.invoke({\"activity\": \"sleep\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the output is an `AIMessage()` object, which represents LangChain's way to abstract the output from an LLM model like ChatGPT or others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These building blocks and abstractions that LangChain provides are what makes this library so unique, because it gives you the tools you didn't know you need it to build awesome stuff powered by LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Lab Exercises\n",
    "\n",
    "Let's take a look at a few examples using some of the capabilities of the LangChain library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a prompt to summarize any piece of text into some desirable format. \n",
    "\n",
    "Let's make use of the `PromptTemplate` to abstract away the following pieces of the prompt: \n",
    "- `content` - the text content to be summarized  \n",
    "- `summary_format` - the format in which we want the summary to be presented (like bullet points and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize this: This is a test.. The output should be in the following format: One word summary.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Summarize this: {content}. The output should be in the following format: {summary_format}.\")\n",
    "\n",
    "# We can look at a simple example to illustrate what that prompt is doing\n",
    "prompt.format(content=\"This is a test.\", summary_format=\"One word summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our prompt template done, let's load the llm and create a nice chain to put everything together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm_chat =  ChatOpenAI()\n",
    "chain = prompt | llm_chat # This is the Pipe symbol! from LCEL that connect model to prompt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have our chain we can run some tests. The cool thing about working with LLMs is that you can use them to create examples for simple tests like this (avoiding the annoynace of searching online for some piece of text, copying and pasting etc...). So, let's generate a few examples of tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Human-machine interaction, also known as HMI, has witnessed significant advancements in recent years, revolutionizing the way humans interact with technology. One intriguing aspect of this field is the development of emotionally intelligent machines. Emotionally intelligent machines are designed to recognize and respond to human emotions, making interactions more natural and engaging. These machines utilize various technologies, including facial recognition, speech analysis, and machine learning algorithms, to understand and interpret human emotions accurately.\\n\\nThe potential applications for emotionally intelligent machines are vast. In healthcare, these machines can be used to monitor patients' emotional well-being, providing valuable insights for medical professionals. For instance, a machine that recognizes signs of stress or depression in a patient can alert healthcare providers, enabling them to intervene promptly. Additionally, emotionally intelligent machines can be employed in customer service settings, enhancing the overall experience. By analyzing customer emotions, these machines can adapt their responses accordingly, offering personalized and empathetic support. This not only improves customer satisfaction but also strengthens brand loyalty and trust.\\n\\nDespite the tremendous benefits, there are ethical considerations surrounding emotionally intelligent machines. Privacy concerns arise when machines analyze and interpret our emotions, as they may gain access to sensitive information. Striking a balance between the benefits and potential risks associated with emotionally intelligent machines is crucial. As the field continues to evolve, it is essential to establish robust regulations and guidelines to ensure the responsible and ethical use of this technology. Nevertheless, emotionally intelligent machines have the potential to transform our interactions with technology, making them more intuitive and human-like, ultimately enhancing our everyday lives.\",\n",
       " 'One fascinating aspect of human-machine interaction is the development of virtual assistants. These intelligent systems are designed to understand and respond to human commands and queries, effectively bridging the gap between humans and machines. Virtual assistants, such as Siri, Alexa, or Google Assistant, have become ubiquitous in our daily lives, transforming the way we interact with technology.\\n\\nVirtual assistants rely on advanced natural language processing algorithms and machine learning models to understand human language and provide relevant responses. They can perform a wide range of tasks, including setting reminders, answering questions, playing music, and even controlling smart home devices. The convenience and efficiency offered by virtual assistants have made them increasingly popular, with people relying on them for various daily activities.\\n\\nHowever, the development of virtual assistants raises important questions about privacy and ethics. As these systems constantly learn from user interactions, they collect vast amounts of personal data, raising concerns about data security and potential misuse. Moreover, there is an ongoing debate about the ethical implications of assigning human-like characteristics to virtual assistants. Some argue that it may lead to users forming emotional connections with these machines, blurring the lines between human and machine interactions.\\n\\nIn conclusion, the rise of virtual assistants exemplifies the evolving landscape of human-machine interaction. While these systems offer unparalleled convenience and efficiency, they also raise important questions about privacy, security, and the ethical implications of blurring the lines between humans and machines. As technology continues to advance, it is crucial to carefully navigate these issues to ensure a harmonious and beneficial interaction between humans and machines.',\n",
       " 'Human-machine interaction, also known as HMI, is a fascinating field that explores the ways in which humans and machines interact and collaborate. One intriguing topic within HMI is the concept of haptic feedback. Haptic feedback refers to the tactile sensations that humans experience when interacting with a machine or device. This technology allows us to engage our sense of touch, enabling a more immersive and responsive interaction with machines.\\n\\nHaptic feedback has found its applications in various domains, from virtual reality (VR) gaming to medical simulations. In VR gaming, haptic feedback devices provide users with a more realistic experience by simulating physical sensations. For instance, when playing a racing game, the device might vibrate to mimic the feeling of driving on a bumpy road or provide resistance when turning a virtual steering wheel. Such feedback enhances the gaming experience and makes it more engaging and lifelike. In the medical field, haptic feedback is being utilized to train surgeons. Surgical simulators equipped with haptic devices allow trainees to practice procedures with realistic tactile sensations, improving their skills and reducing the risk during real surgeries. The ability to feel the pressure, texture, and resistance while manipulating virtual organs or tissues enhances the learning process and helps bridge the gap between theory and practice.\\n\\nOverall, haptic feedback represents a significant advancement in human-machine interaction, enabling a more intuitive and immersive experience. As this technology continues to evolve, we can expect to see its integration into an even wider range of applications, from entertainment and communication to education and industry. By incorporating the sense of touch into our interactions with machines, haptic feedback enhances our understanding, engagement, and overall satisfaction with the technology we use.',\n",
       " 'Human-machine interaction plays a pivotal role in our ever-evolving technological landscape. One fascinating aspect of this interaction is the development of virtual assistants. These intelligent systems are designed to understand and respond to human commands, making our lives more convenient and efficient. Virtual assistants, such as Siri and Alexa, have become increasingly popular and are now integrated into various devices, including smartphones, smart speakers, and even cars.\\n\\nThe success of virtual assistants lies in their ability to interpret natural language and provide accurate responses. Through advanced algorithms and machine learning, these systems learn to understand and adapt to human speech patterns, making interactions feel more human-like. Moreover, virtual assistants are constantly improving, becoming more proficient at recognizing accents, dialects, and even emotions. This enhances the user experience and fosters a stronger bond between humans and machines. As these virtual assistants continue to evolve, they have the potential to revolutionize not only our personal lives but also various industries, such as customer service and healthcare, by providing efficient and personalized assistance. Human-machine interaction has undeniably come a long way, and the future promises even more exciting advancements in this field.',\n",
       " \"Topic: The Impact of Facial Recognition Technology on Human-Machine Interaction\\n\\nFacial recognition technology has emerged as a groundbreaking innovation in the realm of human-machine interaction, revolutionizing various sectors including security, marketing, and personal devices. This technology utilizes artificial intelligence algorithms to identify and verify individuals based on their facial features. The widespread adoption of facial recognition technology has significantly transformed the way humans interact with machines, both in terms of convenience and privacy concerns.\\n\\nOne significant impact of facial recognition technology on human-machine interaction is the convenience it offers in our daily lives. From unlocking our smartphones to accessing secure areas, facial recognition has streamlined authentication processes, making them faster and more accessible. This technology eliminates the need for traditional methods like remembering passwords or carrying physical identification cards, simplifying our interactions with machines. Moreover, facial recognition has found its way into various industries, such as retail and marketing, where it enables personalized experiences. For instance, stores can use facial recognition to identify loyal customers and offer tailored recommendations, enhancing customer satisfaction and driving sales.\\n\\nHowever, the increasing prevalence of facial recognition technology has raised concerns over privacy and ethical issues. Critics argue that the collection and storage of individuals' facial data by both private companies and government agencies pose significant threats to personal privacy. The potential misuse or hacking of such data could lead to identity theft or unauthorized surveillance. Moreover, there are concerns about the accuracy and biases of facial recognition algorithms, which have been shown to have higher error rates for certain racial or gender groups. These issues have sparked debates about the need for regulations and guidelines to protect individuals' privacy while ensuring the responsible and ethical use of facial recognition technology.\\n\\nIn conclusion, facial recognition technology has undoubtedly transformed human-machine interaction, offering convenience and personalization in various domains. However, the ethical implications and privacy concerns associated with its use highlight the need for comprehensive regulations and responsible implementation. Striking a balance between leveraging the benefits of facial recognition and safeguarding individual privacy will be crucial for a harmonious future of human-machine interaction.\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 5\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(llm_chat.predict(\"Create a piece of text with 2 paragraphs about a random topic regarding human-machine interaction.\"))\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we have our examples, let's run our chain on them and check out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"- Human-machine interaction (HMI) has advanced significantly, revolutionizing technology interactions.\\n- Emotionally intelligent machines recognize and respond to human emotions, making interactions more natural and engaging.\\n- These machines use facial recognition, speech analysis, and machine learning algorithms to accurately interpret human emotions.\\n- Emotionally intelligent machines have potential applications in healthcare, monitoring patients' emotional well-being and alerting healthcare providers to intervene promptly.\\n- They can also be used in customer service settings to analyze customer emotions and offer personalized and empathetic support, improving customer satisfaction and brand loyalty.\\n- Ethical considerations include privacy concerns and the need for regulations and guidelines to ensure responsible and ethical use of emotionally intelligent machines.\\n- Despite these considerations, emotionally intelligent machines have the potential to transform technology interactions, making them more intuitive and human-like, enhancing everyday lives.\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_format = \"bullet points\"\n",
    "\n",
    "outputs = []\n",
    "for ex in examples:\n",
    "    outputs.append(chain.invoke({\"content\": ex, \"summary_format\": summary_format}))\n",
    "\n",
    "# Let's display one example output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So it seems our chain worked and we generated some summaries! Let's visualize all the summaries generated in a neat way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Output 0 \n",
       " - Human-machine interaction (HMI) has advanced significantly, revolutionizing technology interactions.\n",
       "- Emotionally intelligent machines recognize and respond to human emotions, making interactions more natural and engaging.\n",
       "- These machines use facial recognition, speech analysis, and machine learning algorithms to accurately interpret human emotions.\n",
       "- Emotionally intelligent machines have potential applications in healthcare, monitoring patients' emotional well-being and alerting healthcare providers to intervene promptly.\n",
       "- They can also be used in customer service settings to analyze customer emotions and offer personalized and empathetic support, improving customer satisfaction and brand loyalty.\n",
       "- Ethical considerations include privacy concerns and the need for regulations and guidelines to ensure responsible and ethical use of emotionally intelligent machines.\n",
       "- Despite these considerations, emotionally intelligent machines have the potential to transform technology interactions, making them more intuitive and human-like, enhancing everyday lives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 1 \n",
       " - Virtual assistants are intelligent systems designed to understand and respond to human commands and queries.\n",
       "- They bridge the gap between humans and machines, transforming the way we interact with technology.\n",
       "- Virtual assistants like Siri, Alexa, and Google Assistant rely on natural language processing and machine learning models to understand human language and provide relevant responses.\n",
       "- They can perform tasks such as setting reminders, answering questions, playing music, and controlling smart home devices.\n",
       "- The convenience and efficiency offered by virtual assistants have made them increasingly popular in daily life.\n",
       "- However, the development of virtual assistants raises concerns about privacy and ethics.\n",
       "- They collect vast amounts of personal data, raising concerns about data security and potential misuse.\n",
       "- There is an ongoing debate about the ethical implications of assigning human-like characteristics to virtual assistants.\n",
       "- Some argue that it may lead to users forming emotional connections with these machines, blurring the lines between human and machine interactions.\n",
       "- It is crucial to carefully navigate these issues to ensure a harmonious and beneficial interaction between humans and machines as technology continues to advance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 2 \n",
       " - Human-machine interaction (HMI) explores how humans and machines interact and collaborate.\n",
       "- Haptic feedback is a topic within HMI that refers to tactile sensations experienced when interacting with machines.\n",
       "- Haptic feedback technology enables a more immersive and responsive interaction with machines.\n",
       "- Haptic feedback has applications in various domains, such as virtual reality gaming and medical simulations.\n",
       "- In VR gaming, haptic feedback devices provide realistic physical sensations to enhance the gaming experience.\n",
       "- In the medical field, haptic feedback is used to train surgeons by simulating realistic tactile sensations during procedures.\n",
       "- Haptic feedback enhances human-machine interaction, making it more intuitive and immersive.\n",
       "- As haptic feedback technology evolves, it can be integrated into a wider range of applications, including entertainment, communication, education, and industry."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 3 \n",
       " - Human-machine interaction is crucial in the technological world\n",
       "- Virtual assistants like Siri and Alexa are intelligent systems designed to understand and respond to human commands\n",
       "- Virtual assistants are integrated into various devices such as smartphones, smart speakers, and cars\n",
       "- Success of virtual assistants lies in their ability to interpret natural language and provide accurate responses\n",
       "- Advanced algorithms and machine learning enable virtual assistants to understand and adapt to human speech patterns\n",
       "- Virtual assistants are constantly improving and becoming better at recognizing accents, dialects, and emotions\n",
       "- Virtual assistants have the potential to revolutionize industries such as customer service and healthcare\n",
       "- Future advancements in human-machine interaction hold exciting potential"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 4 \n",
       " - Facial recognition technology revolutionizes human-machine interaction in sectors like security, marketing, and personal devices.\n",
       "- It utilizes AI algorithms to identify and verify individuals based on their facial features.\n",
       "- Facial recognition offers convenience by streamlining authentication processes and eliminating the need for traditional methods like passwords or physical identification cards.\n",
       "- It enables personalized experiences in industries like retail and marketing by identifying loyal customers and offering tailored recommendations.\n",
       "- Privacy concerns arise from the collection and storage of facial data, posing risks of identity theft and unauthorized surveillance.\n",
       "- Biases and inaccuracies in facial recognition algorithms have been observed, particularly with certain racial or gender groups.\n",
       "- Regulations and guidelines are needed to protect privacy and ensure responsible and ethical use of facial recognition technology."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "for i in range(num_examples):\n",
    "    display(Markdown(f\"Output {i} \\n {outputs[i].content}\"))\n",
    "# Markdown(f\"**Input**: {examples[0]}\\n\\n**Output**: {outputs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our summaries worked, and we were able to apply a given summary format to all of them.\n",
    "\n",
    "LangChain is an extremely powerful library to work with abstractions like these and throughout this course we hope to give you a gliimpse of the cool stuff you can build with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
