{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "#!pip install langchain\n",
    "#!pip install docarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain \n",
    "\n",
    "Working with LLMs involves in one way or another working with a specific type of abstraction: \"Prompts\".\n",
    "\n",
    "However, in the practical context of day-to-day tasks we expect LLMs to perform, these prompts won't be some static and dead type of abstraction. Instead we'll work with dynamic prompts re-usable prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lanchain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) is a framework that allows you to connect LLMs together by allowing you to work with modular components like prompt templates and chains giving you immense flexibility in creating tailored solutions powered by the capabilities of large language models.\n",
    "\n",
    "\n",
    "Its main features are:\n",
    "- **Components**: abstractions for working with LMs\n",
    "- **Off-the-shelf chains**: assembly of components for accomplishing certain higher-level tasks\n",
    "\n",
    "LangChain facilitates the creation of complex pipelines that leverage the connection of components like chains, prompt templates, output parsers and others to compose intricate pipelines that give you everything you need to solve a wide variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of LangChain, we have the following elements:\n",
    "\n",
    "- Models\n",
    "- Prompts\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Models**\n",
    "\n",
    "Models are nothing more than abstractions over the LLM APIs like the ChatGPT API.â€‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"]=\"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\"\n",
    "chat_model = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can predict outputs from both LLMs and ChatModels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "chat_model.predict(\"hi!\")\n",
    "# Output: \"Hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic components are:\n",
    "\n",
    "- Models\n",
    "- Prompt tempaltes\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Show me 5 examples of this concept: {concept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['concept'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['concept'], template='Show me 5 examples of this concept: {concept}'))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Show me 5 examples of this concept: animal'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(concept=\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. A lion is a powerful animal known for its strength and hunting abilities.\\n2. A giraffe is a tall animal with a long neck and spotted coat.\\n3. A dolphin is an intelligent and playful marine animal known for its acrobatic abilities.\\n4. A koala is a cute and cuddly animal native to Australia known for its love of eucalyptus leaves.\\n5. A bald eagle is a majestic bird of prey and a symbol of the United States.')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | chat_model\n",
    "\n",
    "output = chain.invoke({\"concept\": \"animal\"})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. A lion is a powerful animal known for its strength and hunting abilities.\\n2. A giraffe is a tall animal with a long neck and spotted coat.\\n3. A dolphin is an intelligent and playful marine animal known for its acrobatic abilities.\\n4. A koala is a cute and cuddly animal native to Australia known for its love of eucalyptus leaves.\\n5. A bald eagle is a majestic bird of prey and a symbol of the United States.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.predict(\"I am teaching a live-training about LLMs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the predict method over a string input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What would be a good name for a dog that loves to nap??\"\n",
    "\n",
    "\n",
    "chat_model.predict(text)\n",
    "# Output: \"Snuggles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the `predict_messages` method over a list of messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What would be a good dog name for a dog that loves to nap?\"\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "chat_model.predict_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point let's stop and take a look at what this code would look like if we were using the openai api directly instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand what is going on.\n",
    "\n",
    "Instead of writing down the human message dictionary for the openai API as you would do normally using the the original API, langchain is giving you an abstraction over that message through the class\n",
    "`HumanMessage()`, as well as an abstraction over the loop for multiple predictions through the .`predict_messages()` method.\n",
    "\n",
    "Now, why is that an useful thing?\n",
    "\n",
    "Because it allows you to work at a higher level of experimentation and orchestration with the blocks of that make up a workflow using LLMs.\n",
    "\n",
    "By making it easier to create predictions of multiple messages for example, you can experiment with different human message prompts faster and therefore get to better and more efficient results faster without having to write a lot of boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompts**\n",
    "\n",
    "The same works for prompts. Now, prompts are pieces of text we feed to LLMs, and LangChain allows you to work with prompt templates.\n",
    "\n",
    "Prompt Templates are useful abstractions for reusing prompts and they are used to provide context for the specific task that the language model needs to complete. \n",
    "\n",
    "A simple example is a `PromptTemplate` that formats a string into a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "prompt.format(activity=\"sleeping\")\n",
    "# Output: \"What is a good dog name for a dog that loves to nap?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output Parsers**\n",
    "\n",
    "OutputParsers convert the raw output from an LLM into a format that can be used downstream. Here is an example of an OutputParser that converts a comma-separated list into a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n",
    "# Output: ['hi', 'bye']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to an LLM, and then pass the output through an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so these are the basics of langchain. But how can we leverage these abstraction capabilities inside our LLM app application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to put everything together LangChain allows you to build something called \"chains\", which are components that connect prompts, llms and output parsers into a building block that allows you to create more interesting and complex functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A good dog name for a dog that loves to sleep could be \"Snuggles\", \"Dreamer\", \"Snoozy\", \"Dozer\", \"Napper\", \"Slumber\", \"Rest\", \"Cozy\", \"Pillow\", or \"Snooze\".'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good dog name for a dog that loves to {activity}?\")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    prompt=prompt,\n",
    ")\n",
    "chain.run(\"sleep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what the chain is doing is connecting these basic components (the LLM and the prompt template) into\n",
    "a block that can be run separately. The chain allows you to turn workflows using LLLMs into this modular process of composing components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the newer versions of LangChain have a new representation language to create these chains (and more) known as LCEL or LangChain expression language, which is a declarative way to easily compose chains together. The same example as above expressed in this LCEL format would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A good dog name for a dog that loves to sleep could be \"Snuggles\", \"Snooze\", \"Dreamer\", \"Napper\", \"Dozer\", \"Slumber\", \"Cuddles\", \"Pillow\", \"Cozy\", or \"Snoozy\".')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | ChatOpenAI()\n",
    "\n",
    "chain.invoke({\"activity\": \"sleep\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the output is an `AIMessage()` object, which represents LangChain's way to abstract the output from an LLM model like ChatGPT or others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These building blocks and abstractions that LangChain provides are what makes this library so unique, because it gives you the tools you didn't know you need it to build awesome stuff powered by LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Lab Exercises\n",
    "\n",
    "Let's take a look at a few examples using some of the capabilities of the LangChain library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a prompt to summarize any piece of text into some desirable format. \n",
    "\n",
    "Let's make use of the `PromptTemplate` to abstract away the following pieces of the prompt: \n",
    "- `content` - the text content to be summarized  \n",
    "- `summary_format` - the format in which we want the summary to be presented (like bullet points and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize this: This is a test.. The output should be in the following format: One word summary.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Summarize this: {content}. The output should be in the following format: {summary_format}.\")\n",
    "\n",
    "# We can look at a simple example to illustrate what that prompt is doing\n",
    "prompt.format(content=\"This is a test.\", summary_format=\"One word summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have our prompt template done, let's load the llm and create a nice chain to put everything together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm_chat =  ChatOpenAI()\n",
    "chain = prompt | llm_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have our chain we can run some tests. The cool thing about working with LLMs is that you can use them to create examples for simple tests like this (avoiding the annoynace of searching online for some piece of text, copying and pasting etc...). So, let's generate a few examples of tests below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Human-machine interaction, also known as HMI, is an ever-evolving field that explores the relationship between humans and machines. Over the years, this interaction has become increasingly seamless, with machines becoming more intelligent and intuitive. One fascinating aspect of HMI is the integration of voice recognition technology, which has revolutionized the way we interact with machines.\\n\\nVoice recognition technology allows us to communicate with machines using our natural language, eliminating the need for complex interfaces or physical input devices. Whether it's asking a virtual assistant to play our favorite song or instructing a smart home device to turn off the lights, voice recognition technology has made our lives more convenient. However, there are still challenges in perfecting this technology. Accents, background noise, and the use of colloquial language can pose difficulties for machines to accurately interpret and respond to human speech. As researchers continue to improve voice recognition algorithms, we can expect even more sophisticated machines that understand and respond to human commands flawlessly.\\n\\nAnother intriguing aspect of HMI is the concept of human augmentation. As technology advances, there is increasing potential for machines to enhance human capabilities. For example, prosthetic limbs controlled by neural interfaces can restore mobility to individuals with amputations, granting them a new sense of independence. Furthermore, brain-computer interfaces offer the possibility of directly connecting our brains with machines, enabling us to control devices or even communicate telepathically. While these advancements hold incredible promise, ethical considerations must be taken into account, such as privacy concerns and the potential for misuse of such technologies.\\n\\nIn conclusion, human-machine interaction is a dynamic field that continues to shape our everyday lives. From voice recognition technology to human augmentation, the relationship between humans and machines is becoming more intertwined and sophisticated. While there are challenges to overcome, the potential benefits that HMI offers are vast. As technology continues to advance, it is crucial to strike a balance between harnessing the power of machines and preserving our human values.\",\n",
       " 'Human-machine interaction, also known as HMI, has experienced significant advancements in recent years, revolutionizing various aspects of our lives. One fascinating topic within this field is the integration of artificial intelligence (AI) in healthcare systems. AI-powered machines are now capable of assisting medical professionals in diagnosing diseases, analyzing medical images, and even performing surgeries. This collaboration between humans and machines has the potential to improve patient outcomes by reducing errors, enhancing accuracy, and increasing efficiency in healthcare services.\\n\\nMoreover, the use of chatbots in customer service is another remarkable development in human-machine interaction. Chatbots, powered by natural language processing and machine learning algorithms, can engage in conversations with users, addressing their queries and concerns in real-time. These intelligent virtual assistants not only provide immediate responses but also learn from their interactions, continuously improving their abilities to understand and communicate with humans. By automating customer support, businesses can enhance user experience, reduce response time, and handle a higher volume of inquiries efficiently, resulting in increased customer satisfaction and loyalty.\\n\\nIn both these scenarios, human-machine interaction has proven to be transformative, augmenting human capabilities while streamlining processes. As technology continues to evolve, it is crucial to strike a balance between the roles of humans and machines, ensuring that they complement each other effectively. With responsible development and integration, human-machine interaction holds great potential to shape a future where humans and machines work harmoniously, revolutionizing industries and enhancing overall human well-being.',\n",
       " \"Human-Machine Interaction (HMI) has become an integral part of our daily lives, revolutionizing the way we interact with technology. One fascinating aspect of HMI is the development of virtual assistants, such as Siri, Alexa, and Google Assistant. These intelligent systems employ natural language processing and machine learning algorithms to understand and respond to human commands and queries.\\n\\nVirtual assistants have become more than just a tool for retrieving information; they have evolved into personal assistants that can perform various tasks on our behalf. From setting reminders and sending messages to controlling smart home devices, virtual assistants have become an essential part of our routine. However, the challenge lies in striking the balance between convenience and privacy. As these assistants become more integrated into our lives, concerns about data security and privacy have also emerged. Users must navigate the fine line between enjoying the benefits of seamless interactions and safeguarding their personal information.\\n\\nAnother aspect of HMI that has gained significant attention is the development of exoskeletons and robotic prosthetics. These innovations have the potential to greatly improve the quality of life for individuals with mobility impairments. These devices use sensors and actuators to detect and mimic human movements, allowing users to regain lost mobility and perform everyday tasks. The interaction between humans and these machines is a remarkable example of how technology can augment our capabilities. However, challenges remain in terms of affordability and accessibility, as these devices are often expensive and not readily available to all who could benefit from them.\\n\\nHuman-Machine Interaction continues to evolve, blurring the lines between humans and machines. As technology advances, it is crucial to ensure that these interactions are designed with the user's best interests in mind, prioritizing privacy, accessibility, and inclusivity. Ultimately, the goal is to create a harmonious relationship where humans and machines work together to enhance our capabilities and improve our lives.\",\n",
       " 'Human-machine interaction, also known as HMI, is a fascinating field that explores the interaction between humans and machines. One interesting topic in this domain is the use of voice assistants in everyday life. Voice assistants like Siri, Alexa, and Google Assistant have become an integral part of our lives, providing us with a seamless way to interact with technology.\\n\\nThese voice assistants have evolved significantly over the years, with advancements in natural language processing and machine learning. They can now understand and respond to complex queries, making them more human-like in their interactions. This has transformed the way we interact with technology, allowing us to use voice commands to perform tasks such as setting reminders, playing music, or even controlling home appliances. The convenience offered by voice assistants has made them increasingly popular, with more and more people relying on them for various tasks.\\n\\nHowever, the rise of voice assistants has also raised concerns about privacy and data security. As voice assistants constantly listen for voice commands, there is a risk of unintended recordings or potential breaches of privacy. Additionally, the data collected by these voice assistants is often stored in the cloud, raising questions about who has access to this information and how it is being used. Striking a balance between the benefits of voice assistants and ensuring the privacy of users is an ongoing challenge in the field of human-machine interaction.\\n\\nIn conclusion, the use of voice assistants in everyday life is a compelling topic in the realm of human-machine interaction. While they offer convenience and efficiency, concerns about privacy and data security continue to be important considerations. As technology continues to evolve, it is crucial to address these concerns and create a framework that ensures a seamless and secure interaction between humans and machines.',\n",
       " \"Human-machine interaction is rapidly evolving, revolutionizing the way we live and work. One fascinating aspect of this interaction is the concept of virtual assistants. These intelligent systems have become an integral part of our daily lives, assisting us with a myriad of tasks. Whether it's answering queries, setting reminders, or playing music, virtual assistants have seamlessly integrated into our routines, blurring the line between humans and machines.\\n\\nThe rise of virtual assistants has sparked debates about the impact on human interaction. On one hand, proponents argue that these assistants enhance productivity and efficiency, freeing up valuable time for more meaningful activities. They provide quick and accurate information, helping us make informed decisions. However, critics express concerns about the potential loss of human connection. As we rely more on virtual assistants, there is a risk of isolation and reduced social interaction. Some worry that constant interaction with machines might weaken our ability to communicate effectively with other humans, hindering interpersonal skills.\\n\\nNonetheless, it is essential to strike a balance between the convenience of virtual assistants and maintaining healthy human relationships. While these intelligent systems offer benefits, it is crucial to remember that human interaction is irreplaceable. As we continue to develop and improve human-machine interaction, it is essential to ensure that technology enhances our lives without diminishing the value of human connection.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 5\n",
    "examples = []\n",
    "for i in range(num_examples):\n",
    "    examples.append(llm_chat.predict(\"Create a piece of text with 2 paragraphs about a random topic regarding human-machine interaction.\"))\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now that we have our examples, let's run our chain on them and check out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='- Human-machine interaction (HMI) explores the relationship between humans and machines.\\n- HMI has become more seamless as machines have become more intelligent and intuitive.\\n- Voice recognition technology allows us to communicate with machines using natural language.\\n- Voice recognition technology has made our lives more convenient by eliminating complex interfaces or physical input devices.\\n- Challenges in voice recognition technology include accents, background noise, and colloquial language.\\n- Researchers are improving voice recognition algorithms to enhance machine understanding and response to human speech.\\n- Human augmentation is the concept of machines enhancing human capabilities.\\n- Prosthetic limbs controlled by neural interfaces can restore mobility to amputees.\\n- Brain-computer interfaces can directly connect our brains with machines for control or communication.\\n- Ethical considerations such as privacy concerns and potential misuse of technology must be addressed.\\n- HMI continues to shape everyday lives and offers vast potential benefits.\\n- Striking a balance between harnessing machine power and preserving human values is crucial.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_format = \"bullet points\"\n",
    "\n",
    "outputs = []\n",
    "for ex in examples:\n",
    "    outputs.append(chain.invoke({\"content\": ex, \"summary_format\": summary_format}))\n",
    "\n",
    "# Let's display one example output\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! So it seems our chain worked and we generated some summaries! Let's visualize all the summaries generated in a neat way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Output 0 \n",
       " - Human-machine interaction (HMI) explores the relationship between humans and machines.\n",
       "- HMI has become more seamless as machines have become more intelligent and intuitive.\n",
       "- Voice recognition technology allows us to communicate with machines using natural language.\n",
       "- Voice recognition technology has made our lives more convenient by eliminating complex interfaces or physical input devices.\n",
       "- Challenges in voice recognition technology include accents, background noise, and colloquial language.\n",
       "- Researchers are improving voice recognition algorithms to enhance machine understanding and response to human speech.\n",
       "- Human augmentation is the concept of machines enhancing human capabilities.\n",
       "- Prosthetic limbs controlled by neural interfaces can restore mobility to amputees.\n",
       "- Brain-computer interfaces can directly connect our brains with machines for control or communication.\n",
       "- Ethical considerations such as privacy concerns and potential misuse of technology must be addressed.\n",
       "- HMI continues to shape everyday lives and offers vast potential benefits.\n",
       "- Striking a balance between harnessing machine power and preserving human values is crucial."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 1 \n",
       " - Human-machine interaction (HMI) has made significant advancements in recent years.\n",
       "- AI integration in healthcare systems has revolutionized disease diagnosis, medical image analysis, and even surgeries.\n",
       "- Collaboration between humans and machines in healthcare can improve patient outcomes by reducing errors, enhancing accuracy, and increasing efficiency.\n",
       "- Chatbots in customer service use natural language processing and machine learning algorithms to engage in real-time conversations with users.\n",
       "- Chatbots continuously improve their abilities to understand and communicate with humans by learning from their interactions.\n",
       "- Automated customer support with chatbots enhances user experience, reduces response time, and handles a higher volume of inquiries efficiently.\n",
       "- HMI transforms industries and enhances overall human well-being by augmenting human capabilities and streamlining processes.\n",
       "- Responsible development and integration of HMI is crucial to strike a balance between humans and machines.\n",
       "- HMI holds great potential to shape a future where humans and machines work harmoniously."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 2 \n",
       " - Human-Machine Interaction (HMI) has revolutionized the way we interact with technology.\n",
       "- Virtual assistants like Siri, Alexa, and Google Assistant use natural language processing and machine learning to understand and respond to human commands.\n",
       "- Virtual assistants have evolved into personal assistants that can perform various tasks on our behalf.\n",
       "- Striking a balance between convenience and privacy is a challenge with virtual assistants.\n",
       "- Exoskeletons and robotic prosthetics have the potential to improve the quality of life for individuals with mobility impairments.\n",
       "- Affordability and accessibility are challenges for exoskeletons and robotic prosthetics.\n",
       "- HMI blurs the lines between humans and machines.\n",
       "- It is crucial to prioritize privacy, accessibility, and inclusivity in HMI design.\n",
       "- The goal is to create a harmonious relationship between humans and machines to enhance our capabilities and improve our lives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 3 \n",
       " - Human-machine interaction (HMI) explores the interaction between humans and machines.\n",
       "- Voice assistants like Siri, Alexa, and Google Assistant are popular examples of HMI technology.\n",
       "- Voice assistants have evolved with advancements in natural language processing and machine learning.\n",
       "- They can understand and respond to complex queries, making them more human-like.\n",
       "- Voice assistants offer convenience by allowing users to perform tasks through voice commands.\n",
       "- Concerns about privacy and data security arise due to unintended recordings and cloud storage of data.\n",
       "- Striking a balance between the benefits and privacy of voice assistants is an ongoing challenge.\n",
       "- It is important to address these concerns and create a framework for secure interaction between humans and machines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Output 4 \n",
       " - Human-machine interaction is evolving and virtual assistants are a significant aspect of this interaction.\n",
       "- Virtual assistants seamlessly integrate into our daily lives and assist us with various tasks.\n",
       "- Proponents argue that virtual assistants enhance productivity and efficiency by providing quick and accurate information.\n",
       "- Critics express concerns about the potential loss of human connection and reduced social interaction.\n",
       "- There is a risk of weakened interpersonal skills due to constant interaction with machines.\n",
       "- It is important to strike a balance between the convenience of virtual assistants and maintaining healthy human relationships.\n",
       "- Human interaction is irreplaceable and technology should enhance our lives without diminishing the value of human connection."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "for i in range(num_examples):\n",
    "    display(Markdown(f\"Output {i} \\n {outputs[i].content}\"))\n",
    "# Markdown(f\"**Input**: {examples[0]}\\n\\n**Output**: {outputs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our summaries worked, and we were able to apply a given summary format to all of them.\n",
    "\n",
    "LangChain is an extremely powerful library to work with abstractions like these and throughout this course we hope to give you a gliimpse of the cool stuff you can build with it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
