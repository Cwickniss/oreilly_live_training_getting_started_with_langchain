{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prompt - just some text input to the model\n",
    "- Model - LLM like OpenAI, Llama etc...\n",
    "- Output parser - something that turns an output from an LLM into some workable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"List 5 examples of this: {input}\")\n",
    "\n",
    "# temperature means how precise the model is, low means more precise\n",
    "llm_chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: List 5 examples of this: apple'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(input=\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting everything together using the LCEL interface\n",
    "\n",
    "chain = prompt | llm_chat | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='List 5 examples of this: {input}'))])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x117959b90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1179823d0>, temperature=0.0, openai_api_key='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', openai_proxy='')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Why don't scientists trust atoms? Because they make up everything!\\n2. Why don't skeletons fight each other? They don't have the guts!\\n3. Why did the scarecrow win an award? Because he was outstanding in his field!\\n4. Why don't scientists trust stairs? Because they're always up to something!\\n5. Why did the bicycle fall over? Because it was two-tired!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"funny jokes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.runnables.base.RunnableSequence"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a Runnable Sequence?\n",
    "\n",
    "What is a Runnable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the interface behind the LCEL language that is under the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable interface!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a runnable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A runnable is a component that you can run, and compose, which can be a chain, a prompt, an output parser and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: I want examples of this concept: apple'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEMPLATE = \"I want examples of this concept: {input}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "\n",
    "prompt.format(input=\"apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='I want examples of this concept: apple')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this shows the output of the prompt throughout the chain\n",
    "prompt.invoke({\"input\": \"apple\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm_chat | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Apple\\n- Banana\\n- Orange\\n- Strawberry\\n- Grape\\n- Pineapple\\n- Watermelon\\n- Mango\\n- Kiwi\\n- Peach'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"fruit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_chain = chain | RunnableLambda(lambda x: len(x))\n",
    "\n",
    "new_chain.invoke({\"input\": \"fruit\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda` allows you to apply an intermediary function to the output of the previous component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if I wanted to get the output text and the length of the output together? How could I do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: I want examples of this concept: fruit'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format(input=\"fruit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='I want examples of this concept: fruit')])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "silly_chain = prompt | RunnablePassthrough()\n",
    "\n",
    "silly_chain.invoke({\"input\": \"fruit\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'fruit', 'new_key': 'fruit and tell me a joke about it.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly_chain_with_extra_key = RunnablePassthrough.assign(new_key=lambda x: x[\"input\"] + \" and tell me a joke about it.\")\n",
    "\n",
    "silly_chain_with_extra_key.invoke({\"input\": \"fruit\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`RunnablePassthrough`](https://python.langchain.com/docs/expression_language/how_to/passthrough#:~:text=RunnablePassthrough%20allows%20to,pass%20it%20through.) takes in a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_from_llm': 'Apple, orange, banana, pineapple, watermelon, grapefruit, mango, lemon, strawberry, kiwi, peach, pear, cherry, blueberry, raspberry',\n",
       " 'size_of_output': 131}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm_chat | output_parser | RunnableLambda(lambda x: dict(output_from_llm=x)) | RunnablePassthrough.assign(size_of_output=lambda x: len(x[\"output_from_llm\"]))\n",
    "                                                                                                            \n",
    "chain.invoke({\"input\": \"fruit\"})                                                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about running multiple calls to an llm at the same time?\n",
    "\n",
    "You can do that easily with `RunnableParallel`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = prompt | llm_chat | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 9.96 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "6.9 s ± 5.1 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "chain1.invoke({\"input\": \"animal\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"Ask me questions to understand this concept: {concept}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(TEMPLATE)\n",
    "\n",
    "llm_chat = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm_chat | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.29 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "4.1 s ± 1.73 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "chain2.invoke({\"concept\": \"joint distributions\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(chain1=chain1, chain2=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "4.42 s ± 2.38 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "chain.invoke({\"input\": \"animal\", \"concept\": \"joint distributions\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For you to practice!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
