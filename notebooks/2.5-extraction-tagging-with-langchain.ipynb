{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heavily inspired by this course: https://learn.deeplearning.ai/functions-tools-agents-langchain/lesson/5/tagging-and-extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging and Extraction Using OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv(\"../.env\")) # read local .env file\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class TaggingRefsUrls(BaseModel):\n",
    "    \"\"\"Extracting URLs from a paper references section.\"\"\"\n",
    "    urls: List[str] = Field(description=\"A list of urls starting with 'http:...' from the references section of a paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'TaggingRefsUrls',\n",
       " 'description': 'Extracting URLs from a paper references section.',\n",
       " 'parameters': {'title': 'TaggingRefsUrls',\n",
       "  'description': 'Extracting URLs from a paper references section.',\n",
       "  'type': 'object',\n",
       "  'properties': {'urls': {'title': 'Urls',\n",
       "    'description': \"A list of urls starting with 'http:...' from the references section of a paper\",\n",
       "    'type': 'array',\n",
       "    'items': {'type': 'string'}}},\n",
       "  'required': ['urls']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "\n",
    "convert_pydantic_to_openai_function(TaggingRefsUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "tagging_functions = [convert_pydantic_to_openai_function(TaggingRefsUrls)]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"TaggingRefsUrls\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Think carefully, and then tag the text as instructed')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11258ce90>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1125b1450>, temperature=0.0, openai_api_key='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', openai_proxy=''), kwargs={'functions': [{'name': 'TaggingRefsUrls', 'description': 'Extracting URLs from a paper references section.', 'parameters': {'title': 'TaggingRefsUrls', 'description': 'Extracting URLs from a paper references section.', 'type': 'object', 'properties': {'urls': {'title': 'Urls', 'description': \"A list of urls starting with 'http:...' from the references section of a paper\", 'type': 'array', 'items': {'type': 'string'}}}, 'required': ['urls']}}], 'function_call': {'name': 'TaggingRefsUrls'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagging_urls_chain = prompt | model_with_functions\n",
    "\n",
    "tagging_urls_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"urls\": [\\n    \"https://arxiv.org/abs/2212.10466\",\\n    \"https://arxiv.org/abs/2304.05128\",\\n    \"https://arxiv.org/abs/2110.14168\",\\n    \"https://aclanthology.org/2022.emnlp-main.222\",\\n    \"https://arxiv.org/abs/2309.16797\",\\n    \"https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving\",\\n    \"https://aclanthology.org/2023.acl-long.108\"\\n  ]\\n}', 'name': 'TaggingRefsUrls'}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs_string = \"\"\"REFERENCES\n",
    "Howard Chen, Huihan Li, Danqi Chen, and Karthik Narasimhan. Controllable text generation with\n",
    "language constraints. arXiv preprint arXiv:2212.10466, 2022.\n",
    "Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models ¨\n",
    "to self-debug. arXiv preprint arXiv:2304.05128, 2023.\n",
    "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\n",
    "Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\n",
    "solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n",
    "Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,\n",
    "Eric Xing, and Zhiting Hu. RLPrompt: Optimizing discrete text prompts with reinforcement\n",
    "learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\n",
    "Processing, pp. 3369–3391, Abu Dhabi, United Arab Emirates, December 2022. Association for\n",
    "Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.222. URL https://aclanthology.\n",
    "org/2022.emnlp-main.222.\n",
    "Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel. ¨\n",
    "Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint\n",
    "arXiv:2309.16797, 2023.\n",
    "Noah Goodman. Meta-prompt: A simple self-improving language agent, 2023. URL https:\n",
    "//noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving.\n",
    "Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few\n",
    "examples to natural language task descriptions. In Proceedings of the 61st Annual Meeting of the\n",
    "Association for Computational Linguistics (Volume 1: Long Papers), pp. 1935–1952, Toronto,\n",
    "Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.\n",
    "108. URL https://aclanthology.org/2023.acl-long.108.\n",
    "1\"\"\"\n",
    "tagging_urls_chain.invoke({\"input\": refs_string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'urls': ['https://arxiv.org/abs/2212.10466',\n",
       "  'https://arxiv.org/abs/2304.05128',\n",
       "  'https://arxiv.org/abs/2110.14168',\n",
       "  'https://aclanthology.org/2022.emnlp-main.222',\n",
       "  'https://arxiv.org/abs/2309.16797',\n",
       "  'https://noahgoodman.substack.com/p/meta-prompt-a-simple-self-improving',\n",
       "  'https://aclanthology.org/2023.acl-long.108']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "tagging_urls_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
    "\n",
    "tagging_urls_chain.invoke({\"input\": refs_string})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It seems that we were able to extract what we wanted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "\n",
    "Extraction is used for extracting multiple pieces of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Knowledge',\n",
       " 'description': 'Information to extract.',\n",
       " 'parameters': {'title': 'Knowledge',\n",
       "  'description': 'Information to extract.',\n",
       "  'type': 'object',\n",
       "  'properties': {'papers': {'title': 'Papers',\n",
       "    'description': 'List of info about scientific papers',\n",
       "    'type': 'array',\n",
       "    'items': {'title': 'Paper',\n",
       "     'description': 'Information about a person.',\n",
       "     'type': 'object',\n",
       "     'properties': {'title': {'title': 'Title',\n",
       "       'description': \"paper's title\",\n",
       "       'type': 'string'},\n",
       "      'year': {'title': 'Year',\n",
       "       'description': \"paper's year\",\n",
       "       'type': 'integer'}},\n",
       "     'required': ['title']}}},\n",
       "  'required': ['papers']}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "    title: str = Field(description=\"paper's title\")\n",
    "    year: Optional[int] = Field(description=\"paper's year\")\n",
    "\n",
    "class Knowledge(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    papers: List[Paper] = Field(description=\"List of info about scientific papers\")\n",
    "\n",
    "convert_pydantic_to_openai_function(Knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4\")\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Knowledge)]\n",
    "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Knowledge\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_contents = \"\"\"\n",
    "PROMPT ENGINEERING A PROMPT ENGINEER\n",
    "Qinyuan Ye1† Maxamed Axmed2 Reid Pryzant2 Fereshte Khani2\n",
    "1University of Southern California 2Microsoft\n",
    "qinyuany@usc.edu fkhani@microsoft.com\n",
    "ABSTRACT\n",
    "Prompt engineering is a challenging yet crucial task for optimizing the performance of large language models (LLMs). It requires complex reasoning to examine the model’s errors, hypothesize what is missing or misleading in the current\n",
    "prompt, and communicate the task with clarity. While recent works indicate that\n",
    "LLMs can be meta-prompted to perform automatic prompt engineering, their potentials may not be fully untapped due to the lack of sufficient guidance to elicit\n",
    "complex reasoning capabilities in LLMs in the meta-prompt. In this work, we investigate the problem of “prompt engineering a prompt engineer”—constructing\n",
    "a meta-prompt that more effectively guides LLMs to perform automatic prompt\n",
    "engineering. We introduce and analyze key components, such as a step-by-step\n",
    "reasoning template and context specification, which lead to improved performance. In addition, inspired by common optimization concepts such as batch\n",
    "size, step size and momentum, we introduce their verbalized counterparts to the\n",
    "meta-prompt and investigate their effects. Our final method, named PE2, finds\n",
    "a prompt that outperforms “let’s think step by step” by 6.3% on the MultiArith\n",
    "dataset and 3.1% on the GSM8K dataset. To demonstrate its versatility, we apply\n",
    "PE2 to the Instruction Induction benchmark, a suite of counterfactual tasks, and a\n",
    "lengthy, real-world industrial prompt. In these settings, PE2 achieves strong performance and outperforms prior automatic prompt engineering baselines. Further,\n",
    "we show that PE2 makes meaningful and targeted prompt edits, amends erroneous\n",
    "or incomplete prompts, and presents non-trivial counterfactual reasoning abilities.\n",
    "1 INTRODUCTION\n",
    "Large language models (LLMs) are powerful tools for many natural language processing tasks when\n",
    "given the right prompts.1 However, finding the optimal prompt can be challenging due to the model’s\n",
    "sensitivity (Jiang et al., 2020; Zhao et al., 2021; Lu et al., 2022), often necessitating extensive manual\n",
    "trial-and-error efforts. Moreover, once an initial prompt is deployed into production, unforeseen\n",
    "edge cases may emerge, demanding more rounds of manual efforts to further refine the prompt.\n",
    "These challenges give rise to an emerging research field of automatic prompt engineering. Within\n",
    "this field, a notable line of methods involves harnessing the capabilities of LLMs themselves (Zhou\n",
    "et al., 2023b; Pryzant et al., 2023). Specifically, this entails meta-prompting LLMs with instructions\n",
    "such as “inspect the current prompt and a batch of examples, then propose a new prompt.”\n",
    "While these methods achieve impressive performance, a subsequent question arises: What makes a\n",
    "good meta-prompt for automatic prompt engineering? To answer this question, we connect two key\n",
    "observations: (1) Prompt engineering itself is a complex language task that requires deep reasoning:\n",
    "it involves closely examining the model’s errors, hypothesizing what is missing or misleading in\n",
    "the current prompt, and communicating the task more clearly to the LLM. (2) Complex reasoning\n",
    "capabilities in LLMs can be elicited by prompting the model to “think step by step” (Wei et al.,\n",
    "2022; Kojima et al., 2022) and can be further improved by instructing them to reflect on their outputs\n",
    "(Madaan et al., 2023; Chen et al., 2023).\n",
    "†Work done while interning at Microsoft.\n",
    "1The word “prompt” is often overloaded with multiple meanings in recent literature. In this paper, prompt\n",
    "refers to the task description (e.g., “Translate English to French”) or instruction (e.g., “Let’s think step by step”).\n",
    "1\n",
    "arXiv:2311.05661v1 [cs.CL] 9 Nov 2023\n",
    "Preprint\n",
    "MultiArith GSM8K Instruction Induction\n",
    "(14 Tasks)\n",
    "Counterfactual Eval\n",
    "(12 Tasks)\n",
    "25\n",
    "50\n",
    "75\n",
    "100\n",
    "(Average) Accuracy\n",
    "+6.3\n",
    "+3.1\n",
    "+7.4\n",
    "+10.6\n",
    "Initialization\n",
    "Iterative APE\n",
    "APO\n",
    "PE2 (This Work)\n",
    "Production\n",
    "Prompt\n",
    "F1 Score\n",
    "+8.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"papers\": [\\n    {\\n      \"title\": \"Engineering a Prompt Engineer\",\\n      \"year\": 2023\\n    }\\n  ]\\n}', 'name': 'Knowledge'}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "extraction_chain = prompt | extraction_model\n",
    "\n",
    "extraction_chain.invoke({\"input\": paper_contents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'papers': [{'title': 'PROMPT ENGINEERING A PROMPT ENGINEER', 'year': 2023}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\n",
    "\n",
    "extraction_chain.invoke({\"input\": paper_contents})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Engineering a Prompt Engineer', 'year': 2023}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "\n",
    "extraction_chain.invoke({\"input\": paper_contents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction with Parallel Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel function calling\n",
    "# The example below was taken from here: https://github.com/langchain-ai/langchain/blob/master/cookbook/extraction_openai_tools.ipynb?ref=blog.langchain.dev\n",
    "\n",
    "from typing import List, Optional\n",
    "from langchain.chains.openai_tools import create_extraction_chain_pydantic\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.pydantic_v1 import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use a recent model that supports tools\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic is an easy way to define a schema\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about people to extract.\"\"\"\n",
    "\n",
    "    name: str\n",
    "    age: Optional[int] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain_pydantic(Person, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Person(name='jane', age=2), Person(name='bob', age=3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"jane is 2 and bob is 3\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define another element\n",
    "class Class(BaseModel):\n",
    "    \"\"\"Information about classes to extract.\"\"\"\n",
    "\n",
    "    teacher: str\n",
    "    students: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_extraction_chain_pydantic([Person, Class], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Person(name='jane', age=2),\n",
       " Person(name='bob', age=3),\n",
       " Class(teacher='Mrs Sampson', students=['jane', 'bob'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"jane is 2 and bob is 3 and they are in Mrs Sampson's class\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
