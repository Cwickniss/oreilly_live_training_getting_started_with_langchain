{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chains in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\",\\n\\nI'm sorry to hear that you're having trouble getting your computer to start up. This is a common issue and there are a few things that you can try to get your computer back up and running.\\n\\nFirst, you'll want to check the power cords and make sure they are plugged in correctly. You might also want to try a different power outlet to see if that makes a difference.\\n\\nIf that doesn't work, you may need to reset your computer. To do this, turn off your computer, unplug the power cord, and remove the battery (if applicable). Then press the power button to drain any remaining power. Wait a few minutes and then plug the power cord back in and turn on the computer.\\n\\nIf the above steps don't work, then you may need to take your computer to a professional for help. They'll be able to diagnose the issue and get your computer up and running again.\\n\\nI hope this helps!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm.predict(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',\\n\\nI am using a FMCOMMS2/3 board. I want to use the board as a transmitter. I have connected the RF DACs and the splitters and have the board set up with the ADI reference HDL design. I have also designed my own HDL design.\\n\\nMy question is, how do I configure the FMCOMMS2/3 board to transmit? What are the steps that I need to take in order to get the FMCOMMS2/3 board to transmit?\\n\\nThanks!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "chat_model.predict(\"Hi\")\n",
    "\n",
    "# or\n",
    "\n",
    "chat_model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding diff between chatmodel output and llm model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model =  ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are numerous hobbies that can help develop creativity. Here are a few suggestions:\\n\\n1. Painting or drawing: Engaging in visual arts can unleash your creativity and allow you to explore different techniques, colors, and styles.\\n\\n2. Writing or journaling: Expressing your thoughts, feelings, or ideas through writing can enhance your creativity and help you develop a unique voice.\\n\\n3. Photography: Capturing images and experimenting with composition, lighting, and editing can stimulate your creativity and train your eye for detail.\\n\\n4. Playing a musical instrument: Learning to play an instrument or composing music can tap into your creative side and provide an outlet for self-expression.\\n\\n5. Crafting or DIY projects: Engaging in activities like knitting, woodworking, or creating handmade crafts allows you to experiment with various materials and techniques, fostering creativity.\\n\\n6. Cooking or baking: Exploring new recipes, experimenting with flavors, and presenting dishes in creative ways can be a fun and delicious way to express your creativity.\\n\\n7. Gardening or landscaping: Designing and cultivating a garden, experimenting with different plant combinations, and creating unique outdoor spaces can inspire creativity and connect you with nature.\\n\\nRemember, the key is to choose a hobby that genuinely interests you and allows you to explore your creativity in a way that brings you joy and satisfaction.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "text = \"What is a good hobby to develop to be more creative?\"\n",
    "\n",
    "messages = [HumanMessage(content=text)]\n",
    "\n",
    "llm.invoke(text)\n",
    "\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Template basics\n",
    "\n",
    "- advatange of abstracting partial variables that go into a prompt string which makes this format a better option with respect to just a regular string.\n",
    "\n",
    "See: https://python.langchain.com/docs/modules/model_io/prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good hobby to develop to be more creative?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good hobby to develop to be more {activity}?\")\n",
    "\n",
    "prompt.format(activity=\"creative\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that will translate English to Portuguese.'),\n",
       " HumanMessage(content=\"Hi! I am learning the 'langchain' framework\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_prompt = \"You are a helpful assistant that will translate {input_lang} to {output_lang}.\"\n",
    "human_prompt = \"{input_text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template_prompt), \n",
    "    (\"human\", human_prompt)\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_lang=\"English\", output_lang=\"Portuguese\", input_text=\"Hi! I am learning the 'langchain' framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Output Parser](https://python.langchain.com/docs/get_started/quickstart#:~:text=for%20more%20detail.-,output%20parsers)\n",
    "\n",
    "\n",
    "3 Types\n",
    "- LLM text into structured informatiob (e.g JSON)\n",
    "- ChatMessage into string\n",
    "- Extra info (e.g. OpenAI function invocation) into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', ' love', ' learning', ' new', ' and', ' cool', ' frameworks']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "    \n",
    "CommaSeparatedListOutputParser().parse(\"I, love, learning, new, and, cool, frameworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to the LCEL language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing with [LCEL](https://python.langchain.com/docs/expression_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's talk chains!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'orange']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Old standard\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El país en el que nació la ciudad de Honolulu, Hawái, donde nació Barack Obama, el 44º presidente de los Estados Unidos, es Estados Unidos.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runable interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableMap, RunnablePassthrough\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\n",
    "    \"generate a {attribute} color. Return the name of the color and nothing else:\"\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what is a fruit of color: {color}. Return the name of the fruit and nothing else:\"\n",
    ")\n",
    "prompt3 = ChatPromptTemplate.from_template(\n",
    "    \"what is a country with a flag that has the color: {color}. Return the name of the country and nothing else:\"\n",
    ")\n",
    "prompt4 = ChatPromptTemplate.from_template(\n",
    "    \"What is the color of {fruit} and the flag of {country}?\"\n",
    ")\n",
    "\n",
    "model_parser = model | StrOutputParser()\n",
    "\n",
    "color_generator = (\n",
    "    {\"attribute\": RunnablePassthrough()} | prompt1 | {\"color\": model_parser}\n",
    ")\n",
    "color_to_fruit = prompt2 | model_parser\n",
    "color_to_country = prompt3 | model_parser\n",
    "question_generator = (\n",
    "    color_generator | {\"fruit\": color_to_fruit, \"country\": color_to_country} | prompt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='What is the color of There is no fruit specifically named \"Coral.\" and the flag of Comoros?')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_generator.invoke(\"warm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The color of coral is a pinkish-orange hue, resembling the color of the marine organism known as coral. \\n\\nThe flag of Comoros consists of four horizontal stripes. From top to bottom, the stripes are yellow, white, red, and blue. In the upper hoist corner, there is a green isosceles triangle with a white crescent and four white stars inside it.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = question_generator.invoke(\"warm\")\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Generate a final response given the critique\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While Scrum has many benefits, it is important to acknowledge the potential downsides and limitations of the framework. The complexity and learning curve associated with Scrum can be challenging, especially for teams new to agile methodologies. It requires a significant commitment of time and effort, as regular meetings and collaboration are integral to the process. Additionally, the flexible and iterative nature of Scrum may make it difficult to predict and plan for project timelines and deliverables, and it may require a skilled Scrum Master to ensure successful implementation. \\n\\nFurthermore, the emphasis on teamwork and collaboration may present challenges if team members do not work well together or have conflicting interests. The lack of comprehensive documentation and potential for scope creep can also be concerns for certain projects. Additionally, the cultural shift required to implement Scrum and the potential resistance to change from team members accustomed to traditional project management approaches can present significant challenges. \\n\\nIt is important to carefully evaluate the specific needs and constraints of a project and organization before deciding to adopt Scrum. While Scrum may not be suitable for all projects, its benefits in promoting collaboration, adaptability, and productivity within teams still make it a valuable framework to consider. It is essential to address the potential cons by providing proper training, support, and ensuring a strong understanding of Scrum principles and practices.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"scrum\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deployment with LangServe](https://python.langchain.com/docs/get_started/quickstart#:~:text=LangSmith%20head%20here.-,serving%20with%20langserve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying with langserve involves:\n",
    "\n",
    "1. Define the chain \n",
    "2. Define a fastapi app\n",
    "3. Define a route from which to serve the chain `langserve.add_routes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "from typing import List\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import BaseOutputParser\n",
    "from langserve import add_routes\n",
    "\n",
    "# 1. Chain definition\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "\n",
    "# 2. App definition\n",
    "app = FastAPI(\n",
    "  title=\"LangChain Server\",\n",
    "  version=\"1.0\",\n",
    "  description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "# 3. Adding chain route\n",
    "add_routes(\n",
    "    app,\n",
    "    category_chain,\n",
    "    path=\"/category_chain\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Client](https://python.langchain.com/docs/get_started/quickstart#:~:text=try%20it%20out!-,client)\n",
    "\n",
    "Now let's set up a client for programmatically interacting with our service. We can easily do this with the langserve.RemoteRunnable. Using this, we can interact with the served chain as if it were running client-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/category_chain/\")\n",
    "remote_chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
