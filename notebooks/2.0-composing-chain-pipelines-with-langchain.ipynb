{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Chains in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chain in LangChain is about putting stuff together. In the context of working with LLMs what is that stuff?\n",
    "\n",
    "Well, that can be as simple as connecting a prompt with an LLM model.\n",
    "\n",
    "## Model\n",
    "\n",
    "To do that we need abstractions for each of these elements (the prompt and the LLM), so let's see how LangChain handles that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm.predict(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the Panda say when he saw the Python code?\\nA: \"Wow, that\\'s a lot of Bamboozles!\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Tell me a joke about a Panda and the Python programming language\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! These are completion examples, let's look at interacting with the chat models (like ChatGPT and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "print(chat_model.predict(\"Hi\"))\n",
    "# or\n",
    "chat_model.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding diff between chatmodel output and llm model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "chat_model =  ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "- advatange of abstracting partial variables that go into a prompt string which makes this format a better option with respect to just a regular string.\n",
    "\n",
    "See: https://python.langchain.com/docs/modules/model_io/prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good hobby to develop to be more creative?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good hobby to develop to be more {activity}?\")\n",
    "\n",
    "prompt.format(activity=\"creative\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that will translate English to Portuguese.'),\n",
       " HumanMessage(content=\"Hi! I am learning the 'langchain' framework\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_prompt = \"You are a helpful assistant that will translate {input_lang} to {output_lang}.\"\n",
    "human_prompt = \"{input_text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template_prompt), \n",
    "    (\"human\", human_prompt)\n",
    "])\n",
    "\n",
    "chat_prompt.format_messages(input_lang=\"English\", output_lang=\"Portuguese\", input_text=\"Hi! I am learning the 'langchain' framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "[source](https://python.langchain.com/docs/get_started/quickstart#:~:text=for%20more%20detail.-,output%20parsers) \n",
    "\n",
    "3 Types\n",
    "\n",
    "- LLM text into structured information (e.g JSON)\n",
    "- ChatMessage into string\n",
    "- Extra info (e.g. OpenAI function invocation) into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', ' love', ' learning', ' new', ' and', ' cool', ' frameworks']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        return text.strip().split(\",\")\n",
    "    \n",
    "    \n",
    "CommaSeparatedListOutputParser().parse(\"I, love, learning, new, and, cool, frameworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains - Putting Everything Together\n",
    "\n",
    "Ok, now that we have these basic elements. Let's put them together into a [chain](https://python.langchain.com/docs/get_started/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing with [LCEL](https://python.langchain.com/docs/expression_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'purple']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists.\n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.\n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()\n",
    "chain.invoke({\"text\": \"colors\"})\n",
    "# >> ['red', 'blue', 'green', 'yellow', 'orange']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El país en el que nació la ciudad de Honolulu, Hawái, donde nació Barack Obama, el 44º presidente de los Estados Unidos, es Estados Unidos.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"what is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain2.invoke({\"person\": \"obama\", \"language\": \"spanish\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from these examples, a chain in Langchain is nothing more than just a building block for putting things together.\n",
    "\n",
    "This capability allows you build extremely interesting and complex functionality out of simple objects like prompt templates and output parsers.\n",
    "\n",
    "\n",
    "However, for that to work well there is a need for some standard interface to allow this process of composition to run seamlessly. That's where [LangChain's LCEL language](https://python.langchain.com/docs/get_started/introduction#:~:text=JavaScript%20LangChain%20library.-,langchain%20expression%20language%20(lcel)) comes into play. We'll look at that in the next notebook lesson.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
