{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs for the below cells.\n",
    "\n",
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install langchain-openai\n",
    "# !pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "# make sure you have python-dotenv installed\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions \n",
    "\n",
    "Allow you to format the output of an llm to input to a function. Meaning allows you to call functions from outputs of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a traditional [openai function for function calling](https://platform.openai.com/docs/guides/function-calling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bind this function to a model using the `.bind` method from the ChatOpenAI object (or any `ChatModel` object in langchain for that matter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "     (\"human\", \"{input}\")   \n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can create our runnable with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the same as saying chain = prompt | model\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"LIS\"}', 'name': 'weather_search'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 66, 'total_tokens': 82}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'function_call', 'logprobs': None}, id='run-897a4a9a-c904-4b90-afad-f5073b6b32d5-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"What's the weather like in Lisbon?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the `functions` variable and add another function.\n",
    "\n",
    "Let's say search for relevant resources given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"resources_search\",\n",
    "      \"description\": \"Search for relevant resources given a query\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query describing the subject of the resources\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take that same model and bind these functions to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"langchain Python library\"}', 'name': 'resources_search'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 105, 'total_tokens': 122}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_c2295e73ad', 'finish_reason': 'function_call', 'logprobs': None}, id='run-81ffcbce-5804-4a4d-be62-b8022c75bc24-0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"What are the most useful resources of the langchain Python library?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted the model to put a json object rather than some string or some object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': ['Buddy', 'Luna', 'Max', 'Bailey', 'Coco']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import json\n",
    "\n",
    "llm_chat = ChatOpenAI(temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "simple_chain = llm_chat | output_parser | json.loads\n",
    "\n",
    "simple_chain.invoke(\"What are fun five names for dogs?The output should be a json object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we add a list of other runnables. \n",
    "\n",
    "If there is an error it will go through the list of fallback chains (meaning, other ways of processing the input in a useful way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = simple_chain.with_fallbacks([simple_chain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add a prompt template to this, and explore the `.batch` option of a runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Find the derivative of the function f(x) = 3x^2 + 5x - 2 using the power rule and the constant multiple rule.\\n2. Find the derivative of the function g(x) = sin(x) + cos(x) using the sum rule and the derivative of trigonometric functions.\\n3. Find the derivative of the function h(x) = ln(x) + e^x using the derivative of natural logarithm and exponential functions.',\n",
       " '1. Simple linear regression practice: Start by generating a dataset with two variables, one independent and one dependent. Use a software tool like Python or R to perform simple linear regression analysis on the dataset and interpret the results.\\n\\n2. Multiple linear regression practice: Create a dataset with multiple independent variables and one dependent variable. Use a regression tool to perform multiple linear regression analysis and assess the impact of each independent variable on the dependent variable.\\n\\n3. Residual analysis practice: Generate a dataset with a linear relationship between the independent and dependent variables, but with some noise added. Perform linear regression analysis and then analyze the residuals to check for any patterns or trends that may indicate issues with the model.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"Create a list of 3 exercises to drill this concept: {concept}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm_chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm_chat | output_parser\n",
    "\n",
    "chain.batch([{\"concept\": \"derivatives\"}, {\"concept\": \"linear regression\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
