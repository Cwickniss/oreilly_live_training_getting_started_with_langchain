{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs for the below cells.\n",
    "\n",
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "# make sure you have python-dotenv installed\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Functions \n",
    "\n",
    "Allow you to format the output of an llm to input to a function. Meaning allows you to call functions from outputs of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a traditional [openai function for function calling](https://platform.openai.com/docs/guides/function-calling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can bind this function to a model using the `.bind` method from the ChatOpenAI object (or any `ChatModel` object in langchain for that matter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "     (\"human\", \"{input}\")   \n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can create our runnable with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the same as saying chain = prompt | model\n",
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"LIS\"}', 'name': 'weather_search'}}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 66, 'total_tokens': 82}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-89d8a391-61e6-4c3d-bcb9-59c09286ebf5-0', usage_metadata={'input_tokens': 66, 'output_tokens': 16, 'total_tokens': 82})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"What's the weather like in Lisbon?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update the `functions` variable and add another function.\n",
    "\n",
    "Let's say search for relevant resources given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"resources_search\",\n",
    "      \"description\": \"Search for relevant resources given a query\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query describing the subject of the resources\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "      }\n",
    "    }\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take that same model and bind these functions to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"langchain Python library\"}', 'name': 'resources_search'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 105, 'total_tokens': 122}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run-dfb825f8-8b4e-4590-a449-c2fe1d7687e1-0', usage_metadata={'input_tokens': 105, 'output_tokens': 17, 'total_tokens': 122})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable.invoke({\"input\": \"What are the most useful resources of the langchain Python library?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted the model to put a json object rather than some string or some object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': ['Buddy', 'Luna', 'Max', 'Bailey', 'Coco']}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import json\n",
    "\n",
    "llm_chat = ChatOpenAI(temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "simple_chain = llm_chat | output_parser | json.loads\n",
    "\n",
    "simple_chain.invoke(\"What are fun five names for dogs?The output should be a json object.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we add a list of other runnables. \n",
    "\n",
    "If there is an error it will go through the list of fallback chains (meaning, other ways of processing the input in a useful way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = simple_chain.with_fallbacks([simple_chain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add a prompt template to this, and explore the `.batch` option of a runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Find the derivative of the function f(x) = 3x^2 + 5x - 2 using the power rule and the constant multiple rule.\\n2. Find the derivative of the function g(x) = sin(x) + cos(x) using the sum rule and the derivative of trigonometric functions.\\n3. Find the derivative of the function h(x) = e^x * ln(x) using the product rule and the derivative of exponential and logarithmic functions.',\n",
       " \"1. Simple linear regression practice: Start by generating a set of data points and fitting a simple linear regression model to it. Use a tool like Python's scikit-learn library to perform the regression analysis and visualize the results.\\n\\n2. Multiple linear regression practice: Extend the previous exercise by adding more independent variables to the dataset and fitting a multiple linear regression model. This will help you understand how to handle multiple predictors in a regression analysis.\\n\\n3. Residual analysis practice: After fitting a linear regression model, analyze the residuals to check for any patterns or trends. Plot the residuals against the predicted values to see if there are any systematic errors in the model. This exercise will help you understand the assumptions of linear regression and how to interpret the results.\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"Create a list of 3 exercises to drill this concept: {concept}\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm_chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm_chat | output_parser\n",
    "\n",
    "chain.batch([{\"concept\": \"derivatives\"}, {\"concept\": \"linear regression\"}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
