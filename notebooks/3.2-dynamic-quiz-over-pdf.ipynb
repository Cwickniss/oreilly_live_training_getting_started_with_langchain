{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install pypdf\n",
    "!pip install chromadb\n",
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# make sure you have python-dotenv installed\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a study workflow using Jupyter Notebooks, LLMs, and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x12d1be110>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12d83fe50>, model_name='gpt-4-1106-preview', openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x12d745250>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./assets-resources/attention-paper.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "retriever = vectordb.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the key components of the transformer architecture?\"\n",
    "result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key components of the Transformer architecture are as follows:\n",
      "\n",
      "1. **Encoder and Decoder Stacks**: The Transformer model consists of an encoder and a decoder, each made up of N=6 identical layers. \n",
      "\n",
      "2. **Self-Attention Mechanism**: This allows the model to weigh the influence of different parts of the input data differently and is a key component in both the encoder and decoder layers.\n",
      "\n",
      "3. **Multi-Head Attention**: To capture different aspects of information from the input sequence, the Transformer uses multiple attention heads, allowing the model to focus on different positions of the input sequence simultaneously.\n",
      "\n",
      "4. **Position-Wise Feed-Forward Networks**: Each layer in the encoder and decoder contains a fully connected feed-forward network that is applied to each position separately and identically.\n",
      "\n",
      "5. **Positional Encoding**: Since the model lacks recurrence and convolution, it uses positional encodings to give the model some information about the order of the sequence. This can be implemented using fixed sinusoidal encodings or learned embeddings.\n",
      "\n",
      "6. **Residual Connections**: Each sub-layer in the encoder and decoder, including self-attention and feed-forward networks, have residual connections around them, followed by layer normalization. This helps in mitigating the vanishing gradient problem and allows the model to learn deeper representations without degradation.\n",
      "\n",
      "7. **Layer Normalization**: Each sub-layer output goes through normalization where the mean and variance are calculated per layer. This is done before adding the residual connection and helps stabilize training.\n",
      "\n",
      "8. **Output Linear Layer and Softmax**: In the decoder, after the final layer, the output is transformed by a linear layer followed by a softmax function to produce the probability distribution of the next token.\n",
      "\n",
      "9. **Masking**: In the decoder, masking is used to prevent positions from attending to subsequent positions during training. This ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n",
      "\n",
      "These components work together to allow the Transformer to model complex dependencies and handle sequence transduction tasks such as machine translation efficiently and effectively.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\n",
      "ANSWER The self-attention mechanism in transformers differs from traditional sequence alignment methods in several key ways:\n",
      "\n",
      "1. **Global Dependencies**: Self-attention allows the model to consider the entire sequence simultaneously, enabling each position to attend to all positions in the previous layers of the network. This contrasts with traditional sequence alignment methods in RNNs, where the hidden state at each position is computed based on the previous hidden state and the current input, making it harder to capture long-range dependencies.\n",
      "\n",
      "2. **Parallelization**: The self-attention mechanism enables parallel processing of the sequence data, unlike RNNs which process data sequentially. This parallelization significantly speeds up training because it eliminates the need to wait for the computation of the previous state before processing the current state.\n",
      "\n",
      "3. **Fixed Number of Operations**: The number of operations required to relate signals from two arbitrary input or output positions is constant in transformers, regardless of the distance between the positions. In contrast, with traditional methods like ConvS2S, the number of operations grows with the distance between positions.\n",
      "\n",
      "4. **Positional Encodings**: Since self-attention does not inherently consider the order of the sequence, transformers use positional encodings to inject information about the position of the tokens in the sequence. Traditional methods like RNNs and CNNs inherently consider the order due to their sequential processing nature.\n",
      "\n",
      "5. **Direct Dependency Modeling**: Self-attention directly models dependencies between all pairs of input positions, whereas traditional sequence alignment methods like RNNs may have difficulties dealing with long-range dependencies due to issues like vanishing gradients.\n",
      "\n",
      "6. **Simplicity of the Model**: Transformers eschew the complex gating mechanisms of LSTMs and GRUs and the convolutional layers of CNN-based models, relying entirely on the self-attention mechanism to compute representations of its input and output.\n",
      "\n",
      "7. **Efficiency at Test Time**: At inference time, transformers can be more efficient since they can process the entire input sequence at once, whereas RNNs must process each element sequentially.\n",
      "\n",
      "8. **Weighted Relevance**: Self-attention computes a weighted sum of all values based on the compatibility (scored by the attention function) of the query and keys, allowing the model to focus on relevant parts of the input for each output part. This differs from traditional alignment methods that might not have such an explicit and learnable mechanism to weigh the input's relevance.\n",
      "\n",
      "In summary, the self-attention mechanism in transformers provides a more flexible and parallelizable way to handle sequence data, which can lead to performance improvements in tasks that require modeling of complex dependencies.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The self-attention mechanism in transformers differs from traditional sequence alignment methods in several key ways:\\n\\n1. **Global Dependencies**: Self-attention allows the model to consider the entire sequence simultaneously, enabling each position to attend to all positions in the previous layers of the network. This contrasts with traditional sequence alignment methods in RNNs, where the hidden state at each position is computed based on the previous hidden state and the current input, making it harder to capture long-range dependencies.\\n\\n2. **Parallelization**: The self-attention mechanism enables parallel processing of the sequence data, unlike RNNs which process data sequentially. This parallelization significantly speeds up training because it eliminates the need to wait for the computation of the previous state before processing the current state.\\n\\n3. **Fixed Number of Operations**: The number of operations required to relate signals from two arbitrary input or output positions is constant in transformers, regardless of the distance between the positions. In contrast, with traditional methods like ConvS2S, the number of operations grows with the distance between positions.\\n\\n4. **Positional Encodings**: Since self-attention does not inherently consider the order of the sequence, transformers use positional encodings to inject information about the position of the tokens in the sequence. Traditional methods like RNNs and CNNs inherently consider the order due to their sequential processing nature.\\n\\n5. **Direct Dependency Modeling**: Self-attention directly models dependencies between all pairs of input positions, whereas traditional sequence alignment methods like RNNs may have difficulties dealing with long-range dependencies due to issues like vanishing gradients.\\n\\n6. **Simplicity of the Model**: Transformers eschew the complex gating mechanisms of LSTMs and GRUs and the convolutional layers of CNN-based models, relying entirely on the self-attention mechanism to compute representations of its input and output.\\n\\n7. **Efficiency at Test Time**: At inference time, transformers can be more efficient since they can process the entire input sequence at once, whereas RNNs must process each element sequentially.\\n\\n8. **Weighted Relevance**: Self-attention computes a weighted sum of all values based on the compatibility (scored by the attention function) of the query and keys, allowing the model to focus on relevant parts of the input for each output part. This differs from traditional alignment methods that might not have such an explicit and learnable mechanism to weigh the input's relevance.\\n\\nIn summary, the self-attention mechanism in transformers provides a more flexible and parallelizable way to handle sequence data, which can lead to performance improvements in tasks that require modeling of complex dependencies.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_pdf(pdf_qa,query):\n",
    "    print(\"QUERY: \",query)\n",
    "    result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})\n",
    "    answer = result[\"result\"]\n",
    "    print(\"ANSWER\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "ask_pdf(pdf_qa,\"How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\n",
      "ANSWER Sure, here are three simple questions about positional encodings in the context of the Transformer model:\n",
      "\n",
      "1. What is the purpose of positional encodings in the Transformer model architecture?\n",
      "   - A) To add information about the absolute or relative position of the tokens in the sequence.\n",
      "   - B) To replace the attention mechanism.\n",
      "   - C) To increase the model size.\n",
      "\n",
      "2. How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks?\n",
      "   - A) By concatenating the positional encoding vector to the input embedding vector.\n",
      "   - B) By adding the positional encoding vector to the input embedding vector.\n",
      "   - C) By multiplying the input embedding vector with the positional encoding vector.\n",
      "\n",
      "3. What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative?\n",
      "   - A) Learned positional embeddings; it led to significantly better performance.\n",
      "   - B) Learned positional embeddings; it observed nearly identical results to the base model.\n",
      "   - C) Random positional embeddings; it resulted in worse performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, here are three simple questions about positional encodings in the context of the Transformer model:\\n\\n1. What is the purpose of positional encodings in the Transformer model architecture?\\n   - A) To add information about the absolute or relative position of the tokens in the sequence.\\n   - B) To replace the attention mechanism.\\n   - C) To increase the model size.\\n\\n2. How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks?\\n   - A) By concatenating the positional encoding vector to the input embedding vector.\\n   - B) By adding the positional encoding vector to the input embedding vector.\\n   - C) By multiplying the input embedding vector with the positional encoding vector.\\n\\n3. What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative?\\n   - A) Learned positional embeddings; it led to significantly better performance.\\n   - B) Learned positional embeddings; it observed nearly identical results to the base model.\\n   - C) Random positional embeddings; it resulted in worse performance.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_questions = ask_pdf(pdf_qa, \"Quiz me with 3 simple questions on the positional encodings and the role they play in transformers.\")\n",
    "\n",
    "quiz_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = f\"You take in text and spit out Python code doing what the user wants\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Return ONLY a PYTHON list containing the questions in this text: {questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```python\\nquestions = [\\n    \"What is the purpose of positional encodings in the Transformer model architecture?\",\\n    \"How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks?\",\\n    \"What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative?\"\\n]\\n```', response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 266, 'total_tokens': 348}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7dab1b6f-fe80-4946-8489-246019018e66-0', usage_metadata={'input_tokens': 266, 'output_tokens': 82, 'total_tokens': 348})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_code(markdown_string):\n",
    "    pattern = r'```python\\n(.*?)\\n```'\n",
    "    matches = re.findall(pattern, markdown_string, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        python_code = matches[0]\n",
    "        return python_code\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "quiz_chain = chat_prompt | llm | RunnableLambda(lambda x: x.content) | extract_python_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: We haven't discussed runnable at length, but essentially they make up the core of the LCEL interface. \n",
    "\n",
    "`RunnableLambda` allows you to take in an output from part of the chain and pass it along after performing some transformation defined withint its lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'questions = [\\n    \"What is the purpose of positional encodings in the Transformer model architecture?\",\\n    \"How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks?\",\\n    \"What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative?\"\\n]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of positional encodings in the Transformer model architecture?',\n",
       " 'How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks?',\n",
       " 'What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is the purpose of positional encodings in the Transformer model architecture?\n",
      "ANSWER Positional encodings in the Transformer model architecture serve the purpose of giving the model information about the order of the words in the sequence. Since the Transformer relies solely on attention mechanisms and does not use any recurrence (like RNNs) or convolution, it does not inherently consider the position of words in the sequence. Positional encodings are added to the input embeddings to provide the model with some notion of word order, which is crucial for many language-related tasks where the order of words affects the meaning.\n",
      "\n",
      "The positional encodings can take various forms, but in the original \"Attention Is All You Need\" paper by Vaswani et al., they used sine and cosine functions of different frequencies to generate these positional encodings. These are designed so that each position would have a unique encoding and the relative positions could be easily interpreted by the model. The Transformer model can, therefore, learn to use these signals to understand word order and its implications for the meaning of the sentence.\n",
      "QUERY:  How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks?\n",
      "ANSWER In the Transformer model, positional encodings are combined with the input embeddings to provide a notion of the order of the sequence since the self-attention mechanism itself does not have any means to distinguish the positional order of tokens. Positional encodings are added to the input embeddings before the sequence is processed by the encoder and decoder stacks.\n",
      "\n",
      "The positional encodings have the same dimension, d_model, as the embeddings to allow for this combination. The sum of the positional encoding and the token embedding is then fed into the encoder or decoder stacks. This allows each position in the input sequence to have a distinct representation that reflects its position in the sequence, which is crucial for the model to understand the order-dependent information within the sequence.\n",
      "\n",
      "The positional encoding can be defined using sine and cosine functions of different frequencies, as outlined in the original Transformer paper, but other methods of positional encoding can also be used. The key is that each position's encoding must be unique and should provide the model with an effective way to incorporate the order of the tokens into its calculations.\n",
      "\n",
      "The formula for positional encoding provided in the original Transformer paper uses sine and cosine functions with different frequencies for each dimension of the positional encoding vector:\n",
      "\n",
      "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000⋅2π. This choice of positional encoding allows the model to easily learn to attend by relative positions since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos).\n",
      "QUERY:  What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative?\n",
      "ANSWER The alternative to the original sinusoidal positional encodings mentioned in the context provided was the use of learned positional embeddings. The observed effect on model performance when using this alternative was nearly identical results to the base model, as mentioned in row (E) of Table 3 in the provided context.\n"
     ]
    }
   ],
   "source": [
    "# the questions variable was created within the string inside the `questions_list` variable.\n",
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append(ask_pdf(pdf_qa,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Is this: Positional encodings in the Transformer model architecture serve the purpose of giving the model information about the order of the words in the sequence. Since the Transformer relies solely on attention mechanisms and does not use any recurrence (like RNNs) or convolution, it does not inherently consider the position of words in the sequence. Positional encodings are added to the input embeddings to provide the model with some notion of word order, which is crucial for many language-related tasks where the order of words affects the meaning.\n",
      "\n",
      "The positional encodings can take various forms, but in the original \"Attention Is All You Need\" paper by Vaswani et al., they used sine and cosine functions of different frequencies to generate these positional encodings. These are designed so that each position would have a unique encoding and the relative positions could be easily interpreted by the model. The Transformer model can, therefore, learn to use these signals to understand word order and its implications for the meaning of the sentence. the correct answer to this question: What is the purpose of positional encodings in the Transformer model architecture? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: In the Transformer model, positional encodings are combined with the input embeddings to provide a notion of the order of the sequence since the self-attention mechanism itself does not have any means to distinguish the positional order of tokens. Positional encodings are added to the input embeddings before the sequence is processed by the encoder and decoder stacks.\n",
      "\n",
      "The positional encodings have the same dimension, d_model, as the embeddings to allow for this combination. The sum of the positional encoding and the token embedding is then fed into the encoder or decoder stacks. This allows each position in the input sequence to have a distinct representation that reflects its position in the sequence, which is crucial for the model to understand the order-dependent information within the sequence.\n",
      "\n",
      "The positional encoding can be defined using sine and cosine functions of different frequencies, as outlined in the original Transformer paper, but other methods of positional encoding can also be used. The key is that each position's encoding must be unique and should provide the model with an effective way to incorporate the order of the tokens into its calculations.\n",
      "\n",
      "The formula for positional encoding provided in the original Transformer paper uses sine and cosine functions with different frequencies for each dimension of the positional encoding vector:\n",
      "\n",
      "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "\n",
      "where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000⋅2π. This choice of positional encoding allows the model to easily learn to attend by relative positions since for any fixed offset k, PE(pos+k) can be represented as a linear function of PE(pos). the correct answer to this question: How are positional encodings combined with the input embeddings in the Transformer model before processing by the encoder and decoder stacks? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The alternative to the original sinusoidal positional encodings mentioned in the context provided was the use of learned positional embeddings. The observed effect on model performance when using this alternative was nearly identical results to the base model, as mentioned in row (E) of Table 3 in the provided context. the correct answer to this question: What alternative to the original sinusoidal positional encodings was mentioned in the context provided, and what was the observed effect on model performance when using this alternative? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES', 'YES', 'YES']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    # Check for results\n",
    "    evaluations.append(ask_pdf(pdf_qa,f\"Is this: {a} the correct answer to this question: {q} according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\"))\n",
    "\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "scores.append(yes_count)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
