{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install langchain-openai\n",
    "# !pip install pypdf\n",
    "# !pip install chromadb\n",
    "# !pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# make sure you have python-dotenv installed\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a study workflow using Jupyter Notebooks, LLMs, and langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(combine_documents_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x15343d890>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1536dbe50>, model_name='gpt-4-1106-preview', openai_api_key=SecretStr('**********'), openai_api_base='https://api.openai.com/v1', openai_proxy='')), document_prompt=PromptTemplate(input_variables=['page_content'], template='Context:\\n{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x1529a9a50>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"./assets-resources/attention-paper.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_path) # LOAD\n",
    "pdf_docs = loader.load_and_split() # SPLIT\n",
    "embeddings = OpenAIEmbeddings() # EMBED\n",
    "vectordb = Chroma.from_documents(pdf_docs, embedding=embeddings) # STORE\n",
    "retriever = vectordb.as_retriever()\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "pdf_qa = RetrievalQA.from_llm(llm=llm, retriever=retriever) # RETRIEVE\n",
    "pdf_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the key components of the transformer architecture?\"\n",
    "result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture, as described in the provided context, consists of several key components:\n",
      "\n",
      "1. **Encoder and Decoder Stacks**: Both the encoder and decoder are composed of a stack of six identical layers. \n",
      "\n",
      "   - **Encoder**: Each encoder layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a position-wise fully connected feed-forward network. Each of these sub-layers is followed by a residual connection and layer normalization. \n",
      "\n",
      "   - **Decoder**: Each decoder layer also has two sub-layers similar to the encoder, plus a third sub-layer that performs multi-head attention over the encoder's output. The self-attention mechanism in the decoder is modified to prevent positions from attending to subsequent positions (also known as masked self-attention).\n",
      "\n",
      "2. **Attention**: The attention function in the Transformer is a mapping of a query and a set of key-value pairs to an output, with all components being vectors. The output is computed as a weighted sum. This mechanism allows the model to focus on different parts of the input sequence when producing the output, facilitating the modeling of dependencies without regard to their distance in the sequence.\n",
      "\n",
      "3. **Multi-Head Attention**: This is a technique where the attention mechanism is run in parallel multiple times with different, learned linear projections of the queries, keys, and values. This allows the model to capture information from different representation subspaces at different positions.\n",
      "\n",
      "4. **Position-wise Feed-Forward Networks**: In addition to attention sub-layers, each layer in the encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\n",
      "\n",
      "5. **Residual Connections**: These are used around each of the sub-layers (self-attention and feed-forward networks) in both the encoder and decoder. They help in avoiding the vanishing gradient problem by allowing gradients to flow through the networks directly.\n",
      "\n",
      "6. **Layer Normalization**: This component is applied after each residual connection, helping in stabilizing the learning process.\n",
      "\n",
      "7. **Positional Encoding**: Since the Transformer does not use recurrence or convolution, positional encodings are added to the input embeddings to give the model information about the position of the tokens in the sequence. The Transformer uses sinusoidal functions to encode the positions, although learned positional embeddings are also an option as they have shown nearly identical results.\n",
      "\n",
      "These components work together to enable the Transformer to handle sequence transduction tasks such as translation and parsing effectively, with the advantage of allowing for more parallelization in computation compared to recurrent or convolutional models.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\n",
      "ANSWER The self-attention mechanism in transformers differs from traditional sequence alignment methods in the following ways:\n",
      "\n",
      "1. Global Dependency Modeling: Self-attention allows the model to directly compute dependencies between any two positions in the sequence, regardless of their distance. Traditional sequence alignment methods like those in RNNs and CNNs process the sequence step-by-step or in local receptive fields, which can make it harder to capture long-range dependencies.\n",
      "\n",
      "2. Parallelization: Self-attention mechanisms enable parallel computation across all positions in a sequence because they do not require sequential processing. This is in contrast to RNNs, which process elements sequentially and therefore cannot be parallelized across the steps of a sequence.\n",
      "\n",
      "3. Fixed Number of Operations: The Transformer reduces the number of operations required to relate two arbitrary positions in a sequence to a constant, while in RNNs and CNNs, this number grows with the distance between positions (linearly for CNNs like ConvS2S, and potentially unbounded for RNNs).\n",
      "\n",
      "4. Flexibility in Modeling Relationships: Self-attention can flexibly weigh the influence of different positions, whereas traditional models might have fixed patterns of interaction defined by their structure (e.g., convolutional filters or recurrent state transitions).\n",
      "\n",
      "5. Absence of Recurrent or Convolutional Layers: The Transformer model architecture, which uses self-attention, eschews recurrence and convolutions entirely, whereas traditional sequence alignment methods typically rely on these mechanisms to process sequences.\n",
      "\n",
      "6. Positional Encoding: Since self-attention does not inherently consider the order of the sequence, the Transformer uses positional encodings to inject information about the position of the elements in the sequence. Traditional methods like RNNs naturally take the order of elements into account due to their sequential processing.\n",
      "\n",
      "Overall, self-attention mechanisms represent a departure from the incremental processing inherent in traditional sequence alignment methods and provide a more direct way to model relationships in data, which can be especially beneficial for tasks like machine translation and language modeling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The self-attention mechanism in transformers differs from traditional sequence alignment methods in the following ways:\\n\\n1. Global Dependency Modeling: Self-attention allows the model to directly compute dependencies between any two positions in the sequence, regardless of their distance. Traditional sequence alignment methods like those in RNNs and CNNs process the sequence step-by-step or in local receptive fields, which can make it harder to capture long-range dependencies.\\n\\n2. Parallelization: Self-attention mechanisms enable parallel computation across all positions in a sequence because they do not require sequential processing. This is in contrast to RNNs, which process elements sequentially and therefore cannot be parallelized across the steps of a sequence.\\n\\n3. Fixed Number of Operations: The Transformer reduces the number of operations required to relate two arbitrary positions in a sequence to a constant, while in RNNs and CNNs, this number grows with the distance between positions (linearly for CNNs like ConvS2S, and potentially unbounded for RNNs).\\n\\n4. Flexibility in Modeling Relationships: Self-attention can flexibly weigh the influence of different positions, whereas traditional models might have fixed patterns of interaction defined by their structure (e.g., convolutional filters or recurrent state transitions).\\n\\n5. Absence of Recurrent or Convolutional Layers: The Transformer model architecture, which uses self-attention, eschews recurrence and convolutions entirely, whereas traditional sequence alignment methods typically rely on these mechanisms to process sequences.\\n\\n6. Positional Encoding: Since self-attention does not inherently consider the order of the sequence, the Transformer uses positional encodings to inject information about the position of the elements in the sequence. Traditional methods like RNNs naturally take the order of elements into account due to their sequential processing.\\n\\nOverall, self-attention mechanisms represent a departure from the incremental processing inherent in traditional sequence alignment methods and provide a more direct way to model relationships in data, which can be especially beneficial for tasks like machine translation and language modeling.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask_pdf(pdf_qa,query):\n",
    "    print(\"QUERY: \",query)\n",
    "    result = pdf_qa.invoke({\"query\": query, \"chat_history\": []})\n",
    "    answer = result[\"result\"]\n",
    "    print(\"ANSWER\", answer)\n",
    "    return answer\n",
    "\n",
    "\n",
    "ask_pdf(pdf_qa,\"How does the self-attention mechanism in transformers differ from traditional sequence alignment methods?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Quiz me on the positional encodings and the role they play in transformers.\n",
      "ANSWER Sure, let's start with a quiz on positional encodings in the context of the Transformer model:\n",
      "\n",
      "1. What is the purpose of positional encodings in the Transformer architecture?\n",
      "   a) To provide a unique identifier for each word in the vocabulary.\n",
      "   b) To allow the model to take into account the order of the words in the sequence.\n",
      "   c) To replace the self-attention mechanism in the model.\n",
      "   d) To increase the computational efficiency of the model.\n",
      "\n",
      "2. How are positional encodings combined with the input embeddings in the Transformer model?\n",
      "   a) By concatenating the positional encoding with the input embedding vector.\n",
      "   b) By adding the positional encoding to the input embedding vector.\n",
      "   c) By multiplying the positional encoding with the input embedding vector.\n",
      "   d) By using the positional encoding as an input to the self-attention mechanism.\n",
      "\n",
      "3. Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.?\n",
      "   a) Positional encodings are learned parameters that are optimized during training.\n",
      "   b) Positional encodings are fixed and based on a sinusoidal function.\n",
      "   c) Positional encodings are only used in the encoder part of the Transformer.\n",
      "   d) Positional encodings are not necessary if the sequences are short.\n",
      "\n",
      "4. How do positional encodings enable the model to determine the position of each word in the sequence?\n",
      "   a) They encode the absolute position of each word as a one-hot vector.\n",
      "   b) They allow the model to learn the position of each word during training.\n",
      "   c) They provide a unique signal for each position that can be added to the representation of the word at that position.\n",
      "   d) They are used to shuffle the words in the sequence to improve generalization.\n",
      "\n",
      "5. In the context of the provided paper, what variation related to positional encodings was experimented with by the authors?\n",
      "   a) They tried using random noise instead of structured encodings.\n",
      "   b) They replaced sinusoidal positional encodings with learned positional embeddings.\n",
      "   c) They completely removed positional encodings to test the model's performance without them.\n",
      "   d) They introduced an additional positional encoding layer at the end of the encoder stack.\n",
      "\n",
      "Please select the most appropriate answer for each of the questions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, let's start with a quiz on positional encodings in the context of the Transformer model:\\n\\n1. What is the purpose of positional encodings in the Transformer architecture?\\n   a) To provide a unique identifier for each word in the vocabulary.\\n   b) To allow the model to take into account the order of the words in the sequence.\\n   c) To replace the self-attention mechanism in the model.\\n   d) To increase the computational efficiency of the model.\\n\\n2. How are positional encodings combined with the input embeddings in the Transformer model?\\n   a) By concatenating the positional encoding with the input embedding vector.\\n   b) By adding the positional encoding to the input embedding vector.\\n   c) By multiplying the positional encoding with the input embedding vector.\\n   d) By using the positional encoding as an input to the self-attention mechanism.\\n\\n3. Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.?\\n   a) Positional encodings are learned parameters that are optimized during training.\\n   b) Positional encodings are fixed and based on a sinusoidal function.\\n   c) Positional encodings are only used in the encoder part of the Transformer.\\n   d) Positional encodings are not necessary if the sequences are short.\\n\\n4. How do positional encodings enable the model to determine the position of each word in the sequence?\\n   a) They encode the absolute position of each word as a one-hot vector.\\n   b) They allow the model to learn the position of each word during training.\\n   c) They provide a unique signal for each position that can be added to the representation of the word at that position.\\n   d) They are used to shuffle the words in the sequence to improve generalization.\\n\\n5. In the context of the provided paper, what variation related to positional encodings was experimented with by the authors?\\n   a) They tried using random noise instead of structured encodings.\\n   b) They replaced sinusoidal positional encodings with learned positional embeddings.\\n   c) They completely removed positional encodings to test the model's performance without them.\\n   d) They introduced an additional positional encoding layer at the end of the encoder stack.\\n\\nPlease select the most appropriate answer for each of the questions.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_questions = ask_pdf(pdf_qa, \"Quiz me on the positional encodings and the role they play in transformers.\")\n",
    "\n",
    "quiz_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template = f\"You take in text and spit out Python code doing what the user wants\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"Return ONLY a PYTHON list containing the questions in this text: {questions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```python\\nquestions = [\\n    \"What is the purpose of positional encodings in the Transformer architecture?\",\\n    \"How are positional encodings combined with the input embeddings in the Transformer model?\",\\n    \"Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.?\",\\n    \"How do positional encodings enable the model to determine the position of each word in the sequence?\",\\n    \"In the context of the provided paper, what variation related to positional encodings was experimented with by the authors?\"\\n]\\n```', response_metadata={'token_usage': {'completion_tokens': 112, 'prompt_tokens': 504, 'total_tokens': 616}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': 'fp_94f711dcf6', 'finish_reason': 'stop', 'logprobs': None}, id='run-f13d7eaa-1516-427c-b154-96468e3f0d6c-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_python_code(markdown_string):\n",
    "    pattern = r'```python\\n(.*?)\\n```'\n",
    "    matches = re.findall(pattern, markdown_string, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        python_code = matches[0]\n",
    "        return python_code\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "quiz_chain = chat_prompt | llm | RunnableLambda(lambda x: x.content) | extract_python_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: We haven't discussed runnable at length, but essentially they make up the core of the LCEL interface. \n",
    "\n",
    "`RunnableLambda` allows you to take in an output from part of the chain and pass it along after performing some transformation defined withint its lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = quiz_chain.invoke({\"questions\": quiz_questions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'questions = [\\n    \"What is the purpose of positional encodings in the Transformer architecture?\",\\n    \"How are positional encodings combined with the input embeddings in the Transformer model?\",\\n    \"Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.?\",\\n    \"How do positional encodings enable the model to determine the position of each word in the sequence?\",\\n    \"In the context of the provided paper, what variation related to positional encodings was experimented with by the authors?\"\\n]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(questions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of positional encodings in the Transformer architecture?',\n",
       " 'How are positional encodings combined with the input embeddings in the Transformer model?',\n",
       " 'Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.?',\n",
       " 'How do positional encodings enable the model to determine the position of each word in the sequence?',\n",
       " 'In the context of the provided paper, what variation related to positional encodings was experimented with by the authors?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is the purpose of positional encodings in the Transformer architecture?\n",
      "ANSWER Positional encodings in the Transformer architecture serve the purpose of providing information about the order of the tokens in the sequence. Since the Transformer model does not use recurrence or convolution, it does not inherently account for the sequential nature of the input data. Without positional encodings, the model would treat the input as a set of tokens without any sense of word order, which is critical for many tasks such as language understanding and translation.\n",
      "\n",
      "The positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. This way, the model can learn and leverage the sequence order - which word came first, which came second, and so forth. The original Transformer paper proposes the use of sinusoidal functions to generate these positional encodings, providing a unique encoding for each position that can be easily extended to sequence lengths unseen during training. However, learned positional embeddings are an alternative approach that was also mentioned to have nearly identical results to the sinusoidal positional encoding in the paper.\n",
      "\n",
      "The use of positional encodings enables the model to consider the position of each word in the sequence when processing language, which is crucial for understanding the meaning of sentences and effectively translating between languages.\n",
      "QUERY:  How are positional encodings combined with the input embeddings in the Transformer model?\n",
      "ANSWER In the Transformer model, positional encodings are combined with the input embeddings by element-wise addition. Each input embedding, which is a vector representing a token in the sequence, is added to a positional encoding vector of the same dimensionality. This operation gives the model information about the position of each token in the sequence, which is important since the model's self-attention mechanism does not have any inherent notion of token order or sequence position.\n",
      "\n",
      "The positional encodings can either be learned or fixed. In the original Transformer paper, the authors used fixed sinusoidal positional encodings with frequencies that follow a geometric progression from 2π to 10000⋅2π. They defined the positional encodings as follows:\n",
      "\n",
      "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "\n",
      "where pos is the position and i is the dimension. Each dimension of the positional encoding corresponds to a sinusoid. The use of sine and cosine functions helps the model to easily learn to attend by relative positions since for any fixed offset k, PE(pos + k) can be represented as a linear function of PE(pos).\n",
      "\n",
      "These positional encodings are added to the input embeddings before the sequence is processed by the encoder and decoder stacks of the Transformer. This allows the model to take into account the order of the sequence, which is crucial for tasks such as machine translation, where the position of a word in a sentence affects its meaning and therefore the translation.\n",
      "QUERY:  Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.?\n",
      "ANSWER The original Transformer model proposed by Vaswani et al. uses sinusoidal positional encodings. This is mentioned in the context provided, where in Table 3, row (E), it is stated that they replaced their sinusoidal positional encoding with learned positional embeddings, and observed nearly identical results to the base model. This indicates that the base model originally used sinusoidal positional encodings.\n",
      "QUERY:  How do positional encodings enable the model to determine the position of each word in the sequence?\n",
      "ANSWER Positional encodings enable the model to determine the position of each word in the sequence by adding information about the absolute or relative position of the tokens to the input embeddings. Since the Transformer model architecture does not inherently capture the order of the sequence (as it lacks recurrence and convolution), positional encodings are necessary to provide the model with the sequence order information.\n",
      "\n",
      "In the context provided, the positional encodings use sine and cosine functions of different frequencies to encode the positions. Each position in the sequence is encoded using a unique combination of sine and cosine values based on its index (pos). The encoding for a particular dimension is calculated as follows:\n",
      "\n",
      "- For even indices (2i), the positional encoding uses a sine function: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
      "- For odd indices (2i+1), it uses a cosine function: PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "\n",
      "Here, pos is the position in the sequence, and i is the dimension index in the positional encoding vector. The choice of sine and cosine functions allows the model to potentially determine the relative position of words in a sequence because for any fixed offset k, the positional encoding for position pos + k can be derived as a linear function of the positional encoding for position pos. This property can help the model to infer the relative positions of words and make it easier to learn the dependency of a word on its surrounding words.\n",
      "\n",
      "The use of these functions also allows the model to potentially handle sequence lengths longer than those seen during training, as the encoding pattern can be extrapolated beyond the maximum trained sequence length. The positional information, when added to the word embeddings, gives the model a way to account for the order of words, which is crucial for many sequence-dependent tasks such as language understanding and translation.\n",
      "QUERY:  In the context of the provided paper, what variation related to positional encodings was experimented with by the authors?\n",
      "ANSWER In the provided context of the paper, the authors experimented with replacing the sinusoidal positional encoding with learned positional embeddings. This variation is mentioned in row (E) of Table 3. They observed that the results with learned positional embeddings were nearly identical to those of the base model which used the sinusoidal positional encoding.\n"
     ]
    }
   ],
   "source": [
    "# the questions variable was created within the string inside the `questions_list` variable.\n",
    "answers = []\n",
    "for q in questions:\n",
    "    answers.append(ask_pdf(pdf_qa,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  Is this: Positional encodings in the Transformer architecture serve the purpose of providing information about the order of the tokens in the sequence. Since the Transformer model does not use recurrence or convolution, it does not inherently account for the sequential nature of the input data. Without positional encodings, the model would treat the input as a set of tokens without any sense of word order, which is critical for many tasks such as language understanding and translation.\n",
      "\n",
      "The positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. This way, the model can learn and leverage the sequence order - which word came first, which came second, and so forth. The original Transformer paper proposes the use of sinusoidal functions to generate these positional encodings, providing a unique encoding for each position that can be easily extended to sequence lengths unseen during training. However, learned positional embeddings are an alternative approach that was also mentioned to have nearly identical results to the sinusoidal positional encoding in the paper.\n",
      "\n",
      "The use of positional encodings enables the model to consider the position of each word in the sequence when processing language, which is crucial for understanding the meaning of sentences and effectively translating between languages. the correct answer to this question: What is the purpose of positional encodings in the Transformer architecture? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: In the Transformer model, positional encodings are combined with the input embeddings by element-wise addition. Each input embedding, which is a vector representing a token in the sequence, is added to a positional encoding vector of the same dimensionality. This operation gives the model information about the position of each token in the sequence, which is important since the model's self-attention mechanism does not have any inherent notion of token order or sequence position.\n",
      "\n",
      "The positional encodings can either be learned or fixed. In the original Transformer paper, the authors used fixed sinusoidal positional encodings with frequencies that follow a geometric progression from 2π to 10000⋅2π. They defined the positional encodings as follows:\n",
      "\n",
      "PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
      "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "\n",
      "where pos is the position and i is the dimension. Each dimension of the positional encoding corresponds to a sinusoid. The use of sine and cosine functions helps the model to easily learn to attend by relative positions since for any fixed offset k, PE(pos + k) can be represented as a linear function of PE(pos).\n",
      "\n",
      "These positional encodings are added to the input embeddings before the sequence is processed by the encoder and decoder stacks of the Transformer. This allows the model to take into account the order of the sequence, which is crucial for tasks such as machine translation, where the position of a word in a sentence affects its meaning and therefore the translation. the correct answer to this question: How are positional encodings combined with the input embeddings in the Transformer model? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: The original Transformer model proposed by Vaswani et al. uses sinusoidal positional encodings. This is mentioned in the context provided, where in Table 3, row (E), it is stated that they replaced their sinusoidal positional encoding with learned positional embeddings, and observed nearly identical results to the base model. This indicates that the base model originally used sinusoidal positional encodings. the correct answer to this question: Which of the following statements about positional encodings is true for the original Transformer model proposed by Vaswani et al.? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: Positional encodings enable the model to determine the position of each word in the sequence by adding information about the absolute or relative position of the tokens to the input embeddings. Since the Transformer model architecture does not inherently capture the order of the sequence (as it lacks recurrence and convolution), positional encodings are necessary to provide the model with the sequence order information.\n",
      "\n",
      "In the context provided, the positional encodings use sine and cosine functions of different frequencies to encode the positions. Each position in the sequence is encoded using a unique combination of sine and cosine values based on its index (pos). The encoding for a particular dimension is calculated as follows:\n",
      "\n",
      "- For even indices (2i), the positional encoding uses a sine function: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
      "- For odd indices (2i+1), it uses a cosine function: PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
      "\n",
      "Here, pos is the position in the sequence, and i is the dimension index in the positional encoding vector. The choice of sine and cosine functions allows the model to potentially determine the relative position of words in a sequence because for any fixed offset k, the positional encoding for position pos + k can be derived as a linear function of the positional encoding for position pos. This property can help the model to infer the relative positions of words and make it easier to learn the dependency of a word on its surrounding words.\n",
      "\n",
      "The use of these functions also allows the model to potentially handle sequence lengths longer than those seen during training, as the encoding pattern can be extrapolated beyond the maximum trained sequence length. The positional information, when added to the word embeddings, gives the model a way to account for the order of words, which is crucial for many sequence-dependent tasks such as language understanding and translation. the correct answer to this question: How do positional encodings enable the model to determine the position of each word in the sequence? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n",
      "QUERY:  Is this: In the provided context of the paper, the authors experimented with replacing the sinusoidal positional encoding with learned positional embeddings. This variation is mentioned in row (E) of Table 3. They observed that the results with learned positional embeddings were nearly identical to those of the base model which used the sinusoidal positional encoding. the correct answer to this question: In the context of the provided paper, what variation related to positional encodings was experimented with by the authors? according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\n",
      "ANSWER YES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['YES', 'YES', 'YES', 'YES', 'YES']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for q,a in zip(questions, answers):\n",
    "    # Check for results\n",
    "    evaluations.append(ask_pdf(pdf_qa,f\"Is this: {a} the correct answer to this question: {q} according to the paper? Return ONLY '''YES''' or '''NO'''. Output:\"))\n",
    "\n",
    "evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "yes_count = evaluations.count('YES')\n",
    "scores.append(yes_count)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
