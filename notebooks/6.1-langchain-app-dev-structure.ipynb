{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run below:\n",
    "%pip install langchain\n",
    "%pip install langchain-openai\n",
    "%pip install -U langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Environment setup\n",
    "\n",
    "2. Prototype in with jupyter notebook + langsmith for inspection\n",
    "\n",
    "3. Set up the chain in the project app structure\n",
    "\n",
    "4. Set up server\n",
    "\n",
    "5. Deploy locally\n",
    "\n",
    "6. Test it on the playground\n",
    "\n",
    "7. Call it as an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment setup\n",
    "\n",
    "```\n",
    "conda create -n rag-pdf-app python=3.11\n",
    "conda activate rag-pdf-app\n",
    "pip install -U \"langchain-cli [serve]\" \"langserve [all]\"\n",
    "langchain app new .\n",
    "poetry add langchain\n",
    "poetry add langchain-community\n",
    "poetry add langchain-openai\n",
    "poetry add chromadb\n",
    "poetry add pypdf\n",
    "poetry add tiktoken\n",
    "poetry add openai\n",
    "poetry add jupyter\n",
    "poetry add python-dotenv\n",
    "```\n",
    "\n",
    "Below is optional depending on your development setup:\n",
    "\n",
    "```\n",
    "poetry run jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prototype in with jupyter notebook + langsmith for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in rag-pdf-app/app/chain.py\n",
    "\n",
    "# inspired by this template from langchain: https://github.com/langchain-ai/langchain/blob/master/templates/rag-chroma-private/rag_chroma_private/chain.py\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain import hub\n",
    "from typing import List, Tuple\n",
    "\n",
    "def load_pdf(file_path: str=\"./paper.pdf\") -> str:\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "def index_docs(docs: List[str], \n",
    "                persist_directory: str=\"./i-shall-persist\", \n",
    "                embedding_model: str=\"llama3\"):\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model)\n",
    "    vectordb = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)\n",
    "    retriever = vectordb.as_retriever()\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "\n",
    "file_path = \"./paper.pdf\"\n",
    "\n",
    "docs = load_pdf(file_path)\n",
    "\n",
    "retriever = index_docs(docs)\n",
    "\n",
    "template = \"\"\"\n",
    "Ansdwer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "# 2 suggestions for creating the rag chain:\n",
    "\n",
    "# chain = (\n",
    "#     RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()}) # RunnablePassthrough source: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html#langchain-core-runnables-passthrough-runnablepassthrough:~:text=Runnable%20to%20passthrough,and%20experiment%20with.\n",
    "#     # RunnableParallel source: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.RunnableParallel.html\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "# )\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type_kwargs={\"prompt\": prompt}, return_source_documents=True) | RunnableLambda(lambda x: x[\"result\"])\n",
    "# qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "# Add typing for input\n",
    "class Question(BaseModel):\n",
    "    __root__: str\n",
    "    # The __root__ field in Pydantic models is used to define a model\n",
    "    # where you expect a single value or a list rather than a dictionary \n",
    "    # of named fields. Essentially, it allows your model to handle instances \n",
    "    # where data does not naturally fit into a key-value structure, \n",
    "    # such as a single value or a list.\n",
    "\n",
    "\n",
    "rag_chain = chain.with_types(input_type=Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set up the chain in the project app structure\n",
    "\n",
    "![](./assets-resources/langchain-project-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the folder: `rag-pdf-app/app/`\n",
    "\n",
    "and save the rag code from above in `chain.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Set up server\n",
    "\n",
    "Go to `rag-pdf-app/app/server.py`\n",
    "\n",
    "and change:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from fastapi.responses import RedirectResponse\n",
    "from langserve import add_routes\n",
    "from app.chain import rag_chain\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def redirect_root_to_docs():\n",
    "    return RedirectResponse(\"/docs\")\n",
    "\n",
    "\n",
    "# Edit this to add the chain you want to add\n",
    "# Add routes connects the chain to our app exposing the methods of the chain to our web server\n",
    "add_routes(app,rag_chain, path=\"/rag-local\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deploy locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From within the conda environment in the root folder of the project:\n",
    "\n",
    "`langchain serve`\n",
    "\n",
    "or with poetry:\n",
    "\n",
    "`poetry run langchain serve --port=8100`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Test it on the playground\n",
    "\n",
    "`rag-local/playground/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Call it as an API\n",
    "\n",
    "```\n",
    "from langserve.client import RemoteRunnable\n",
    "\n",
    "runnable = RemoteRunnable(\"http://localhost:8000/rag-local\")\n",
    "\n",
    "runnable.invoke(\"What is attention in this paper?\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
