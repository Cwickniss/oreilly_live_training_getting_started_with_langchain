{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langgraph\n",
    "!pip install langchain\n",
    "!pip install langchainhub\n",
    "!pip install langchain-openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # Set OPENAI API Key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your openai key\"\n",
    "\n",
    "# OR (load from .env file)\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of LangGraph\n",
    "\n",
    "check out the langgraph docs here: \n",
    "\n",
    "- https://langchain-ai.github.io/langgraph/concepts/#background-agents-ai-workflows-as-graphs\n",
    "\n",
    "LangGraph models agent workflows as state machines. You define the behavior of your agents using three key components:\n",
    "\n",
    "- State: A shared data structure that represents the current snapshot of your application. It can be any Python type, but is typically a TypedDict or Pydantic BaseModel.\n",
    "\n",
    "- Nodes: Python functions that encode the logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State.\n",
    "\n",
    "- Edges: Control flow rules that determine which Node to execute next based on the current State. They can be conditional branches or fixed transitions.\n",
    "\n",
    "By composing Nodes and Edges, you can create complex, looping workflows that evolve the State over time. The real power, though, comes from how LangGraph manages that State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example was taken from the langchain documentation\n",
    "# source: https://langchain-ai.github.io/langgraph/#overview\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(messages):\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    else:\n",
    "        return \"action\"\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = MessageGraph()\n",
    "\n",
    "tools = [TavilySearchResults(max_results=1)]\n",
    "model = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)\n",
    "\n",
    "workflow.add_node(\"agent\", model)\n",
    "workflow.add_node(\"action\", ToolNode(tools))\n",
    "\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Conditional agent -> action OR agent -> END\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# Always transition `action` -> `agent`\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\") # Here we only save in-memory\n",
    "\n",
    "# Setting the interrupt means that any time an action is called, the machine will stop\n",
    "app = workflow.compile(checkpointer=memory, interrupt_before=[\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the weather in sf currently\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (call_vYpZwWls8Lpl0pZ7rg2A5k14)\n",
      " Call ID: call_vYpZwWls8Lpl0pZ7rg2A5k14\n",
      "  Args:\n",
      "    query: current weather in San Francisco\n"
     ]
    }
   ],
   "source": [
    "# Run the graph\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "for event in app.stream(\"what is the weather in sf currently\", thread, stream_mode=\"values\"):\n",
    "    event[-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph orchestrates everything:\n",
    "\n",
    "The MessageGraph contains the agent's \"Memory\"\n",
    "Conditional edges enable dynamic routing between the chatbot, tools, and the user.\n",
    "\n",
    "Persistence makes it easy to stop, resume, and even rewind for full control over your application\n",
    "\n",
    "With LangGraph, you can build complex, stateful agents by just defining your nodes, edges, and state schema - the graph take care of the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
