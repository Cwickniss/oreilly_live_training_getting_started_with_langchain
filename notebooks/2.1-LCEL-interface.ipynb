{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain Expression Language ([LCEL](https://python.langchain.com/docs/get_started/introduction))\n",
    "\n",
    "LCEL is a declarative way to compose chains. What does that mean? Means its an easy way to put chains together.\n",
    "\n",
    "Here's a bullet point summary of the LangChain Expression Language (LCEL) page:\n",
    "\n",
    "LCEL Basics: Simplifies building complex chains from basic components using a unified interface and composition primitives.\n",
    "\n",
    "Unified Interface: Every LCEL object implements the Runnable interface, supporting common invocation methods like invoke, batch, stream, ainvoke, and more.\n",
    "\n",
    "Composition Primitives: LCEL provides tools for composing chains, parallelizing components, adding fallbacks, and dynamically configuring internal chain elements.\n",
    "\n",
    "Examples and Comparisons: The page illustrates LCEL's capabilities through examples, comparing tasks performed with and without LCEL, such as invoking chains, streaming results, batch processing, and asynchronous execution.\n",
    "\n",
    "Model Flexibility: Demonstrates how LCEL allows for easy switching between different models and providers (like OpenAI or Anthropic), and runtime configurability of chat models or LLMs.\n",
    "\n",
    "Advanced Features: Discusses LCEL features like logging intermediate results with LangSmith integration and adding fallback logic for enhanced reliability.\n",
    "\n",
    "Next Steps: Recommends further exploration of the full LCEL Interface, additional composition primitives in the How-to section, and common use cases in the Cookbook section, particularly Retrieval-augmented generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Personalized recommendations: Machine learning algorithms can be used in applications such as streaming platforms, e-commerce websites, and social media platforms to provide personalized recommendations based on a user's preferences and behavior patterns.\\n\\n2. Fraud detection: Machine learning can be employed in financial institutions to detect fraudulent transactions by analyzing historical data and identifying patterns that indicate potential fraud. This helps in preventing financial losses and securing transactions.\\n\\n3. Image and speech recognition: Machine learning algorithms are widely used in applications like facial recognition systems, voice assistants, and autonomous vehicles to accurately recognize and interpret images and speech.\\n\\n4. Medical diagnosis: Machine learning models can be trained on large datasets of medical records to assist in diagnosing diseases, predicting patient outcomes, and recommending treatment plans. This can help healthcare professionals make more informed decisions and improve patient care.\\n\\n5. Natural language processing: Machine learning techniques are applied in applications such as chatbots, virtual assistants, and language translation services to understand and respond to human language. This helps in improving communication and enhancing user experience.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_chat = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template((\"List 5 examples of applications for this concept: {concept}\"))\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm_chat | output_parser\n",
    "\n",
    "chain.invoke({\"concept\": \"machine learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok nice! So we put everything together using this [pipe](https://en.wikipedia.org/wiki/Pipeline_(Unix)) `|` symbol (or [unix pipe operator](https://en.wikipedia.org/wiki/Pipeline_(Unix)) if you want to get fancy) That's the power of the LCEL language, putting different components together through a simple interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://python.langchain.com/docs/expression_language/get_started#:~:text=its%20cone-fidence!%22-,4.%20entire%20pipeline)\n",
    "\n",
    "So what is happening is:\n",
    "\n",
    "- We pass in user input on the desired concept as {\"concept\": \"machine learning\"}\n",
    "- The prompt component takes the user input, which is then used to construct a `PromptValue` after using the `concept` to construct the `prompt`.\n",
    "- The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a `ChatMessage` object.\n",
    "- Finally, the `output_parser` component takes in a `ChatMessage`, and transforms this into a Python string, which is returned from the invoke method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    A[Input concept=machine learning] --dict--> B[Prompt Template]\n",
    "    B --PromptValue--> C[ChatModel]\n",
    "    C --ChatMessage--> D[StrOutputParser]\n",
    "    D --string--> E[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bullet point summary of the key features and benefits of LangChain Expression Language (LCEL):\n",
    "\n",
    "Declarative Composing: LCEL allows for easy composition of chains, ranging from simple \"prompt + LLM\" chains to complex ones with hundreds of steps.\n",
    "\n",
    "Streaming Support: LCEL offers optimal time-to-first-token, enabling streaming of tokens from an LLM to a streaming output parser for quick, incremental output.\n",
    "\n",
    "Async Support: Chains built with LCEL can be used both synchronously (e.g., in Jupyter notebooks for prototyping) and asynchronously (e.g., in a LangServe server), maintaining consistent code for prototypes and production.\n",
    "\n",
    "Optimized Parallel Execution: LCEL automatically executes parallel steps in a chain (like fetching documents from multiple retrievers) in both sync and async interfaces, reducing latency.\n",
    "\n",
    "Retries and Fallbacks: Users can configure retries and fallbacks for any part of the LCEL chain, enhancing reliability at scale. Streaming support for these features is in development.\n",
    "\n",
    "Access to Intermediate Results: LCEL allows access to intermediate step results, useful for user updates or debugging. This feature includes streaming intermediate results and is available on all LangServe servers.\n",
    "\n",
    "Input and Output Schemas: LCEL chains come with Pydantic and JSONSchema schemas, inferred from the chain's structure, which aid in validating inputs and outputs. This is a core part of LangServe.\n",
    "\n",
    "Seamless LangSmith Tracing Integration: As chains become more complex, LCEL provides automatic logging of all steps to LangSmith for enhanced observability and debuggability.\n",
    "\n",
    "Seamless LangServe Deployment Integration: LCEL chains can be easily deployed using LangServe, facilitating smoother deployment processes.\n",
    "\n",
    "These features highlight LCEL's versatility and efficiency in both development and production environments, making it a powerful tool for creating and managing complex language chains."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
