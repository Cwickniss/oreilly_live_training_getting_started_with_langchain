{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs for the below cells.\n",
    "\n",
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install tiktoken\n",
    "# !pip install faiss-cpu\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install -U duckduckgo-search==3.9.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangChain Expression Language ([LCEL](https://python.langchain.com/docs/get_started/introduction))\n",
    "\n",
    "LCEL is a declarative way to compose chains of components. \n",
    "\n",
    "What does that mean? Means its an easy way to put useful building blocks together.\n",
    "\n",
    "\n",
    "Here's quick summary of the LangChain Expression Language (LCEL) page:\n",
    "\n",
    "- LCEL Basics: Simplifies building complex chains from basic components using a unified interface and composition primitives.\n",
    "\n",
    "- Unified Interface: Every LCEL object implements the Runnable interface, supporting common invocation methods like invoke, batch, stream, ainvoke, and more.\n",
    "\n",
    "- Composition Primitives: LCEL provides tools for composing chains, parallelizing components, adding fallbacks, and dynamically configuring internal chain elements.\n",
    "\n",
    "- Model Flexibility: LCEL allows for easy switching between different models and providers (like OpenAI or Anthropic), and runtime configurability of chat models or LLMs.\n",
    "\n",
    "- Advanced Features: LCEL features things like logging intermediate results with LangSmith integration and adding fallback logic for enhanced reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, cool but what is a component?\n",
    "\n",
    "A component is something that implements the `Runnable` protocol.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok....and what is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an object with some nice desirable features like:\n",
    "\n",
    "- input and output schemas (describe what that object takes as input and the structure of its output)\n",
    "\n",
    "Some nice methods are:\n",
    "\n",
    "- `invoke` [ainvoke]\n",
    "- `stream` [astream]\n",
    "- `batch` [abatch]\n",
    "\n",
    "\n",
    "Below is a list of common i/o types for each component:\n",
    "\n",
    "![](./assets-resources/components_input_type_output_type.png)\n",
    "\n",
    "[source for the image](https://python.langchain.com/docs/expression_language/interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm_chat = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template((\"List 5 examples of applications for this concept: {concept}\"))\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm_chat | output_parser\n",
    "\n",
    "chain.invoke({\"concept\": \"machine learning\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok nice! So we put everything together using this [pipe](https://en.wikipedia.org/wiki/Pipeline_(Unix)) `|` symbol (or [unix pipe operator](https://en.wikipedia.org/wiki/Pipeline_(Unix)) if you want to get fancy) That's the power of the LCEL language, putting different components together through a simple interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[source](https://python.langchain.com/docs/expression_language/get_started#:~:text=its%20cone-fidence!%22-,4.%20entire%20pipeline)\n",
    "\n",
    "So what is happening is:\n",
    "\n",
    "- We pass in user input on the desired concept as {\"concept\": \"machine learning\"}\n",
    "- The prompt component takes the user input, which is then used to construct a `PromptValue` after using the `concept` to construct the `prompt`.\n",
    "- The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a `ChatMessage` object.\n",
    "- Finally, the `output_parser` component takes in a `ChatMessage`, and transforms this into a Python string, which is returned from the invoke method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    A[Input concept=machine learning] --dict--> B[Prompt Template]\n",
    "    B --PromptValue--> C[ChatModel]\n",
    "    C --ChatMessage--> D[StrOutputParser]\n",
    "    D --string--> E[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a bullet point summary of the key features and benefits of LangChain Expression Language (LCEL):\n",
    "\n",
    "Declarative Composing: LCEL allows for easy composition of chains, ranging from simple \"prompt + LLM\" chains to complex ones with hundreds of steps.\n",
    "\n",
    "Streaming Support: LCEL offers optimal time-to-first-token, enabling streaming of tokens from an LLM to a streaming output parser for quick, incremental output.\n",
    "\n",
    "Async Support: Chains built with LCEL can be used both synchronously (e.g., in Jupyter notebooks for prototyping) and asynchronously (e.g., in a LangServe server), maintaining consistent code for prototypes and production.\n",
    "\n",
    "Optimized Parallel Execution: LCEL automatically executes parallel steps in a chain (like fetching documents from multiple retrievers) in both sync and async interfaces, reducing latency.\n",
    "\n",
    "Retries and Fallbacks: Users can configure retries and fallbacks for any part of the LCEL chain, enhancing reliability at scale. Streaming support for these features is in development.\n",
    "\n",
    "Access to Intermediate Results: LCEL allows access to intermediate step results, useful for user updates or debugging. This feature includes streaming intermediate results and is available on all LangServe servers.\n",
    "\n",
    "Input and Output Schemas: LCEL chains come with Pydantic and JSONSchema schemas, inferred from the chain's structure, which aid in validating inputs and outputs. This is a core part of LangServe.\n",
    "\n",
    "Seamless LangSmith Tracing Integration: As chains become more complex, LCEL provides automatic logging of all steps to LangSmith for enhanced observability and debuggability.\n",
    "\n",
    "Seamless LangServe Deployment Integration: LCEL chains can be easily deployed using LangServe, facilitating smoother deployment processes.\n",
    "\n",
    "These features highlight LCEL's versatility and efficiency in both development and production environments, making it a powerful tool for creating and managing complex language chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into the `Runnable` components in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableLambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Define a custom function\n",
    "def add_one(x: int) -> int:\n",
    "    return x + 1\n",
    "\n",
    "# Create a RunnableLambda and invoke it\n",
    "runnable = RunnableLambda(add_one)\n",
    "output = runnable.invoke(5)\n",
    "print(output)  # Output: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`RunnableSequence`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnablesequence%20invokes%20a%20series%20of%20runnables%20sequentially%2C%20with%20one%20runnable%E2%80%99s%20output%20serving%20as%20the%20next%E2%80%99s%20input.%20construct%20using%20the%20%7C%20operator%20or%20by%20passing%20a%20list%20of%20runnables%20to%20runnablesequence.)\n",
    "\n",
    "RunnableSequence invokes a series of runnables sequentially, with one runnable’s output serving as the next’s input. Construct using the | operator or by passing a list of runnables to RunnableSequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "# Suppose we have a list of numbers\n",
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Preprocess function: Multiply each number by 2\n",
    "def preprocess(data):\n",
    "    return [x * 2 for x in data]\n",
    "\n",
    "# Apply model function: Square each number\n",
    "def apply_model(data):\n",
    "    return [x ** 2 for x in data]\n",
    "\n",
    "# Postprocess function: Sum all the numbers\n",
    "def postprocess(data):\n",
    "    return sum(data)\n",
    "\n",
    "# Create RunnableLambdas\n",
    "preprocess_runnable = RunnableLambda(preprocess)\n",
    "apply_model_runnable = RunnableLambda(apply_model)\n",
    "postprocess_runnable = RunnableLambda(postprocess)\n",
    "\n",
    "# Create a RunnableSequence and invoke it\n",
    "sequence = preprocess_runnable | apply_model_runnable | postprocess_runnable\n",
    "# runnable_sequence = RunnableSequence(sequence)\n",
    "output = sequence.invoke(data)\n",
    "\n",
    "print(output)\n",
    "# or\n",
    "runnable_sequence = RunnableSequence(first=preprocess_runnable, middle=[apply_model_runnable], last=postprocess_runnable)\n",
    "runnable_sequence.invoke(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Runnableparallel`]([`Runnableparallel`](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnableparallel%20invokes%20runnables%20concurrently%2C%20providing%20the%20same%20input%20to%20each.%20construct%20it%20using%20a%20dict%20literal%20within%20a%20sequence%20or%20by%20passing%20a%20dict%20to%20runnableparallel.)https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable:~:text=runnableparallel%20invokes%20runnables%20concurrently%2C%20providing%20the%20same%20input%20to%20each.%20construct%20it%20using%20a%20dict%20literal%20within%20a%20sequence%20or%20by%20passing%20a%20dict%20to%20runnableparallel.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel invokes runnables concurrently, providing the same input to each. Construct it using a dict literal within a sequence or by passing a dict to RunnableParallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# A RunnableSequence constructed using the `|` operator\n",
    "sequence = RunnableLambda(lambda x: x + 1) | RunnableLambda(lambda x: x * 2)\n",
    "sequence.invoke(1) # 4\n",
    "sequence.batch([1, 2, 3]) # [4, 6, 8]\n",
    "\n",
    "# A sequence that contains a RunnableParallel constructed using a dict literal\n",
    "sequence = RunnableLambda(lambda x: x + 1) | {\n",
    "    'mul_2': RunnableLambda(lambda x: x * 2),\n",
    "    'mul_5': RunnableLambda(lambda x: x * 5)\n",
    "}\n",
    "sequence.invoke(1) # {'mul_2': 4, 'mul_5': 10}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build chains, what you're doing is building a type of Runnable!\n",
    "\n",
    "That could be a RunnableSequence or a RunnableParallel  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "prompt = PromptTemplate.from_template(\"Summarize this {text}\")\n",
    "prompt2 = PromptTemplate.from_template(\"Explain this {text}\")\n",
    "\n",
    "\n",
    "chain1 = prompt | llm\n",
    "\n",
    "chain2 = prompt2 | llm\n",
    "\n",
    "runnable_parallel = chain1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RunnableParallel can be useful for manipulating the output of one Runnable to match the input format of the next Runnable in a sequence.](https://python.langchain.com/docs/expression_language/how_to/map#:~:text=RunnableParallel%20can%20be,the%0A%E2%80%9Cquestion%E2%80%9D%20key.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel (aka. RunnableMap) makes it easy to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\n",
    "\n",
    "([see it in docs here](https://python.langchain.com/docs/expression_language/how_to/map#:~:text=runnableparallel%20(aka.%20runnablemap)%20makes%20it%20easy%20to%20execute%20multiple%20runnables%20in%20parallel%2C%20and%20to%20return%20the%20output%20of%20these%20runnables%20as%20a%20map.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "model = ChatOpenAI()\n",
    "joke_chain = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model\n",
    "poem_chain = (\n",
    "    ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(joke=joke_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel are also useful for running independent processes in parallel, since each Runnable in the map is executed in parallel. For example, we can see our earlier joke_chain, poem_chain and map_chain all have about the same runtime, even though map_chain executes both of the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "joke_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "poem_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "map_chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RunnablePassthrough](https://python.langchain.com/docs/expression_language/how_to/passthrough#:~:text=RunnablePassthrough%20allows%20to,pass%20it%20through.)\n",
    "\n",
    "Allows you to pass inputs unchanged or with addition of new keys.\n",
    "\n",
    "Usually you would connect this with `RunnableParallel` to assign data to new key in the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just call it, it will take the input and pass it along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Create a RunnablePassthrough\n",
    "passthrough = RunnablePassthrough()\n",
    "passthrough.invoke(5) # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you call it with the `.assign(...)` mthod, then it will take the input, and add extra arguments passed to the assign function. \n",
    "\n",
    "The key being added has to be a lambda function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Passthrough with assignment, adds an extra key-value pair\n",
    "runnable = RunnablePassthrough.assign(extra_key_add_5=lambda x:  x[\"num\"]+5)\n",
    "input_data = {\"num\": 1}\n",
    "result = runnable.invoke(input_data)\n",
    "# Output: {'num': 1, 'extra_value': 42}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In combination with RunnableParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "runnable.invoke(1) # {'origin': 1, 'modified': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified example from langchain docs: https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.passthrough.RunnablePassthrough.html?highlight=runnablepassthrough#langchain_core.runnables.passthrough.RunnablePassthrough \n",
    "# or for an \"LLM\" example:\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] + \" (applied the parsing logic)\" # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we have these 4 types of main objects:\n",
    "\n",
    "- `RunnableLambda`\n",
    "- `RunnableSequence`\n",
    "- `RunnablePassthrough`\n",
    "- `RunnableParallel`\n",
    "\n",
    "combined they make up kind of the core building block system of langchain allow you to create complex inner logics powered by llms. \n",
    "\n",
    "Let's create some fun examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "def fun_llm(prompt: str):\n",
    "    return ChatOpenAI().predict(f\"Make this funny: {prompt}\")\n",
    "\n",
    "\n",
    "def corporate_llm(prompt: str):\n",
    "    return ChatOpenAI().predict(f\"Make this topic a subject of a very corporate email: {prompt}\")\n",
    "\n",
    "fun_chain = RunnableLambda(fun_llm) | {\n",
    "    \"original-text-input\": RunnablePassthrough(), \n",
    "    \"make-it-corporate\": lambda x: corporate_llm(x)\n",
    "}\n",
    "\n",
    "\n",
    "fun_chain.invoke(\"gorillas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    "  )\n",
    "\n",
    "runnable.invoke('hello')\n",
    "# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down whats happening:\n",
    "\n",
    "We create a `runnable` object, which is an instance of the `RunableSequence` object. This chain takes in as input a simple string: `hello`, and what it does is it \n",
    "simulates passing that same string to 2 `different` llms (which in this case are the same) and applies the logic of each of these llms as well as a third logic described by the new extra key added by the RunnablePassthrough.assign() method, that takes as input the output of the `llm1` and `llm2` as input to combine it through the lambda function associated with the new extra key created: `total_chars`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this example a bit more interesting by applying some research workflow vibe to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a couple of llms to do different things with some piece of content.\n",
    "# In this case let's use different llms to create summarization levels for some text.\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def llm_summarization_level1(prompt: str):\n",
    "    return ChatOpenAI().predict(f\"Summarize this text: {prompt}\")\n",
    "\n",
    "def llm_summarization_level2(prompt: str):\n",
    "    return ChatOpenAI().predict(f\"Summarize this text in bullet points: {prompt}\")\n",
    "\n",
    "def llm_summarization_level3(prompt: str):\n",
    "    return ChatOpenAI().predict(f\"Summarize this text into one short paragraph: {prompt}\")\n",
    "\n",
    "\n",
    "# now let's create a chain that will run all of these llms in parallel and then return the results\n",
    "\n",
    "runnable = {\n",
    "    'level1':  llm_summarization_level1,\n",
    "    'level2':  llm_summarization_level2,\n",
    "    'level3':  llm_summarization_level3,\n",
    "} | RunnablePassthrough.assign(prompt_level4=lambda x: f\"Combine these 3 summarizations\\\n",
    "    into one combining all their \\n \\\n",
    "    useful features. \\\n",
    "    Level1: {x['level1']}.\\n \\\n",
    "    Level2: {x['level2']}.\\n\\\n",
    "    Level3: {x['level3']}\"\n",
    "                       ) | RunnablePassthrough.assign(output=RunnableLambda(lambda x: ChatOpenAI().predict(x['prompt_level4']))) \n",
    "# For the assign method with RunningPasshthrough, we'll combine the summarizations into a fourth one.\n",
    "\n",
    "runnable.invoke(\n",
    "    \"\"\"\n",
    "    The code you provided is from the nest_asyncio library, which is designed to patch Python's asyncio library to allow nested usage of the asyncio event loop. To understand this code, it's crucial first to grasp what an event loop is in the context of asynchronous programming.\n",
    "What is an Event Loop?\n",
    "An event loop in programming, particularly in asynchronous programming, is a central control structure that manages and dispatches events or messages in a program. In the context of Python's asyncio library, the event loop is a core feature that runs asynchronous tasks and callbacks, handles network IO operations, and manages subprocesses. It is essentially the heart of the asyncio module, enabling asynchronous programming by juggling and scheduling the execution of various tasks.\n",
    "Key Concepts in the nest_asyncio Code\n",
    "Patching asyncio: The code modifies (patches) certain parts of the asyncio library. This is done to change the default behavior of asyncio, particularly to support nested event loops, which are not allowed in standard asyncio.\n",
    "Reentrancy in Event Loops: The primary function of nest_asyncio is to make the asyncio event loop reentrant. In computing, reentrancy refers to the ability of a function to be paused in the middle of execution and safely called again (\"re-entered\") before its previous executions are complete. This is not normally supported by the asyncio event loops, as they are designed to prevent re-entry (or nesting) to avoid complex problems and unexpected behavior.\n",
    "Modifying Loop Behavior: The code alters the behavior of the event loop methods like run_forever, run_until_complete, and the internal _run_once. These modifications allow the event loop to pause and resume (re-enter) gracefully, facilitating nesting.\n",
    "Context Managers: The code uses context managers (manage_run and manage_asyncgens) to properly manage the state of the event loop during entry and exit of asynchronous contexts, which is crucial for handling nested loops correctly.\n",
    "Task Patching: It also patches the Task class of asyncio to modify its step function. This is necessary to ensure that the tasks (units of work scheduled by the event loop) behave correctly in a nested loop scenario.\n",
    "Tornado Patching: If the Tornado library (an asynchronous networking library) is used, nest_asyncio makes Tornado aware of the Python asyncio Future, ensuring compatibility.\n",
    "Understanding the Event Loop in Async Programming\n",
    "In asynchronous programming, especially in Python's asyncio, the event loop is pivotal. It allows the execution of multiple tasks seemingly in parallel by switching between them. This switching is non-blocking, meaning the program can handle other tasks while waiting for some IO operation to complete, thereby increasing efficiency and responsiveness.\n",
    "\n",
    "The nest_asyncio library's primary role is to tweak the asyncio's event loop to support nested operation, which is particularly useful in scenarios like running an asyncio event loop inside another asyncio event loop, something that standard asyncio does not support by default.\n",
    "\n",
    "For more details on asynchronous programming and event loops in Python, the Python asyncio documentation is a comprehensive resource.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple research assistant, heavily based on this implementation by Harrison Chase:\n",
    "- https://gist.github.com/hwchase17/69a8cdef9b01760c244324339ab64f0c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "import json\n",
    "\n",
    "RESULTS_PER_QUESTION = 1\n",
    "\n",
    "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "\n",
    "def web_search(query: str, num_results: int = RESULTS_PER_QUESTION):\n",
    "    results = ddg_search.results(query, num_results)\n",
    "    return [r[\"link\"] for r in results]\n",
    "\n",
    "\n",
    "def scrape_text(url: str):\n",
    "    # Send a GET request to the webpage\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the content of the request with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extract all text from the webpage\n",
    "            page_text = soup.get_text(separator=\" \", strip=True)\n",
    "\n",
    "            # Print the extracted text\n",
    "            return page_text\n",
    "        else:\n",
    "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return f\"Failed to retrieve the webpage: {e}\"\n",
    "\n",
    "\n",
    "def collapse_list_of_lists(list_of_lists):\n",
    "    content = []\n",
    "    for l in list_of_lists:\n",
    "        content.append(\"\\n\\n\".join(l))\n",
    "    return \"\\n\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUMMARY_TEMPLATE = \"\"\"{text} \n",
    "-----------\n",
    "Using the above text, answer in short the following question: \n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"  # noqa: E501\n",
    "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
    "\n",
    "\n",
    "url = \"https://blog.langchain.dev/announcing-langsmith/\"\n",
    "\n",
    "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
    "    summary = RunnablePassthrough.assign(\n",
    "    text=lambda x: scrape_text(x[\"url\"])[:10000]\n",
    ") | SUMMARY_PROMPT | ChatOpenAI(model=\"gpt-3.5-turbo-1106\") | StrOutputParser()\n",
    ") | (lambda x: f\"URL: {x['url']}\\n\\nSUMMARY: {x['summary']}\")\n",
    "\n",
    "# Let's see the input_schema of our created chain\n",
    "print(scrape_and_summarize_chain.input_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, even though this chain requires two keys in the input dictionary, that's not reflected in the input_schema() of the chain, because in it only goes the variables contained in the ChatPromptTemplate(). \n",
    "\n",
    "Interestingly enough, the `text` variable which is in the ChatPromptTemplate, is not in the schema because its already part of the chain itself (therefore not being necessary in the input_schema() I guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this\n",
    "# let's test this chain on some url\n",
    "scrape_and_summarize_chain.invoke({\"question\": \"What is langsmith?\", \"url\": url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_chain = RunnablePassthrough.assign(\n",
    "    urls = lambda x: web_search(x[\"question\"]) # urls will be the output of the web_search function()\n",
    ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this web search chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_chain.invoke({\"question\": \"Look up from langchain.chains import example_generator\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Write 3 google search queries to search online that form an \"\n",
    "            \"objective opinion from the following: {question}\\n\"\n",
    "            \"You must respond with a list of strings in the following format: \"\n",
    "            '[\"query 1\", \"query 2\", \"query 3\"].',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_question_chain = SEARCH_PROMPT | ChatOpenAI(temperature=0) | StrOutputParser() | json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_question_chain.invoke({\"question\": \"What are the most useful langchain utilities for research?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_research_chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_research_chain.invoke({\"question\": \"What is langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
    "\n",
    "\n",
    "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
    "RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
    "--------\n",
    "{research_summary}\n",
    "--------\n",
    "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
    "The report should focus on the answer to the question, should be well structured, informative, \\\n",
    "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
    "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
    "You must write the report with markdown syntax.\n",
    "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
    "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
    "You must write the report in apa format.\n",
    "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
    "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = RunnablePassthrough.assign(\n",
    "    research_summary= full_research_chain | collapse_list_of_lists\n",
    ") | prompt | ChatOpenAI(model=\"gpt-3.5-turbo-1106\") | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableMap`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's modify this example for something that uses the updated version of the pydantic library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder you are executing pip from can no longer be found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this example requires pydantic==\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm_chat = ChatOpenAI()\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever.get_relevant_documents(\"Where did harrison work?\")\n",
    "\n",
    "template = \"Answer the question based on this context: {context}\\n\\nQuestion: {question}\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "    \n",
    "}) | prompt | llm_chat | output_parser\n",
    "\n",
    "\n",
    "\n",
    "chain.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect just the `RunnableMap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='bears like to eat honey')],\n",
       " 'question': 'What did harrison do?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "inputs_with_lambda = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "})\n",
    "\n",
    "inputs_with_lambda.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='harrison worked at kensho'),\n",
       "  Document(page_content='bears like to eat honey')],\n",
       " 'question': 'What did harrison do?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_runnable_passthrough = RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "})\n",
    "\n",
    "inputs_with_runnable_passthrough.invoke({\"question\": \"What did harrison do?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use tools with runnables in some neat and interesting ways. Let's see that in the next notebook (2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that what the `RunnableMap` is doing, is create the right inputs for the downstream of this chain which in this case is seeting the `context` key with a value of a list of `Document` objects, and the question key with itself.\n",
    "(which now we can probably change to something simpler with the `RunnablePassthrough` for example)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-langchain",
   "language": "python",
   "name": "oreilly-langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
